{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "def print_pretty_matrix(title, matrix):\n",
    "    if not isinstance(matrix, np.ndarray):\n",
    "        raise TypeError(\"Input must be a NumPy array\")\n",
    "    \n",
    "    rows, cols = matrix.shape\n",
    "    print(f\"\\n\\n{title}\\n********\")\n",
    "    for row in range(rows):\n",
    "        formatted_row = \" | \".join(f\"{matrix[row, col]:>10}\" for col in range(cols))\n",
    "        print(f\"| {formatted_row} |\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "rewards\n",
      "********\n",
      "|       -0.1 |       -0.1 |       -0.1 |       -0.1 |\n",
      "|       -0.1 |       -1.0 |       -0.1 |        1.0 |\n",
      "|       -0.1 |       -0.1 |        1.0 |       -0.1 |\n",
      "|       -1.0 |       -0.1 |       -0.1 |      100.0 |\n"
     ]
    }
   ],
   "source": [
    "# Parámetros del Grid World\n",
    "grid_size = 4\n",
    "start = (0, 0)\n",
    "goal = (3, 3)\n",
    "learning_rate = 0.0005\n",
    "discount_factor = 0.9\n",
    "epsilon = 0.1\n",
    "episodes = 10000\n",
    "target_update_frequency = 100\n",
    "# Definir las acciones\n",
    "actions = [\"up\", \"down\", \"left\", \"right\"]\n",
    "action_to_index = {action: i for i, action in enumerate(actions)}\n",
    "\n",
    "# Definir la matriz de recompensas del Grid World\n",
    "rewards = np.full((grid_size, grid_size), -0.1)  # Penalización por movimiento\n",
    "rewards[1, 3] = 1  # Recompensa positiva\n",
    "rewards[2, 2] = 1  # Recompensa positiva\n",
    "rewards[1, 1] = -1  # Recompensa negativa\n",
    "rewards[3, 0] = -1  # Recompensa negativa\n",
    "rewards[goal] = 100  # Recompensa por llegar a la meta\n",
    "\n",
    "print_pretty_matrix(\"rewards\", rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio: 0, Pérdida: 10028.9306640625\n",
      "Episodio: 1000, Pérdida: 0.8057648539543152\n",
      "Episodio: 2000, Pérdida: 0.6272516250610352\n",
      "Episodio: 3000, Pérdida: 3.512993335723877\n",
      "Episodio: 4000, Pérdida: 0.3909302353858948\n",
      "Episodio: 5000, Pérdida: 0.27299731969833374\n",
      "Episodio: 6000, Pérdida: 0.3558445870876312\n",
      "Episodio: 7000, Pérdida: 0.2181273102760315\n",
      "Episodio: 8000, Pérdida: 0.3518146276473999\n",
      "Episodio: 9000, Pérdida: 0.07332314550876617\n",
      "\n",
      "Matriz de Recompensas:\n",
      "[[ -0.1  -0.1  -0.1  -0.1]\n",
      " [ -0.1  -1.   -0.1   1. ]\n",
      " [ -0.1  -0.1   1.   -0.1]\n",
      " [ -1.   -0.1  -0.1 100. ]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Definir la red neuronal para aproximar la función Q\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(grid_size * grid_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 4)  # 4 acciones posibles: arriba, abajo, izquierda, derecha\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# Inicializar la red DQN principal y la red objetivo\n",
    "dqn = DQN()\n",
    "target_dqn = copy.deepcopy(dqn)\n",
    "optimizer = optim.Adam(dqn.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Función para convertir el estado (posición en la cuadrícula) en un tensor de entrada para la red\n",
    "def state_to_tensor(state):\n",
    "    tensor = torch.zeros(grid_size * grid_size)\n",
    "    tensor[state[0] * grid_size + state[1]] = 1.0\n",
    "    return tensor.unsqueeze(0).float()\n",
    "\n",
    "# Función para elegir la acción (con epsilon-greedy)\n",
    "def choose_action(state):\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        return random.randint(0, 3)  # Acción aleatoria\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            q_values = dqn(state_to_tensor(state))\n",
    "        return torch.argmax(q_values).item()\n",
    "\n",
    "# Función para obtener la próxima posición dada una acción\n",
    "def take_action(state, action):\n",
    "    if action == 0:  # up\n",
    "        return (max(state[0] - 1, 0), state[1])\n",
    "    elif action == 1:  # down\n",
    "        return (min(state[0] + 1, grid_size - 1), state[1])\n",
    "    elif action == 2:  # left\n",
    "        return (state[0], max(state[1] - 1, 0))\n",
    "    elif action == 3:  # right\n",
    "        return (state[0], min(state[1] + 1, grid_size - 1))\n",
    "\n",
    "# Entrenamiento con Double DQN\n",
    "for episode in range(episodes):\n",
    "    state = start\n",
    "\n",
    "    while state != goal:\n",
    "        action = choose_action(state)\n",
    "        next_state = take_action(state, action)\n",
    "\n",
    "        # Obtener la recompensa correspondiente\n",
    "        reward = rewards[next_state]\n",
    "\n",
    "        # Calcular el valor Q objetivo usando la red objetivo\n",
    "        with torch.no_grad():\n",
    "            next_q_values = dqn(state_to_tensor(next_state))\n",
    "            best_action = torch.argmax(next_q_values).item()\n",
    "            target_q_value = reward + discount_factor * target_dqn(state_to_tensor(next_state))[0, best_action]\n",
    "\n",
    "        # Obtener el valor Q estimado actual\n",
    "        q_values = dqn(state_to_tensor(state))\n",
    "        current_q_value = q_values[0, action]\n",
    "\n",
    "        # Calcular la pérdida\n",
    "        loss = loss_fn(current_q_value, target_q_value)\n",
    "\n",
    "        # Actualizar la red principal\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Mover al siguiente estado\n",
    "        state = next_state\n",
    "\n",
    "    # Actualizar la red objetivo cada 'target_update_frequency' episodios\n",
    "    if episode % target_update_frequency == 0:\n",
    "        target_dqn.load_state_dict(dqn.state_dict())\n",
    "\n",
    "    # Imprimir la pérdida cada 1000 episodios\n",
    "    if episode % 1000 == 0:\n",
    "        print(f\"Episodio: {episode}, Pérdida: {loss.item()}\")\n",
    "\n",
    "# Mostrar la matriz de recompensas para referencia\n",
    "print(\"\\nMatriz de Recompensas:\")\n",
    "print(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.058767318725586"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q(0,0): [[-0.07794925 -0.01008882  0.04600317  0.07517506]]\n",
      "Q(0,1): [[-0.11044546 -0.00890867  0.063161    0.05893465]]\n",
      "Q(0,2): [[-0.07694578 -0.07176758  0.06956495 -0.04668758]]\n",
      "Q(0,3): [[-0.12292645  0.0629231   0.07387641 -0.01150181]]\n",
      "Q(1,0): [[-0.03791503  0.183098    0.14410655 -0.00131273]]\n",
      "Q(1,1): [[-0.11482402 -0.07017019  0.11718072  0.08417861]]\n",
      "Q(1,2): [[-0.02801664  0.05317232  0.07353303  0.05540504]]\n",
      "Q(1,3): [[-0.05168027 -0.03166492  0.03067819  0.08033122]]\n",
      "Q(2,0): [[-0.05656049  0.01939723  0.14320491  0.04027523]]\n",
      "Q(2,1): [[-0.14294015  0.23239294  0.09981627  0.11295213]]\n",
      "Q(2,2): [[-0.09099971  0.12558104  0.03222947  0.01954882]]\n",
      "Q(2,3): [[-0.14825185  0.03904763  0.05670653  0.07405663]]\n",
      "Q(3,0): [[-0.00987893  0.09967978  0.0408778   0.08319162]]\n",
      "Q(3,1): [[-0.10838757  0.10081992  0.00804136 -0.00882077]]\n",
      "Q(3,2): [[-0.10095967 -0.12758033  0.04881566  0.06136369]]\n",
      "Q(3,3): [[-0.18945417  0.08904378  0.0470854  -0.06094315]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        print(f\"Q({i},{j}): {dqn(state_to_tensor((i, j)))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 100\n",
    "target_update_frequency = episodes // 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\usuario\\anaconda3\\envs\\env_101\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio: 0, Pérdida: 9937.7158203125\n",
      "Episodio: 1, Pérdida: 9871.63671875\n",
      "Episodio: 2, Pérdida: 9731.8349609375\n",
      "Episodio: 3, Pérdida: 9740.2734375\n",
      "Episodio: 4, Pérdida: 9599.759765625\n",
      "Episodio: 5, Pérdida: 9391.3193359375\n",
      "Episodio: 6, Pérdida: 8986.7265625\n",
      "Episodio: 7, Pérdida: 9662.7626953125\n",
      "Episodio: 8, Pérdida: 9391.9912109375\n",
      "Episodio: 9, Pérdida: 8703.8916015625\n",
      "\n",
      "Matriz de Recompensas:\n",
      "[[ -0.1  -0.1  -0.1  -0.1]\n",
      " [ -0.1  -1.   -0.1   1. ]\n",
      " [ -0.1  -0.1   1.   -0.1]\n",
      " [ -1.   -0.1  -0.1 100. ]]\n"
     ]
    }
   ],
   "source": [
    "# Definir la red neuronal para aproximar la función Q\n",
    "class DQN(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = tf.keras.layers.Dense(128, activation='relu', input_shape=(grid_size * grid_size,))\n",
    "        self.fc2 = tf.keras.layers.Dense(128, activation='relu')\n",
    "        self.fc3 = tf.keras.layers.Dense(4)  # 4 acciones posibles: arriba, abajo, izquierda, derecha\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return self.fc3(x)\n",
    "\n",
    "# Inicializar la red DQN principal y la red objetivo\n",
    "dqn = DQN()\n",
    "target_dqn = DQN()\n",
    "target_dqn.set_weights(dqn.get_weights())  # Inicializar la red objetivo con los pesos de la red principal\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "# Función para convertir el estado (posición en la cuadrícula) en un tensor de entrada para la red\n",
    "def state_to_tensor(state):\n",
    "    tensor = np.zeros(grid_size * grid_size)\n",
    "    tensor[state[0] * grid_size + state[1]] = 1.0\n",
    "    return np.expand_dims(tensor, axis=0).astype(np.float32)\n",
    "\n",
    "# Función para elegir la acción (con epsilon-greedy)\n",
    "def choose_action(state):\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        return random.randint(0, 3)  # Acción aleatoria\n",
    "    else:\n",
    "        q_values = dqn(state_to_tensor(state))\n",
    "        return np.argmax(q_values.numpy())\n",
    "\n",
    "# Función para obtener la próxima posición dada una acción\n",
    "def take_action(state, action):\n",
    "    if action == 0:  # up\n",
    "        return (max(state[0] - 1, 0), state[1])\n",
    "    elif action == 1:  # down\n",
    "        return (min(state[0] + 1, grid_size - 1), state[1])\n",
    "    elif action == 2:  # left\n",
    "        return (state[0], max(state[1] - 1, 0))\n",
    "    elif action == 3:  # right\n",
    "        return (state[0], min(state[1] + 1, grid_size - 1))\n",
    "\n",
    "# Entrenamiento con Double DQN\n",
    "for episode in range(episodes):\n",
    "    state = start\n",
    "\n",
    "    while state != goal:\n",
    "        action = choose_action(state)\n",
    "        next_state = take_action(state, action)\n",
    "\n",
    "        # Obtener la recompensa correspondiente\n",
    "        reward = rewards[next_state]\n",
    "\n",
    "        # Calcular el valor Q objetivo usando la red objetivo\n",
    "        next_q_values = dqn(state_to_tensor(next_state))\n",
    "        best_action = np.argmax(next_q_values.numpy())\n",
    "        target_q_value = reward + discount_factor * target_dqn(state_to_tensor(next_state))[0, best_action].numpy()\n",
    "\n",
    "        # Obtener el valor Q estimado actual\n",
    "        with tf.GradientTape() as tape:\n",
    "            q_values = dqn(state_to_tensor(state))\n",
    "            current_q_value = q_values[0, action]\n",
    "            target_q_value = tf.convert_to_tensor(target_q_value, dtype=tf.float32)\n",
    "\n",
    "            current_q_value = tf.expand_dims(current_q_value, axis=0)\n",
    "            target_q_value = tf.expand_dims(target_q_value, axis=0)\n",
    "            \n",
    "            # Calcular la pérdida\n",
    "            loss = loss_fn(current_q_value, target_q_value)\n",
    "\n",
    "        # Actualizar la red principal\n",
    "        grads = tape.gradient(loss, dqn.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, dqn.trainable_variables))\n",
    "\n",
    "        # Mover al siguiente estado\n",
    "        state = next_state\n",
    "\n",
    "    # Actualizar la red objetivo cada 'target_update_frequency' episodios\n",
    "    if episode % target_update_frequency == 0:\n",
    "        target_dqn.set_weights(dqn.get_weights())\n",
    "\n",
    "    # Imprimir la pérdida cada 1000 episodios\n",
    "    # if episode % 1000 == 0:\n",
    "    print(f\"Episodio: {episode}, Pérdida: {loss.numpy()}\")\n",
    "\n",
    "# Mostrar la matriz de recompensas para referencia\n",
    "print(\"\\nMatriz de Recompensas:\")\n",
    "print(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experience replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "# Parámetros\n",
    "memory_size = 10000\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# # Configuración de paralelismo\n",
    "# # tf.config.threading.set_inter_op_parallelism_threads(4)\n",
    "# # tf.config.threading.set_intra_op_parallelism_threads(4)\n",
    "\n",
    "\n",
    "# # Parámetros\n",
    "# grid_size = 4\n",
    "# learning_rate = 0.001\n",
    "# discount_factor = 0.99\n",
    "# epsilon = 0.1\n",
    "# episodes = 100\n",
    "# target_update_frequency = episodes // 10\n",
    "# memory_size = 10000\n",
    "# batch_size = 64\n",
    "\n",
    "# # Definir la red neuronal para aproximar la función Q\n",
    "# class DQN(tf.keras.Model):\n",
    "#     def __init__(self):\n",
    "#         super(DQN, self).__init__()\n",
    "#         self.fc1 = tf.keras.layers.Dense(128, activation='relu', input_shape=(grid_size * grid_size,))\n",
    "#         self.fc2 = tf.keras.layers.Dense(128, activation='relu')\n",
    "#         self.fc3 = tf.keras.layers.Dense(4)  # 4 acciones posibles: arriba, abajo, izquierda, derecha\n",
    "\n",
    "#     def call(self, x):\n",
    "#         x = self.fc1(x)\n",
    "#         x = self.fc2(x)\n",
    "#         return self.fc3(x)\n",
    "\n",
    "# # Inicializar la red DQN principal y la red objetivo\n",
    "# # Inicializar la red DQN principal y la red objetivo\n",
    "# dqn = DQN()\n",
    "# target_dqn = DQN()\n",
    "# target_dqn.set_weights(dqn.get_weights())  # Inicializar la red objetivo con los pesos de la red principal\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "# loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "# # Memoria de repetición (Experience Replay Memory)\n",
    "# replay_memory = deque(maxlen=memory_size)\n",
    "\n",
    "# # Función para convertir el estado (posición en la cuadrícula) en un tensor de entrada para la red\n",
    "# def state_to_tensor(state):\n",
    "#     tensor = np.zeros(grid_size * grid_size)\n",
    "#     tensor[state[0] * grid_size + state[1]] = 1.0\n",
    "#     return np.expand_dims(tensor, axis=0).astype(np.float32)\n",
    "\n",
    "# # Función para elegir la acción (con epsilon-greedy)\n",
    "# @tf.function\n",
    "# def choose_action(state):\n",
    "#     if random.uniform(0, 1) < epsilon:\n",
    "#         return tf.convert_to_tensor(random.randint(0, 3), dtype=tf.int64)  # Acción aleatoria\n",
    "#     else:\n",
    "#         q_values = dqn(state_to_tensor(state))\n",
    "#         return tf.argmax(q_values[0])\n",
    "\n",
    "# # Función para obtener la próxima posición dada una acción\n",
    "# def take_action(state, action):\n",
    "#     if action == 0:  # up\n",
    "#         return (max(state[0] - 1, 0), state[1])\n",
    "#     elif action == 1:  # down\n",
    "#         return (min(state[0] + 1, grid_size - 1), state[1])\n",
    "#     elif action == 2:  # left\n",
    "#         return (state[0], max(state[1] - 1, 0))\n",
    "#     elif action == 3:  # right\n",
    "#         return (state[0], min(state[1] + 1, grid_size - 1))\n",
    "\n",
    "# # Función de entrenamiento con Double DQN y Experience Replay\n",
    "# @tf.function\n",
    "# def train_step():\n",
    "#     minibatch = random.sample(replay_memory, batch_size)\n",
    "\n",
    "#     states = np.array([state_to_tensor(exp[0]) for exp in minibatch])\n",
    "#     actions = np.array([exp[1] for exp in minibatch])\n",
    "#     rewards = np.array([exp[2] for exp in minibatch])\n",
    "#     next_states = np.array([state_to_tensor(exp[3]) for exp in minibatch])\n",
    "#     dones = np.array([exp[4] for exp in minibatch])\n",
    "\n",
    "#     states = tf.convert_to_tensor(states, dtype=tf.float32)\n",
    "#     actions = tf.convert_to_tensor(actions, dtype=tf.int64)\n",
    "#     rewards = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
    "#     next_states = tf.convert_to_tensor(next_states, dtype=tf.float32)\n",
    "#     dones = tf.convert_to_tensor(dones, dtype=tf.float32)\n",
    "\n",
    "#     # Asegurar que las dimensiones son correctas\n",
    "#     states = tf.squeeze(states, axis=1)  # Convierte de (64, 1, 16) a (64, 16)\n",
    "#     next_states = tf.squeeze(next_states, axis=1)  # Convierte de (64, 1, 16) a (64, 16)\n",
    "\n",
    "#     with tf.GradientTape() as tape:\n",
    "#         # Obtener los valores Q actuales\n",
    "#         q_values = dqn(states)\n",
    "#         q_values = tf.reduce_sum(q_values * tf.one_hot(actions, 4), axis=1)\n",
    "\n",
    "#         # Calcular los valores Q objetivos\n",
    "#         next_q_values = target_dqn(next_states)\n",
    "#         best_next_q_values = tf.reduce_max(next_q_values, axis=1)\n",
    "#         target_q_values = rewards + discount_factor * best_next_q_values * (1.0 - dones)\n",
    "\n",
    "#         # Calcular la pérdida\n",
    "#         loss = loss_fn(target_q_values, q_values)\n",
    "\n",
    "#     # Actualizar la red principal\n",
    "#     grads = tape.gradient(loss, dqn.trainable_variables)\n",
    "#     optimizer.apply_gradients(zip(grads, dqn.trainable_variables))\n",
    "\n",
    "#     return loss\n",
    "\n",
    "# episodes = 100\n",
    "# target_update_frequency = episodes // 5\n",
    "# # Entrenamiento con Double DQN\n",
    "# for episode in range(episodes):\n",
    "    \n",
    "#     state = (random.randint(0, 0), random.randint(0, 0))\n",
    "\n",
    "#     # max_steps = batch_size * 100\n",
    "#     while state != (3, 3):  # Suponiendo que la meta es (3, 3)\n",
    "#         action = choose_action(state)\n",
    "#         next_state = take_action(state, action)\n",
    "\n",
    "#         # Obtener la recompensa correspondiente\n",
    "#         reward = rewards[next_state[0], next_state[1]]\n",
    "#         done = (next_state == (3, 3))\n",
    "\n",
    "#         # Almacenar la experiencia en la memoria de repetición\n",
    "#         replay_memory.append((state, action.numpy(), reward, next_state, done))\n",
    "\n",
    "#         # Actualizar el estado actual\n",
    "#         state = next_state\n",
    "\n",
    "#         # Solo entrenar si hay suficientes experiencias en la memoria\n",
    "#         if len(replay_memory) >= batch_size:\n",
    "#             loss = train_step()\n",
    "#             #print(\"training\")\n",
    "\n",
    "#         # max_steps -= 1\n",
    "\n",
    "#     # Actualizar la red objetivo cada 'target_update_frequency' episodios\n",
    "#     if episode % target_update_frequency == 0:\n",
    "#         target_dqn.set_weights(dqn.get_weights())\n",
    "\n",
    "#     # Imprimir la pérdida cada 1000 episodios\n",
    "#     # if episode % 1000 == 0:\n",
    "#     print(f\"Episodio: {episode}, Pérdida: {loss.numpy()}\")\n",
    "\n",
    "# # Mostrar la matriz de recompensas para referencia\n",
    "# print(\"\\nMatriz de Recompensas:\")\n",
    "# print(rewards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio: 0, Pérdida: 1.4323081813927274e-06\n",
      "Episodio: 1, Pérdida: 0.0005665533826686442\n",
      "Episodio: 2, Pérdida: 0.022762442007660866\n",
      "Episodio: 3, Pérdida: 0.0020240885205566883\n",
      "Episodio: 4, Pérdida: 0.0024692073930054903\n",
      "Episodio: 5, Pérdida: 0.008173024281859398\n",
      "Episodio: 6, Pérdida: 0.0033509517088532448\n",
      "Episodio: 7, Pérdida: 0.006048254203051329\n",
      "Episodio: 8, Pérdida: 0.0034438101574778557\n",
      "Episodio: 9, Pérdida: 0.05461530759930611\n",
      "Episodio: 10, Pérdida: 0.02130291797220707\n",
      "Episodio: 11, Pérdida: 1.347800850868225\n",
      "Episodio: 12, Pérdida: 0.3518032431602478\n",
      "Episodio: 13, Pérdida: 0.12438702583312988\n",
      "Episodio: 14, Pérdida: 0.04423190653324127\n",
      "Episodio: 15, Pérdida: 0.1933916062116623\n",
      "Episodio: 16, Pérdida: 439.7012939453125\n",
      "Episodio: 17, Pérdida: 1026.909912109375\n",
      "Episodio: 18, Pérdida: 0.7979012131690979\n",
      "Episodio: 19, Pérdida: 0.10414981842041016\n",
      "Episodio: 20, Pérdida: 410.15643310546875\n",
      "Episodio: 21, Pérdida: 3.1724562644958496\n",
      "Episodio: 22, Pérdida: 7.418040752410889\n",
      "Episodio: 23, Pérdida: 2.4011611938476562\n",
      "Episodio: 24, Pérdida: 5.663890838623047\n",
      "Episodio: 25, Pérdida: 0.43118467926979065\n",
      "Episodio: 26, Pérdida: 581.8002319335938\n",
      "Episodio: 27, Pérdida: 567.1137084960938\n",
      "Episodio: 28, Pérdida: 0.6946985721588135\n",
      "Episodio: 29, Pérdida: 19.18145751953125\n",
      "Episodio: 30, Pérdida: 866.7943725585938\n",
      "Episodio: 31, Pérdida: 311.730712890625\n",
      "Episodio: 32, Pérdida: 326.7095947265625\n",
      "Episodio: 33, Pérdida: 27.653457641601562\n",
      "Episodio: 34, Pérdida: 1.240740180015564\n",
      "Episodio: 35, Pérdida: 1057.9041748046875\n",
      "Episodio: 36, Pérdida: 1.4453229904174805\n",
      "Episodio: 37, Pérdida: 38.37267303466797\n",
      "Episodio: 38, Pérdida: 244.33114624023438\n",
      "Episodio: 39, Pérdida: 244.9739990234375\n",
      "Episodio: 40, Pérdida: 214.56576538085938\n",
      "Episodio: 41, Pérdida: 247.5736083984375\n",
      "Episodio: 42, Pérdida: 182.25961303710938\n",
      "Episodio: 43, Pérdida: 174.72174072265625\n",
      "Episodio: 44, Pérdida: 8.588983535766602\n",
      "Episodio: 45, Pérdida: 158.70001220703125\n",
      "Episodio: 46, Pérdida: 152.0554962158203\n",
      "Episodio: 47, Pérdida: 136.79136657714844\n",
      "Episodio: 48, Pérdida: 284.7237548828125\n",
      "Episodio: 49, Pérdida: 4.151021480560303\n",
      "Episodio: 50, Pérdida: 602.0831909179688\n",
      "Episodio: 51, Pérdida: 190.86463928222656\n",
      "Episodio: 52, Pérdida: 79.16278076171875\n",
      "Episodio: 53, Pérdida: 92.75150299072266\n",
      "Episodio: 54, Pérdida: 43.80889892578125\n",
      "Episodio: 55, Pérdida: 306.0451965332031\n",
      "Episodio: 56, Pérdida: 37.737850189208984\n",
      "Episodio: 57, Pérdida: 48.506431579589844\n",
      "Episodio: 58, Pérdida: 36.02321243286133\n",
      "Episodio: 59, Pérdida: 74.98294067382812\n",
      "Episodio: 60, Pérdida: 39.58407211303711\n",
      "Episodio: 61, Pérdida: 347.464111328125\n",
      "Episodio: 62, Pérdida: 54.47380828857422\n",
      "Episodio: 63, Pérdida: 20.893651962280273\n",
      "Episodio: 64, Pérdida: 48.06077575683594\n",
      "Episodio: 65, Pérdida: 21.091171264648438\n",
      "Episodio: 66, Pérdida: 15.79322338104248\n",
      "Episodio: 67, Pérdida: 7.912862300872803\n",
      "Episodio: 68, Pérdida: 111.33030700683594\n",
      "Episodio: 69, Pérdida: 394.13067626953125\n",
      "Episodio: 70, Pérdida: 13.83005142211914\n",
      "Episodio: 71, Pérdida: 182.08132934570312\n",
      "Episodio: 72, Pérdida: 45.063201904296875\n",
      "Episodio: 73, Pérdida: 62.37398147583008\n",
      "Episodio: 74, Pérdida: 28.94605827331543\n",
      "Episodio: 75, Pérdida: 40.408592224121094\n",
      "Episodio: 76, Pérdida: 5.504002571105957\n",
      "Episodio: 77, Pérdida: 12.859432220458984\n",
      "Episodio: 78, Pérdida: 40.364158630371094\n",
      "Episodio: 79, Pérdida: 17.847841262817383\n",
      "Episodio: 80, Pérdida: 27.82768440246582\n",
      "Episodio: 81, Pérdida: 228.51705932617188\n",
      "Episodio: 82, Pérdida: 155.2711181640625\n",
      "Episodio: 83, Pérdida: 104.10552978515625\n",
      "Episodio: 84, Pérdida: 58.16175079345703\n",
      "Episodio: 85, Pérdida: 9.451288223266602\n",
      "Episodio: 86, Pérdida: 19.113452911376953\n",
      "Episodio: 87, Pérdida: 30.782670974731445\n",
      "Episodio: 88, Pérdida: 10.802343368530273\n",
      "Episodio: 89, Pérdida: 45.22480773925781\n",
      "Episodio: 90, Pérdida: 25.302000045776367\n",
      "Episodio: 91, Pérdida: 295.6624755859375\n",
      "Episodio: 92, Pérdida: 136.01580810546875\n",
      "Episodio: 93, Pérdida: 89.37322235107422\n",
      "Episodio: 94, Pérdida: 101.69792175292969\n",
      "Episodio: 95, Pérdida: 7.387852191925049\n",
      "Episodio: 96, Pérdida: 89.65647888183594\n",
      "Episodio: 97, Pérdida: 65.8560791015625\n",
      "Episodio: 98, Pérdida: 42.48771667480469\n",
      "Episodio: 99, Pérdida: 58.687808990478516\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# Configuración de paralelismo\n",
    "# tf.config.threading.set_inter_op_parallelism_threads(4)\n",
    "# tf.config.threading.set_intra_op_parallelism_threads(4)\n",
    "\n",
    "\n",
    "# Parámetros\n",
    "grid_size = 4\n",
    "learning_rate = 0.001\n",
    "discount_factor = 0.99\n",
    "epsilon = 0.1\n",
    "episodes = 100\n",
    "target_update_frequency = episodes // 10\n",
    "memory_size = 1000\n",
    "batch_size = 16\n",
    "\n",
    "# Definir la red neuronal para aproximar la función Q\n",
    "class DQN(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = tf.keras.layers.Dense(16, activation='relu', input_shape=(grid_size * grid_size,))\n",
    "        self.fc2 = tf.keras.layers.Dense(8, activation='relu')\n",
    "        self.fc3 = tf.keras.layers.Dense(4)  # 4 acciones posibles: arriba, abajo, izquierda, derecha\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return self.fc3(x)\n",
    "\n",
    "# Inicializar la red DQN principal y la red objetivo\n",
    "dqn = DQN()\n",
    "target_dqn = DQN()\n",
    "target_dqn.set_weights(dqn.get_weights())  # Inicializar la red objetivo con los pesos de la red principal\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "# Memoria de repetición (Experience Replay Memory)\n",
    "replay_memory = deque(maxlen=memory_size)\n",
    "\n",
    "# Función para convertir el estado (posición en la cuadrícula) en un tensor de entrada para la red\n",
    "def state_to_tensor(state):\n",
    "    tensor = np.zeros(grid_size * grid_size)\n",
    "    tensor[state[0] * grid_size + state[1]] = 1.0\n",
    "    return tf.convert_to_tensor(np.expand_dims(tensor, axis=0).astype(np.float32))\n",
    "\n",
    "# Función para elegir la acción (con epsilon-greedy)\n",
    "@tf.function\n",
    "def choose_action(state):\n",
    "    state_tensor = state_to_tensor(state)\n",
    "    if tf.random.uniform(()) < epsilon:\n",
    "        return tf.random.uniform(shape=[], minval=0, maxval=4, dtype=tf.int64)  # Acción aleatoria\n",
    "    else:\n",
    "        q_values = dqn(state_tensor)\n",
    "        return tf.argmax(q_values[0])\n",
    "\n",
    "# Función para obtener la próxima posición dada una acción\n",
    "def take_action(state, action):\n",
    "    if action == 0:  # up\n",
    "        return (max(state[0] - 1, 0), state[1])\n",
    "    elif action == 1:  # down\n",
    "        return (min(state[0] + 1, grid_size - 1), state[1])\n",
    "    elif action == 2:  # left\n",
    "        return (state[0], max(state[1] - 1, 0))\n",
    "    elif action == 3:  # right\n",
    "        return (state[0], min(state[1] + 1, grid_size - 1))\n",
    "\n",
    "# Función de entrenamiento con Double DQN y Experience Replay\n",
    "# @tf.function\n",
    "def train_step():\n",
    "    # print(\"train_step\")\n",
    "    minibatch = random.sample(replay_memory, batch_size)\n",
    "\n",
    "    states = tf.convert_to_tensor([state_to_tensor(exp[0]) for exp in minibatch], dtype=tf.float32)\n",
    "    actions = tf.convert_to_tensor([exp[1] for exp in minibatch], dtype=tf.int64)\n",
    "    rewards = tf.convert_to_tensor([exp[2] for exp in minibatch], dtype=tf.float32)\n",
    "    next_states = tf.convert_to_tensor([state_to_tensor(exp[3]) for exp in minibatch], dtype=tf.float32)\n",
    "    dones = tf.convert_to_tensor([exp[4] for exp in minibatch], dtype=tf.float32)\n",
    "\n",
    "    states = tf.squeeze(states, axis=1)  # Convierte de (64, 1, 16) a (64, 16)\n",
    "    next_states = tf.squeeze(next_states, axis=1)  # Convierte de (64, 1, 16) a (64, 16)\n",
    "\n",
    "    # print(\"GRADIENT\")\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Obtener los valores Q actuales\n",
    "        q_values = dqn(states)\n",
    "        q_values = tf.reduce_sum(q_values * tf.one_hot(actions, 4), axis=1)\n",
    "\n",
    "        # Calcular los valores Q objetivos\n",
    "        next_q_values = target_dqn(next_states)\n",
    "        best_next_q_values = tf.reduce_max(next_q_values, axis=1)\n",
    "\n",
    "        # print(minibatch)\n",
    "        target_q_values = rewards + discount_factor * best_next_q_values * (1.0 - dones)\n",
    "\n",
    "        # Calcular la pérdida\n",
    "        loss = loss_fn(target_q_values, q_values)\n",
    "\n",
    "    # Actualizar la red principal\n",
    "    grads = tape.gradient(loss, dqn.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, dqn.trainable_variables))\n",
    "\n",
    "    return loss\n",
    "\n",
    "# Entrenamiento con Double DQN\n",
    "for episode in range(episodes):\n",
    "    \n",
    "    # state = (random.randint(0, grid_size - 1), random.randint(0, grid_size - 1))\n",
    "    state = (0, 0)\n",
    "\n",
    "    max_steps = 1000\n",
    "    while state != (3, 3) and max_steps > 0:  # Suponiendo que la meta es (3, 3)\n",
    "        action = choose_action(state)\n",
    "        next_state = take_action(state, action)\n",
    "\n",
    "        # Obtener la recompensa correspondiente\n",
    "        reward = rewards[next_state]\n",
    "        done = (next_state == (3, 3))\n",
    "\n",
    "        # print([state, action.numpy(), next_state, reward, done])\n",
    "\n",
    "        # Almacenar la experiencia en la memoria de repetición\n",
    "        replay_memory.append((state, action.numpy(), reward, next_state, done))\n",
    "\n",
    "        # Actualizar el estado actual\n",
    "        state = next_state\n",
    "\n",
    "        # Solo entrenar si hay suficientes experiencias en la memoria\n",
    "        if len(replay_memory) >= batch_size:\n",
    "            loss = train_step()\n",
    "            # print(\"training\")\n",
    "        \n",
    "        max_steps -= 1\n",
    "    # print(state)\n",
    "\n",
    "    # Actualizar la red objetivo cada 'target_update_frequency' episodios\n",
    "    if len(replay_memory) >= batch_size and episode % target_update_frequency == 0:\n",
    "        target_dqn.set_weights(dqn.get_weights())\n",
    "\n",
    "    # Imprimir la pérdida cada 1000 episodios\n",
    "    # if episode % 1000 == 0:\n",
    "    print(f\"Episodio: {episode}, Pérdida: {loss.numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -0.1,  -0.1,  -0.1,  -0.1],\n",
       "       [ -0.1,  -1. ,  -0.1,   1. ],\n",
       "       [ -0.1,  -0.1,   1. ,  -0.1],\n",
       "       [ -1. ,  -0.1,  -0.1, 100. ]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58.68781"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q(0,0): abajo\n",
      "Q(0,1): abajo\n",
      "Q(0,2): abajo\n",
      "Q(0,3): abajo\n",
      "Q(1,0): abajo\n",
      "Q(1,1): abajo\n",
      "Q(1,2): abajo\n",
      "Q(1,3): abajo\n",
      "Q(2,0): derecha\n",
      "Q(2,1): abajo\n",
      "Q(2,2): abajo\n",
      "Q(2,3): abajo\n",
      "Q(3,0): derecha\n",
      "Q(3,1): derecha\n",
      "Q(3,2): derecha\n",
      "Q(3,3): abajo\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        policy_direction = tf.argmax(dqn(state_to_tensor((i, j))), axis=1).numpy()[0]\n",
    "        # print(policy_direction)\n",
    "        dict_policy = {\n",
    "            0: \"arriba\",\n",
    "            1: \"abajo\",\n",
    "            2: \"izquierda\",\n",
    "            3: \"derecha\",\n",
    "        }\n",
    "        print(f\"Q({i},{j}): {dict_policy[policy_direction]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_101",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
