{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def print_pretty_matrix(title, matrix):\n",
    "    if not isinstance(matrix, np.ndarray):\n",
    "        raise TypeError(\"Input must be a NumPy array\")\n",
    "    \n",
    "    rows, cols = matrix.shape\n",
    "    print(f\"\\n\\n{title}\\n********\")\n",
    "    for row in range(rows):\n",
    "        formatted_row = \" | \".join(f\"{matrix[row, col]:>10}\" for col in range(cols))\n",
    "        print(f\"| {formatted_row} |\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "rewards\n",
      "********\n",
      "|       -0.1 |       -0.1 |       -0.1 |       -0.1 |\n",
      "|       -0.1 |       -1.0 |       -0.1 |        1.0 |\n",
      "|       -0.1 |       -0.1 |        1.0 |       -0.1 |\n",
      "|       -1.0 |       -0.1 |       -0.1 |      100.0 |\n"
     ]
    }
   ],
   "source": [
    "# Parámetros del Grid World\n",
    "grid_size = 4\n",
    "start = (0, 0)\n",
    "goal = (3, 3)\n",
    "learning_rate = 0.0005\n",
    "discount_factor = 0.9\n",
    "epsilon = 0.1\n",
    "episodes = 10000\n",
    "# Definir las acciones\n",
    "actions = [\"up\", \"down\", \"left\", \"right\"]\n",
    "action_to_index = {action: i for i, action in enumerate(actions)}\n",
    "\n",
    "# Definir la matriz de recompensas del Grid World\n",
    "rewards = np.full((grid_size, grid_size), -0.1)  # Penalización por movimiento\n",
    "rewards[1, 3] = 1  # Recompensa positiva\n",
    "rewards[2, 2] = 1  # Recompensa positiva\n",
    "rewards[1, 1] = -1  # Recompensa negativa\n",
    "rewards[3, 0] = -1  # Recompensa negativa\n",
    "rewards[goal] = 100  # Recompensa por llegar a la meta\n",
    "\n",
    "print_pretty_matrix(\"rewards\", rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\usuario\\AppData\\Local\\Temp\\ipykernel_10560\\562935559.py:79: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  loss = loss_fn(current_q_value, torch.tensor(target_q_value))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "\n",
      "Matriz de Recompensas:\n",
      "[[ -0.1  -0.1  -0.1  -0.1]\n",
      " [ -0.1  -1.   -0.1   1. ]\n",
      " [ -0.1  -0.1   1.   -0.1]\n",
      " [ -1.   -0.1  -0.1 100. ]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Definir la red neuronal para aproximar la función Q\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(grid_size * grid_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 4)  # 4 acciones posibles: arriba, abajo, izquierda, derecha\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# Inicializar la red DQN\n",
    "dqn = DQN()\n",
    "optimizer = optim.Adam(dqn.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Función para convertir el estado (posición en la cuadrícula) en un tensor de entrada para la red\n",
    "def state_to_tensor(state):\n",
    "    tensor = torch.zeros(grid_size * grid_size)\n",
    "    tensor[state[0] * grid_size + state[1]] = 1.0\n",
    "    return tensor.unsqueeze(0).float()\n",
    "\n",
    "# Función para elegir la acción (con epsilon-greedy)\n",
    "def choose_action(state):\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        return random.randint(0, 3)  # Acción aleatoria\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            q_values = dqn(state_to_tensor(state))\n",
    "        return torch.argmax(q_values).item()\n",
    "\n",
    "# Función para obtener la próxima posición dada una acción\n",
    "def take_action(state, action):\n",
    "    if action == 0:  # up\n",
    "        return (max(state[0] - 1, 0), state[1])\n",
    "    elif action == 1:  # down\n",
    "        return (min(state[0] + 1, grid_size - 1), state[1])\n",
    "    elif action == 2:  # left\n",
    "        return (state[0], max(state[1] - 1, 0))\n",
    "    elif action == 3:  # right\n",
    "        return (state[0], min(state[1] + 1, grid_size - 1))\n",
    "\n",
    "# Entrenamiento con DQN\n",
    "for episode in range(episodes):\n",
    "    state = start\n",
    "\n",
    "    if episode % (episodes // 10) == 0:\n",
    "        print(episode)\n",
    "\n",
    "    while state != goal:\n",
    "        action = choose_action(state)\n",
    "        next_state = take_action(state, action)\n",
    "\n",
    "        # Obtener la recompensa correspondiente\n",
    "        reward = rewards[next_state]\n",
    "\n",
    "        # Calcular el valor Q objetivo\n",
    "        with torch.no_grad():\n",
    "            next_q_values = dqn(state_to_tensor(next_state))\n",
    "            max_next_q_value = torch.max(next_q_values).item()\n",
    "\n",
    "        target_q_value = reward + discount_factor * max_next_q_value\n",
    "\n",
    "        # Obtener el valor Q estimado actual\n",
    "        q_values = dqn(state_to_tensor(state))\n",
    "        current_q_value = q_values[0, action]\n",
    "\n",
    "        # Convertir el objetivo a float32 para que coincida con el tipo de datos del tensor de PyTorch\n",
    "        target_q_value = torch.tensor(target_q_value).float()\n",
    "\n",
    "        # Calcular la pérdida\n",
    "        loss = loss_fn(current_q_value, torch.tensor(target_q_value))\n",
    "\n",
    "        # Actualizar la red\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Mover al siguiente estado\n",
    "        state = next_state\n",
    "\n",
    "# Mostrar la matriz de recompensas para referencia\n",
    "print(\"\\nMatriz de Recompensas:\")\n",
    "print(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.029872387647628784"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q(0,0): tensor([[134.2445, 147.0143, 132.8422, 148.6153]], grad_fn=<AddmmBackward0>)\n",
      "Q(0,1): tensor([[150.1373, 164.6835, 134.6351, 165.0654]], grad_fn=<AddmmBackward0>)\n",
      "Q(0,2): tensor([[166.9176, 183.9651, 149.3241, 183.0212]], grad_fn=<AddmmBackward0>)\n",
      "Q(0,3): tensor([[174.6132, 204.0490, 167.4269, 183.5471]], grad_fn=<AddmmBackward0>)\n",
      "Q(1,0): tensor([[134.1128, 162.7217, 144.8471, 164.0856]], grad_fn=<AddmmBackward0>)\n",
      "Q(1,1): tensor([[149.7819, 183.1437, 149.9641, 185.1521]], grad_fn=<AddmmBackward0>)\n",
      "Q(1,2): tensor([[166.7669, 204.9561, 167.0045, 205.0407]], grad_fn=<AddmmBackward0>)\n",
      "Q(1,3): tensor([[184.9557, 226.7475, 185.6731, 204.0625]], grad_fn=<AddmmBackward0>)\n",
      "Q(2,0): tensor([[152.3635, 179.8279, 160.6451, 185.0702]], grad_fn=<AddmmBackward0>)\n",
      "Q(2,1): tensor([[165.8397, 200.1668, 164.0280, 205.9291]], grad_fn=<AddmmBackward0>)\n",
      "Q(2,2): tensor([[184.3490, 224.4703, 184.1567, 226.7924]], grad_fn=<AddmmBackward0>)\n",
      "Q(2,3): tensor([[205.8872, 251.8546, 205.4970, 226.7765]], grad_fn=<AddmmBackward0>)\n",
      "Q(3,0): tensor([[167.7049, 188.1864, 167.2815, 202.0914]], grad_fn=<AddmmBackward0>)\n",
      "Q(3,1): tensor([[182.6288, 207.4502, 180.2224, 226.5085]], grad_fn=<AddmmBackward0>)\n",
      "Q(3,2): tensor([[205.4647, 226.4763, 200.6309, 253.5180]], grad_fn=<AddmmBackward0>)\n",
      "Q(3,3): tensor([[143.7718, 167.3189, 140.5872, 168.5795]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        print(f\"Q({i},{j}): {dqn(state_to_tensor((i, j)))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_101",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
