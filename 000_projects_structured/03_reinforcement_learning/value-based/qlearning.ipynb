{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def print_pretty_matrix(title, matrix):\n",
    "    if not isinstance(matrix, np.ndarray):\n",
    "        raise TypeError(\"Input must be a NumPy array\")\n",
    "    \n",
    "    rows, cols = matrix.shape\n",
    "    print(f\"\\n\\n{title}\\n********\")\n",
    "    for row in range(rows):\n",
    "        formatted_row = \" | \".join(f\"{matrix[row, col]:>10}\" for col in range(cols))\n",
    "        print(f\"| {formatted_row} |\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Q(0)\n",
      "********\n",
      "|        0.0 |        0.0 |        0.0 |        0.0 |\n",
      "|        0.0 |        0.0 |        0.0 |        0.0 |\n",
      "|        0.0 |        0.0 |        0.0 |        0.0 |\n",
      "|        0.0 |        0.0 |        0.0 |        0.0 |\n",
      "\n",
      "\n",
      "Q(1)\n",
      "********\n",
      "|        0.0 |        0.0 |        0.0 |        0.0 |\n",
      "|        0.0 |        0.0 |        0.0 |        0.0 |\n",
      "|        0.0 |        0.0 |        0.0 |        0.0 |\n",
      "|        0.0 |        0.0 |        0.0 |        0.0 |\n",
      "\n",
      "\n",
      "Q(2)\n",
      "********\n",
      "|        0.0 |        0.0 |        0.0 |        0.0 |\n",
      "|        0.0 |        0.0 |        0.0 |        0.0 |\n",
      "|        0.0 |        0.0 |        0.0 |        0.0 |\n",
      "|        0.0 |        0.0 |        0.0 |        0.0 |\n",
      "\n",
      "\n",
      "Q(3)\n",
      "********\n",
      "|        0.0 |        0.0 |        0.0 |        0.0 |\n",
      "|        0.0 |        0.0 |        0.0 |        0.0 |\n",
      "|        0.0 |        0.0 |        0.0 |        0.0 |\n",
      "|        0.0 |        0.0 |        0.0 |        0.0 |\n",
      "\n",
      "\n",
      "rewards\n",
      "********\n",
      "|       -0.1 |       -0.1 |       -0.1 |       -0.1 |\n",
      "|       -0.1 |       -1.0 |       -0.1 |        1.0 |\n",
      "|       -0.1 |       -0.1 |        1.0 |       -0.1 |\n",
      "|       -1.0 |       -0.1 |       -0.1 |      100.0 |\n"
     ]
    }
   ],
   "source": [
    "# Parámetros del Grid World\n",
    "grid_size = 4\n",
    "start = (0, 0)\n",
    "goal = (3, 3)\n",
    "learning_rate = 0.1\n",
    "discount_factor = 0.9\n",
    "epsilon = 0.1\n",
    "episodes = 2000\n",
    "\n",
    "# Inicializar la tabla Q\n",
    "Q = np.zeros((grid_size, grid_size, 4))  # 4 acciones posibles: arriba, abajo, izquierda, derecha\n",
    "\n",
    "# Definir las acciones\n",
    "actions = [\"up\", \"down\", \"left\", \"right\"]\n",
    "action_to_index = {action: i for i, action in enumerate(actions)}\n",
    "\n",
    "# Definir la matriz de recompensas del Grid World\n",
    "rewards = np.full((grid_size, grid_size), -0.1)  # Penalización por movimiento\n",
    "rewards[1, 3] = 1  # Recompensa positiva\n",
    "rewards[2, 2] = 1  # Recompensa positiva\n",
    "rewards[1, 1] = -1  # Recompensa negativa\n",
    "rewards[3, 0] = -1  # Recompensa negativa\n",
    "rewards[goal] = 100  # Recompensa por llegar a la meta\n",
    "\n",
    "for i in range(4):\n",
    "    print_pretty_matrix(f\"Q({i})\", Q[:, :, 1])\n",
    "print_pretty_matrix(\"rewards\", rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q(0,0): [53.14790797 47.99136465 53.01702197 59.44139   ]\n",
      "Q(0,1): [59.35462905 64.00460119 53.22759133 66.1571    ]\n",
      "Q(0,2): [65.96036809 73.619      59.04152736 71.02100539]\n",
      "Q(0,3): [-1.00000000e-02  8.14569914e+01  0.00000000e+00  1.60861756e+01]\n",
      "Q(1,0): [-4.15750249e-02  6.14619011e+01  1.35945340e+01  1.74205995e+01]\n",
      "Q(1,1): [11.23676458 24.98592398 10.53432164 73.61556424]\n",
      "Q(1,2): [65.965345   81.91       64.83141537 80.92382646]\n",
      "Q(1,3): [ 3.82670904 89.8999912  30.40425298 27.59368375]\n",
      "Q(2,0): [-1.99000000e-02 -2.65557183e-01 -1.99000000e-02  7.27325144e+01]\n",
      "Q(2,1): [-0.1         1.46925203  7.88369675 81.90705241]\n",
      "Q(2,2): [73.26805374 72.9608166  72.61068649 89.9       ]\n",
      "Q(2,3): [ 81.67173688 100.          81.76840577  89.4279129 ]\n",
      "Q(3,0): [ 6.53523043 -0.18455718 -0.1        -0.01      ]\n",
      "Q(3,1): [-1.00000000e-02 -1.00000000e-02 -1.00900000e-01  2.74367081e+01]\n",
      "Q(3,2): [81.82913325 13.35369281  3.31599007 10.        ]\n",
      "Q(3,3): [0. 0. 0. 0.]\n",
      "\n",
      "Matriz de Recompensas:\n",
      "[[ -0.1  -0.1  -0.1  -0.1]\n",
      " [ -0.1  -1.   -0.1   1. ]\n",
      " [ -0.1  -0.1   1.   -0.1]\n",
      " [ -1.   -0.1  -0.1 100. ]]\n"
     ]
    }
   ],
   "source": [
    "# Función para obtener la próxima acción (con epsilon-greedy)\n",
    "def choose_action(state):\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        return random.choice(actions)\n",
    "    else:\n",
    "        return actions[np.argmax(Q[state[0], state[1], :])]\n",
    "\n",
    "# Función para obtener la próxima posición dada una acción\n",
    "def take_action(state, action):\n",
    "    if action == \"up\":\n",
    "        return (max(state[0] - 1, 0), state[1])\n",
    "    elif action == \"down\":\n",
    "        return (min(state[0] + 1, grid_size - 1), state[1])\n",
    "    elif action == \"left\":\n",
    "        return (state[0], max(state[1] - 1, 0))\n",
    "    elif action == \"right\":\n",
    "        return (state[0], min(state[1] + 1, grid_size - 1))\n",
    "\n",
    "# Entrenamiento con Q-learning\n",
    "for episode in range(episodes):\n",
    "    state = start\n",
    "\n",
    "    while state != goal:\n",
    "        action = choose_action(state)\n",
    "        next_state = take_action(state, action)\n",
    "\n",
    "        # Obtener la recompensa correspondiente\n",
    "        reward = rewards[next_state]\n",
    "\n",
    "        # Actualizar la tabla Q\n",
    "        old_value = Q[state[0], state[1], action_to_index[action]]\n",
    "        next_max = np.max(Q[next_state[0], next_state[1], :])\n",
    "        Q[state[0], state[1], action_to_index[action]] = old_value + learning_rate * (reward + discount_factor * next_max - old_value)\n",
    "\n",
    "        # Mover al siguiente estado\n",
    "        state = next_state\n",
    "\n",
    "# Mostrar la tabla Q final\n",
    "for i in range(grid_size):\n",
    "    for j in range(grid_size):\n",
    "        print(f\"Q({i},{j}): {Q[i, j, :]}\")\n",
    "\n",
    "# Mostrar la matriz de recompensas para referencia\n",
    "print(\"\\nMatriz de Recompensas:\")\n",
    "print(rewards)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_101",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
