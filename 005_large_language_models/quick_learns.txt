Letâ€™s introduce LoRA (Low-Rank Adaptation of Large Language Models by Edward Hu et al.). This LoRA technique is based on the Parameter-Efficient fine-tuning method of LLM. Using PEFT, we can fine-tune LLM with high modelling performance but it will require training of only a small number of parameters. Another advantage of PEFT is that we can fine-tune any large model using less data.
