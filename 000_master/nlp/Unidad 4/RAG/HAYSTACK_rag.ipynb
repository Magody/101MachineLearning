{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Build with Haystack 2.0.x\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/deepset-ai/haystack-cookbook/blob/main/notebooks/gemma_chat_rag.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" width=\"200\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "<img src=\"https://huggingface.co/blog/assets/gemma/Gemma-logo-small.png\" width=\"200\" style=\"display:inline;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
        "\n",
        "\n",
        "\n",
        "<img src=\"https://meetcody.ai/wp-content/webp-express/webp-images/doc-root/wp-content/uploads/2023/07/LlamaCover-1-1151x648.png\"  width=\"280\" style=\"display:inline;\">\n",
        "\n",
        "<img src=\"https://haystack.deepset.ai/images/haystack-ogimage.png\" width=\"280\" style=\"display:inline;\">\n",
        "\n",
        "\n",
        "\n",
        "We will see what we can build a RAG-based system with the [Haystack LLM framework](https://haystack.deepset.ai/)."
      ],
      "metadata": {
        "id": "I8Q51pZ5NI0L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">>[Installation](#scrollTo=1LRwBMJdF_d1)\n",
        "\n",
        ">>[Authorization](#scrollTo=Lh60ZvTdGDdh)\n",
        "\n",
        ">>[Chat with Gemma (travel assistant) ](#scrollTo=rfW8gRwpGZjc)\n",
        "\n",
        ">>[RAG with Gemma (about Ecuador) ](#scrollTo=7XAtaoEiHE6B)\n",
        "\n",
        ">>>[Load data from Wikipedia](#scrollTo=TCeqQB3kHqcz)\n",
        "\n",
        ">>>[Indexing Pipeline](#scrollTo=h1fvmgsZH0i8)\n",
        "\n",
        ">>>[RAG Pipeline](#scrollTo=Qk8v_s8xIdLV)\n",
        "\n",
        ">>>[Let's ask some questions!](#scrollTo=DrKccbWeMyjB)\n",
        "\n"
      ],
      "metadata": {
        "colab_type": "toc",
        "id": "11BpoHxANG38"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation"
      ],
      "metadata": {
        "id": "1LRwBMJdF_d1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5ggXrtFs18rs",
        "outputId": "802f0892-2a51-439c-dd66-1095cfe28248",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: haystack-ai==2.2.4 in /usr/local/lib/python3.10/dist-packages (2.2.4)\n",
            "Requirement already satisfied: transformers>=4.43.1 in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.2.1)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.1.1)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.44.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from haystack-ai==2.2.4) (3.1.4)\n",
            "Requirement already satisfied: lazy-imports in /usr/local/lib/python3.10/dist-packages (from haystack-ai==2.2.4) (0.3.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from haystack-ai==2.2.4) (10.5.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from haystack-ai==2.2.4) (3.4.2)\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.10/dist-packages (from haystack-ai==2.2.4) (1.26.4)\n",
            "Requirement already satisfied: openai>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from haystack-ai==2.2.4) (1.54.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from haystack-ai==2.2.4) (2.2.2)\n",
            "Requirement already satisfied: posthog in /usr/local/lib/python3.10/dist-packages (from haystack-ai==2.2.4) (3.7.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from haystack-ai==2.2.4) (2.8.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from haystack-ai==2.2.4) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from haystack-ai==2.2.4) (2.3.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0 in /usr/local/lib/python3.10/dist-packages (from haystack-ai==2.2.4) (9.0.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from haystack-ai==2.2.4) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from haystack-ai==2.2.4) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers>=4.43.1) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.43.1) (0.26.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.43.1) (24.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.43.1) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.43.1) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.43.1) (0.20.3)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.5.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (11.0.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers>=4.43.1) (2024.10.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->haystack-ai==2.2.4) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->haystack-ai==2.2.4) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->haystack-ai==2.2.4) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->haystack-ai==2.2.4) (0.7.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->haystack-ai==2.2.4) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->haystack-ai==2.2.4) (1.3.1)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->haystack-ai==2.2.4) (3.0.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->haystack-ai==2.2.4) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->haystack-ai==2.2.4) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil->haystack-ai==2.2.4) (1.16.0)\n",
            "Collecting requests (from haystack-ai==2.2.4)\n",
            "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: monotonic>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog->haystack-ai==2.2.4) (1.6)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from posthog->haystack-ai==2.2.4) (2.2.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->haystack-ai==2.2.4) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->haystack-ai==2.2.4) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->haystack-ai==2.2.4) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->haystack-ai==2.2.4) (2024.8.30)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.1.0->haystack-ai==2.2.4) (1.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai>=1.1.0->haystack-ai==2.2.4) (1.0.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.1.0->haystack-ai==2.2.4) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->haystack-ai==2.2.4) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->haystack-ai==2.2.4) (2.23.4)\n",
            "Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: requests\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.3.0\n",
            "    Uninstalling requests-2.3.0:\n",
            "      Successfully uninstalled requests-2.3.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "wikipedia 1.3.1 requires requests==2.3.0, but you have requests 2.32.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed requests-2.32.3\n"
          ]
        }
      ],
      "source": [
        "!pip install haystack-ai==2.2.4 \"transformers>=4.43.1\" sentence-transformers accelerate bitsandbytes # 2.1.2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Authorization\n",
        "\n",
        "- you need an Hugging Face account\n",
        "- you need to accept Google conditions here: https://huggingface.co/google/gemma-7b-it and wait for the authorization\n",
        "\n",
        "- you need to accept Meta conditions here: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct and wait for the authorization"
      ],
      "metadata": {
        "id": "Lh60ZvTdGDdh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# OPCION 1\n",
        "#import getpass, os\n",
        "#os.environ[\"HF_API_TOKEN\"] = getpass.getpass(\"#Your Hugging Face token\")\n",
        "\n",
        "# OPCION 2:\n",
        "#from huggingface_hub import notebook_login\n",
        "#notebook_login()\n",
        "\n",
        "# OPCION 3: Trabajar con claves secretas (antes otorgar acceso)\n",
        "from google.colab import userdata\n",
        "HF_API_TOKEN = userdata.get('HF_TOKEN')"
      ],
      "metadata": {
        "id": "6lBjYZOC3Ug2"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b> Main clases of the Haystack Framework:</b>\n",
        "\n",
        "- HuggingFaceTGIGenerator (solo generaci贸n, interactions cortas): This component is designed for text generation, not for chat. If you want to use these LLMs for chat, use HuggingFaceTGIChatGenerator instead.\n",
        "- HuggingFaceTGIChatGenerator: para conversaciones: HuggingFaceTGIChatGenerator enables chat completion using Hugging Face Hub-hosted chat-based LLMs.\n",
        "-  HuggingFaceLocalGenerator: HuggingFaceLocalGenerator provides an interface to generate text using a Hugging Face model that runs locally.\n",
        "\n",
        "<b> LLMs:</b>\n",
        "- Google/gemma-1.1-2b-it ---> OK\n",
        "- meta-llama/Meta-Llama-3-8B-Instruct ---> OK\n",
        "\n"
      ],
      "metadata": {
        "id": "N9G79lpgHAhW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chat without a knowledge base"
      ],
      "metadata": {
        "id": "rfW8gRwpGZjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we call the model using the free Hugging Face Inference API with the `HuggingFaceTGIChatGenerator`.\n",
        "\n",
        "(We might also load it in Colab using the `HuggingFaceLocalChatGenerator` in a quantized version)."
      ],
      "metadata": {
        "id": "ADndXQaAO2f5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack.components.generators.chat import HuggingFaceTGIChatGenerator\n",
        "from haystack.dataclasses import ChatMessage\n",
        "from haystack.utils import Secret\n",
        "\n",
        "#  \"google/gemma-2b-it\", \"google/gemma-1.1-2b-it\"\n",
        "\n",
        "def chatGenerator(model=\"google/gemma-1.1-2b-it\", max_new_tokens=350):\n",
        "  return HuggingFaceTGIChatGenerator(\n",
        "    model=model,\n",
        "    generation_kwargs={\"max_new_tokens\": max_new_tokens})\n"
      ],
      "metadata": {
        "id": "DDQgIA8v2GAW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#google/gemma-1.1-2b-it -> SI\n",
        "chatGenerator = chatGenerator(model = \"meta-llama/Meta-Llama-3-8B-Instruct\") # To call the model using the free Hugging Face Inference API with the HuggingFaceTGIChatGenerator\n",
        "chatGenerator.warm_up() # If the url is not provided, check if the model is deployed on the free tier of the HF inference API. Load the tokenizer\n"
      ],
      "metadata": {
        "id": "ajJM-J6tPeoB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To chat with the agent"
      ],
      "metadata": {
        "id": "Mq86mm0uB8zo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "userQuestions = [\"Who is the Ecuador's President\", \"Qui茅n es el presidente actual de Ecuador\"]\n",
        "messages = []\n",
        "for msg in userQuestions:\n",
        "  messages.append(ChatMessage.from_user(msg))\n",
        "  response = chatGenerator.run(messages=messages)\n",
        "  assistant_resp = response['replies'][0]\n",
        "  print(\" \"+assistant_resp.content)\n",
        "  messages.append(assistant_resp)\n",
        "\n",
        "\n",
        "# Para trabajar de forma iterativa con el agente usar, de forma alternativa, el siguiente c贸digo:\n",
        "#messages = []\n",
        "\n",
        "#while True:\n",
        "#  msg = input(\"Enter your message or Q to exit\\n \")\n",
        "#  if msg==\"Q\":\n",
        "#    break\n",
        "#  messages.append(ChatMessage.from_user(msg))\n",
        "#  response = generator.run(messages=messages)\n",
        "#  assistant_resp = response['replies'][0]\n",
        "#  print(\" \"+assistant_resp.content)\n",
        "#  messages.append(assistant_resp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F94aaAGlB8Ix",
        "outputId": "ceacf3ab-96dc-4739-b0e4-95e0f0c99b7a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " assistant\n",
            "\n",
            "As of my knowledge cutoff in 2021, the President of Ecuador is Guillermo Lasso. He is a businessman and politician who has been serving as the President of Ecuador since May 24, 2021.\n",
            " assistant\n",
            "\n",
            "Actualmente, el presidente de Ecuador es Guillermo Lasso.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/inference/_client.py:2232: FutureWarning: `stop_sequences` is a deprecated argument for `text_generation` task and will be removed in version '0.28.0'. Use `stop` instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chat with a knowledge base (RAG)\n",
        "\n",
        "### To create the knowledge base from Wikipedia"
      ],
      "metadata": {
        "id": "7XAtaoEiHE6B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install wikipedia\n",
        "!pip install wikipedia==1.3.1"
      ],
      "metadata": {
        "id": "7Mlv8v0X_qGP",
        "outputId": "1792c072-3e9e-4a3d-9251-1644aa255394",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 681
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wikipedia==1.3.1 in /usr/local/lib/python3.10/dist-packages (1.3.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from wikipedia==1.3.1) (4.12.3)\n",
            "Collecting requests==2.3.0 (from wikipedia==1.3.1)\n",
            "  Using cached requests-2.3.0-py2.py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->wikipedia==1.3.1) (2.6)\n",
            "Using cached requests-2.3.0-py2.py3-none-any.whl (452 kB)\n",
            "Installing collected packages: requests\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.3\n",
            "    Uninstalling requests-2.32.3:\n",
            "      Successfully uninstalled requests-2.32.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "bigframes 1.25.0 requires requests>=2.27.1, but you have requests 2.3.0 which is incompatible.\n",
            "cachecontrol 0.14.1 requires requests>=2.16.0, but you have requests 2.3.0 which is incompatible.\n",
            "google-api-core 2.19.2 requires requests<3.0.0.dev0,>=2.18.0, but you have requests 2.3.0 which is incompatible.\n",
            "google-cloud-bigquery 3.25.0 requires requests<3.0.0dev,>=2.21.0, but you have requests 2.3.0 which is incompatible.\n",
            "google-cloud-storage 2.8.0 requires requests<3.0.0dev,>=2.18.0, but you have requests 2.3.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.3.0 which is incompatible.\n",
            "moviepy 1.0.3 requires requests<3.0,>=2.8.1, but you have requests 2.3.0 which is incompatible.\n",
            "pandas-datareader 0.10.0 requires requests>=2.19.0, but you have requests 2.3.0 which is incompatible.\n",
            "pooch 1.8.2 requires requests>=2.19.0, but you have requests 2.3.0 which is incompatible.\n",
            "posthog 3.7.0 requires requests<3.0,>=2.7, but you have requests 2.3.0 which is incompatible.\n",
            "spacy 3.7.5 requires requests<3.0.0,>=2.13.0, but you have requests 2.3.0 which is incompatible.\n",
            "sphinx 8.1.3 requires requests>=2.30.0, but you have requests 2.3.0 which is incompatible.\n",
            "tensorflow 2.17.1 requires requests<3,>=2.21.0, but you have requests 2.3.0 which is incompatible.\n",
            "tensorflow-datasets 4.9.7 requires requests>=2.19.0, but you have requests 2.3.0 which is incompatible.\n",
            "tweepy 4.14.0 requires requests<3,>=2.27.0, but you have requests 2.3.0 which is incompatible.\n",
            "weasel 0.4.1 requires requests<3.0.0,>=2.13.0, but you have requests 2.3.0 which is incompatible.\n",
            "yfinance 0.2.49 requires requests>=2.31, but you have requests 2.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed requests-2.3.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "requests"
                ]
              },
              "id": "0c323626ad6f4a5dbbe4c517e1f42ebf"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load data from Wikipedia"
      ],
      "metadata": {
        "id": "TCeqQB3kHqcz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sources=\"\"\"2024 Ecuadorian conflict\n",
        "Daniel Noboa\n",
        "Jos茅 Adolfo Mac铆as Villamar\n",
        "Los Choneros\n",
        "Crime in Ecuador\n",
        "Ecuadorian security crisis\n",
        "\"\"\".split(\"\\n\")"
      ],
      "metadata": {
        "id": "c_yp6s9aHnhx"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "from pprint import pprint\n",
        "import rich\n",
        "import random"
      ],
      "metadata": {
        "id": "Uu9PclysIP4T"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the content from Wikipedia:\n",
        "import wikipedia\n",
        "from haystack.dataclasses import Document # Haystack 2.0 uses data classes to help components communicate with each other in a simple and modular way. By doing this, data flows seamlessly through the Haystack pipelines\n",
        "\n",
        "raw_docs=[]\n",
        "\n",
        "for title in sources:\n",
        "    print(title)\n",
        "    page = wikipedia.page(title=title, auto_suggest=False)\n",
        "    doc = Document(content=page.content, meta={\"title\": page.title, \"url\":page.url})\n",
        "    raw_docs.append(doc)"
      ],
      "metadata": {
        "id": "cpTwStuJHvxR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "outputId": "5337b3c3-acda-4c28-b7f0-74125358a99c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024 Ecuadorian conflict\n",
            "Daniel Noboa\n",
            "Jos茅 Adolfo Mac铆as Villamar\n",
            "Los Choneros\n",
            "Crime in Ecuador\n",
            "Ecuadorian security crisis\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'query'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-c922794c1bff>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtitle\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msources\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mpage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwikipedia\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauto_suggest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDocument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"title\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"url\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mpage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mraw_docs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wikipedia/wikipedia.py\u001b[0m in \u001b[0;36mpage\u001b[0;34m(title, pageid, auto_suggest, redirect, preload)\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0;31m# if there is no suggestion or search results, the page doesn't exist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mPageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mWikipediaPage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mredirect\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mredirect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mpageid\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mWikipediaPage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpageid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpageid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wikipedia/wikipedia.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, title, pageid, redirect, preload, original_title)\u001b[0m\n\u001b[1;32m    297\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Either a title or a pageid must be specified\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mredirect\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mredirect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpreload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wikipedia/wikipedia.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self, redirect, preload)\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0mrequest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_wiki_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m     \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'query'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m     \u001b[0mpageid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pages'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m     \u001b[0mpage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pages'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpageid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'query'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Indexing Pipeline (of the knowledge base)"
      ],
      "metadata": {
        "id": "h1fvmgsZH0i8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack import Pipeline\n",
        "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
        "from haystack.components.preprocessors import DocumentCleaner, DocumentSplitter\n",
        "from haystack.components.writers import DocumentWriter\n",
        "from haystack.document_stores.types import DuplicatePolicy"
      ],
      "metadata": {
        "id": "MHVRdvRNHwKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document_store = InMemoryDocumentStore()"
      ],
      "metadata": {
        "id": "EoJjHmLjH7Ag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install mistral-haystack\n",
        "\n",
        "# Docs: https://docs.mistral.ai/api/\n",
        "\n",
        "# To request the API KEY: https://console.mistral.ai/\n",
        "# Free\n",
        "\n",
        "#List of embedders of Haystack: https://docs.haystack.deepset.ai/docs/embedders\n",
        "# https://www.sbert.net/docs/sentence_transformer/pretrained_models.html"
      ],
      "metadata": {
        "id": "oRWG9FRUJEFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Definition of the pipeline:\n",
        "\n",
        "#Opci贸n 1 (no):\n",
        "#from haystack.components.embedders import HuggingFaceAPIDocumentEmbedder, HuggingFaceAPITextEmbedder\n",
        "#embedder = HuggingFaceAPIDocumentEmbedder(api_type=\"serverless_inference_api\",\n",
        "#\t\t\t\t                                           api_params={\"model\": \"sentence-transformers/multi-qa-distilbert-cos-v1\"})\n",
        "\n",
        "#Opci贸n 2 (no): Error code: 429 - {'message': 'Requests rate limit exceeded'}\n",
        "#from haystack_integrations.components.embedders.mistral.document_embedder import MistralDocumentEmbedder\n",
        "#from haystack.utils import Secret\n",
        "#embedder = MistralDocumentEmbedder(api_key=Secret.from_token(MISTRAL_API_KEY), model=\"open-mistral-nemo-2407\")\n",
        "\n",
        "\n",
        "from haystack.components.embedders import SentenceTransformersDocumentEmbedder\n",
        "embedder = SentenceTransformersDocumentEmbedder(model=\"distiluse-base-multilingual-cased-v1\")\n",
        "\n",
        "indexing = Pipeline()\n",
        "indexing.add_component(\"cleaner\", DocumentCleaner())\n",
        "indexing.add_component(\"splitter\", (split_by='sentence', split_length=3))\n",
        "indexing.add_component(\"embedder\", embedder)\n",
        "indexing.add_component(\"writer\", DocumentWriter(document_store=document_store, policy=DuplicatePolicy.OVERWRITE))\n",
        "\n",
        "indexing.connect(\"cleaner\", \"splitter\")\n",
        "indexing.connect(\"splitter\", \"embedder\")\n",
        "indexing.connect(\"splitter\", \"writer\")"
      ],
      "metadata": {
        "id": "fBbmbJX6H9cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indexing.run({\"cleaner\":{\"documents\":raw_docs}}) # Run the pipeline with the content extracted from Wikipedia"
      ],
      "metadata": {
        "id": "ddizanfoIFz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len = document_store.count_documents() # Number of documents in the document store\n",
        "\n",
        "#document_store.filter_documents()[0].meta\n",
        "docs_dict = []\n",
        "\n",
        "for i in range(len):\n",
        "  docs_dict.append(document_store.filter_documents()[i].to_dict()) # Embedding of the first document\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "data = pd.DataFrame(docs_dict)\n",
        "data.sample(5)\n",
        "data2 = data[['id', 'content', 'embedding', 'title', 'url', 'source_id', ]]\n",
        "data2.to_csv(\"corpusSE.csv\", index = False)"
      ],
      "metadata": {
        "id": "MxuGU_GPXFcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RAG Pipeline"
      ],
      "metadata": {
        "id": "Qk8v_s8xIdLV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack.components.builders import PromptBuilder\n",
        "\n",
        "prompt_template = \"\"\"\n",
        "<start_of_turn>user\n",
        "Using the information contained in the context, give a comprehensive answer to the question.\n",
        "If the answer is contained in the context, also report the source URL.\n",
        "If the answer cannot be deduced from the context, do not give an answer.\n",
        "\n",
        "Context:\n",
        "  {% for doc in documents %}\n",
        "  {{ doc.content }} URL:{{ doc.meta['url'] }}\n",
        "  {% endfor %};\n",
        "  Question: {{query}}<end_of_turn>\n",
        "\n",
        "<start_of_turn>model\n",
        "\"\"\"\n",
        "prompt_builder = PromptBuilder(template=prompt_template)"
      ],
      "metadata": {
        "id": "wF7mPcnwIbfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we use the `HuggingFaceTGIGenerator` since it is not a chat setting and we don't envision multi-turn conversations but just RAG.\n",
        "\n",
        "To check documentation: https://docs.haystack.deepset.ai/v2.0/docs/huggingfacetgigenerator"
      ],
      "metadata": {
        "id": "pbvNtRzxPSOe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack.components.generators import HuggingFaceTGIGenerator\n",
        "# jace: google/gemma-7b-it\", \"mistralai/Mistral-7B-v0.1\"\n",
        "\n",
        "def generatorRAG(model=\"google/gemma-1.1-2b-it\", max_new_tokens=500):\n",
        "  return HuggingFaceTGIGenerator(\n",
        "    model=model,\n",
        "    generation_kwargs={\"max_new_tokens\": max_new_tokens})\n",
        "\n",
        "generatorRAG = generatorRAG(model=\"meta-llama/Meta-Llama-3-8B-Instruct\")"
      ],
      "metadata": {
        "id": "B2GTQqgXKMUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack.components.retrievers.in_memory import InMemoryBM25Retriever\n",
        "\n",
        "rag = Pipeline()\n",
        "rag.add_component(\"retriever\", InMemoryBM25Retriever(document_store=document_store, top_k=5))\n",
        "rag.add_component(\"prompt_builder\", prompt_builder)\n",
        "rag.add_component(\"llm\", generatorRAG)\n",
        "\n",
        "rag.connect(\"retriever.documents\", \"prompt_builder.documents\")\n",
        "rag.connect(\"prompt_builder.prompt\", \"llm.prompt\")"
      ],
      "metadata": {
        "id": "lx6PNcm-I1zF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's ask some questions!"
      ],
      "metadata": {
        "id": "DrKccbWeMyjB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_generative_answer(query):\n",
        "\n",
        "  results = rag.run({\n",
        "      \"retriever\": {\"query\": query},\n",
        "      \"prompt_builder\": {\"query\": query}\n",
        "    }\n",
        "  )\n",
        "\n",
        "  answer = results[\"llm\"][\"replies\"][0]\n",
        "  rich.print(answer)"
      ],
      "metadata": {
        "id": "kUsZ5NPzKCEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_generative_answer(\"What did happend on Ecuador during On 9 January 2024?\")\n"
      ],
      "metadata": {
        "id": "LXxvLGuNKrKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_generative_answer(\"Who is the president of Ecuador?\")"
      ],
      "metadata": {
        "id": "NkDQuKnZEmjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a simple demo.\n",
        "We can improve the RAG Pipeline using better retrieval techniques: Embedding Retrieval, Hybrid Retrieval..."
      ],
      "metadata": {
        "id": "8Ap55HKoMSgx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(*Notebook by [Stefano Fiorucci](https://github.com/anakin87)*)"
      ],
      "metadata": {
        "id": "Wo9JfscmOEb3"
      }
    }
  ]
}