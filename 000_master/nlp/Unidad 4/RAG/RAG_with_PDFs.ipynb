{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnQkyFzlti8h"
      },
      "source": [
        "# **Escuela CACIC 2024**\n",
        "### **CURSO 2: Deep Learning**\n",
        "#### **Docentes:** Dr. Marcelo Errecalde y Lic. Horacio Thompson\n",
        "\n",
        "\n",
        "8 de octubre de 2024\n",
        "\n",
        "Contacto: hjthompson@unsl.edu.ar\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKoD_8jgnyE6"
      },
      "source": [
        "# Introducción"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQAxP7_6P_ki"
      },
      "source": [
        "\n",
        "\n",
        "En esta notebook se explora la implementación de un sistema de Preguntas-Respuestas (*Question-Answering*) utilizando Grandes Modelos de Lenguaje (LLMs). Aunque estos modelos son capaces de generar frases coherentes, uno de los mayores desafíos es que pueden devolver respuestas incorrectas o mal fundamentadas, fenómeno conocido como *alucinaciones*. Para mitigar este problema, surge la técnica conocida como *Retrieval-Augmented Generation* (RAG) que combina el poder generativo de los LLMs con la recuperación de información (*Retrieval Information*), permitiendo la integración de información específica y actualizada, y fundamentalmente mejorando la precisión y justificación de las respuestas.\n",
        "\n",
        "Para la implementación de esta notebook, la técnica RAG requiere de al menos las siguientes tres etapas:\n",
        "\n",
        "**1. Base de conocimiento (Knowledge base).** Construiremos una base de datos vectorial que permita indexar y almacenar (de alguna manera) los documentos a analizar, sus embeddings y metadatos. Esto incluye:\n",
        "- a) Leer documentos en formato PDF\n",
        "- b) Chunking: dividir los documentos en fragmentos más pequeños\n",
        "- c) Vectorizar: representar los chunks a través de embeddings, los cuales capturan la semántica y contexto de los textos. Para esto, usaremos un modelo codificador que reciba un texto y devuelva un vector denso de dimensión fija.\n",
        "- d) Construir la base de datos vectorial: indexar y almacenar los chunks, embeddings y metadatos, facilitando la búsqueda y recuperación de chunks relevantes.\n",
        "\n",
        "**2. Recuperación de información (Information retrieval).** A partir de una consulta, se recuperan los chunks más relevantes que contengan información útil para devolver una respuesta apropiada. Para ello:\n",
        "- a) Vectorizar la pregunta: utilizando el mismo modelo codificador que la etapa anterior, de modo que la representación sea coherente con la utilizada para los chunks.\n",
        "- b) Búsqueda semántica: utilizaremos un recuperador de información (retriever) que busque en la base de datos vectorial los chunks más similares a la consulta. Esto, por supuesto se realizará a nivel de embeddings, por lo que comparan vectores; utilizaremos la métrica Coseno, aunque existen otras. Luego, devolvemos los chunks más relevantes, ordenando esta relevancia de acuerdo a la distancia.\n",
        "\n",
        "**3. Generación aumentada (Augmented generation).** Crearemos un prompt utilizando los chunks recuperados y la consulta original; luego indicaremos al LLM que genere una respuesta basándose únicamente en el contexto proporcionado por los chunks.\n",
        "\n",
        "<br>\n",
        "\n",
        "Para probar el método utilizaremos 3 documentos en formato PDFs:\n",
        "- Attention is all you need (Vaswani et. al, 2017) [https://arxiv.org/abs/2306.08302]\n",
        "- Unifying large language models and knowledge graphs: A roadmap (Pan et. al, 2024) [https://arxiv.org/abs/2306.08302]\n",
        "- Guión de la película \"Mentiroso Mentiroso\" (Liar Liar). Draft, 1996. [https://imsdb.com/scripts/Liar-Liar.html]\n",
        "\n",
        "\n",
        "Utilizaremos modelo Gemini (de Google) para realizar consultas y obtener respuestas fundamentadas en el contenido del documento. Finalmente, se comparará este enfoque con el rendimiento del modelo sin aplicar RAG, evidenciando cómo la falta de RI puede llevar a *alucinaciones*.\n",
        "\n",
        "Nota: En la notebook se explican conceptos clave en las celdas de texto y se ha documentado detalladamente el código experimental para facilitar su comprensión y replicación.\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ln8N7fidawat"
      },
      "source": [
        "# Instalación e importación de librerías"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TNqxIV6HwNwu"
      },
      "outputs": [],
      "source": [
        "# Instalación de librerías\n",
        "!pip install langchain\n",
        "!pip install google-generativeai langchain-google-genai\n",
        "!pip install chromadb pypdf2 python-dotenv\n",
        "!pip install PyPDF\n",
        "!pip install -U langchain-community\n",
        "!pip install sentence-transformers\n",
        "!pip install langchainhub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fhzwtV1Xq0Ov"
      },
      "outputs": [],
      "source": [
        "# Importar librerías genéricas\n",
        "import re\n",
        "from IPython.display import Markdown\n",
        "\n",
        "# Importar librerías para la preparación de datos\n",
        "from langchain.document_loaders import PyPDFDirectoryLoader # leer pdfs\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter # dividir en chunks\n",
        "from langchain.vectorstores import Chroma # BD vectorial\n",
        "from langchain.embeddings import SentenceTransformerEmbeddings # Embeddings\n",
        "\n",
        "# Importar librerías para el proceso de Recuperación, Aumentación y Generación\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI # Conexión a Google Generative con LangChain para conversación\n",
        "from langchain.prompts import ChatPromptTemplate  # Crear prompts personalizados para enviar a modelos preparados para conversación\n",
        "from langchain_core.runnables import RunnablePassthrough # Pasar contenido sin modificar (para Pipelines)\n",
        "from langchain_core.output_parsers import StrOutputParser # Estandarizar salidas de los modelos (para Pipelines)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V40puCKJbhZl"
      },
      "source": [
        "Para usar Gemini, necesitaremos crear una API Key.\n",
        "\n",
        "1.   Debes crear tu API Key en: https://aistudio.google.com/\n",
        "2.   Copiar clave en *Secretos* o asignarla directamente en la variable GOOGLE_API_KEY (consejo: oculta tu API Key 🙂)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3allU5OZk48"
      },
      "outputs": [],
      "source": [
        "# Trabajar con claves secretas\n",
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMln4qlZhGVA"
      },
      "source": [
        "# 1. Base de conocimiento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTQRhS33gR4Z"
      },
      "source": [
        "### Cargar PDFs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDFt1HnLbuSt"
      },
      "source": [
        "Cargar los documentos en el entorno actual. Tienes dos opciones:\n",
        "1. Cargar PDFs desde tu ordenador\n",
        "2. Cargar tu Drive usando un path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "id": "9M1mskkQbzvC",
        "outputId": "2d1a2fa9-bbad-4d38-cb03-48db6bfb1256"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-cbefd4b4-f35c-44ed-9beb-96255a85a4e7\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-cbefd4b4-f35c-44ed-9beb-96255a85a4e7\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "CARGAR_DESDE_TU_ORDENADOR = True # @param {type:\"boolean\"}\n",
        "\n",
        "if CARGAR_DESDE_TU_ORDENADOR:\n",
        "  # Subir un archivo PDF desde tu computadora\n",
        "  from google.colab import files\n",
        "  uploaded = files.upload()\n",
        "  DATA_PATH = '/content'\n",
        "else:\n",
        "  # Montar unidad de Google Drive\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  # Path donde están los PDFs\n",
        "  DATA_PATH = userdata.get('PATH_PDFs')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bIxb-O1eg7D"
      },
      "source": [
        "### Lectura del PDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZHTBaA7YKj7",
        "outputId": "5bc62751-c160-4830-a995-468470188cd0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "127"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Leyendo los PDFs del directorio configurado\n",
        "loader = PyPDFDirectoryLoader(DATA_PATH)\n",
        "data_on_pdf = loader.load()\n",
        "# Cantidad de páginas cargadas\n",
        "len(data_on_pdf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmjMOKiT3-EW"
      },
      "outputs": [],
      "source": [
        "# Normalizar espacios en el contenido de cada documento\n",
        "for doc in data_on_pdf:\n",
        "    doc.page_content = re.sub(r'\\s+', ' ', doc.page_content).strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcakUtOigg6A"
      },
      "source": [
        "### Chunking: partición de los PDFs en *chunks* (fragmentos)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PvOcFHoBrka4",
        "outputId": "202b2abc-e977-4025-deb7-9a8be167f914"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "212"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Particionando los datos.\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=2000, # Delimitar tamaño de los chunks\n",
        "    chunk_overlap=100 # overlapping para preservar el contexto\n",
        "    # separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"], -> usar otros separadores\n",
        ")\n",
        "splits = text_splitter.split_documents(data_on_pdf)\n",
        "# Cantidad de chunks obtenidos\n",
        "len(splits)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVUGiN3ngj_T"
      },
      "source": [
        "### Vectorizar chunks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qw8CBVHlhVY2"
      },
      "source": [
        "Cargar modelo de embeddings (*encoder*).\n",
        "\n",
        "Puedes buscar tus propios modelos en: https://huggingface.co/sentence-transformers\n",
        "\n",
        "En este caso, utilizaremos el modelo *paraphrase-MiniLM-L6-v2*, que es una opción adecuada para la comparación semántica de textos y funciona bien en contextos multilingües. También puedes experimentar con el modelo *all-MiniLM-L6-v2*, que está diseñado para propósitos generales."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "2b7df0309786450aa7026bc6748c6710",
            "20b54694d82a4f2e8b8344d6b4dece42",
            "afa886057f904497a8545425ccc9554b",
            "528976674269476c9f785abc1d905deb",
            "4723fc12c187413893cba9a36030b1b1",
            "f839cabfd8714719896ec63f5ec2c1a1",
            "5deea9a58d9244a2ad5256df79aad61a",
            "eabe480281ba400999dd0e423a1c3a8f",
            "e997ef95cbb04145b2436236582a396c",
            "96ab33fef38140a4b591d6a3e9000984",
            "5b676b69524d4612b3e99e0769b4805f",
            "463c3f9642fb42b28e12469a64233605",
            "7fdf6f6ad15445658163698fd32e292c",
            "b1d29366256949b2932638955c135311",
            "fdcd675e07bb400e91aa021dd5be9a98",
            "715290fe97494a14b1fd57af36d1de95",
            "fe76081e53fb40dabf2198815fd14e61",
            "80a8ec9b6b4d4686bf9f2a7874ff49d7",
            "5ecac8bb08d647369b934cedeb7165b6",
            "0909bd3a02334ebb9c937582ab58dbb8",
            "86e71df5d86448e68cced6df2d17c395",
            "b204d3524e194dcf9fae16aa670d2791",
            "b0964db1123547baaf30fd6da3066261",
            "c046d8b46f324f29921da945d6de6cda",
            "4c84fa8c0bc349cebdea3e9401898ad2",
            "aafd9e107ea74978bcbb0ea45d79b25c",
            "49e3fa3e17d54a31adf2d47741b5c3ab",
            "49766cd63f1b4fa9b495b1f1d0144bf9",
            "827d64cdb37d4cbeb62f0795a8cdc978",
            "0965899c059e4c119ed765609df5392b",
            "45f46296b31d4e4899703b766ae79c1b",
            "9775c53802844b03ae72e453b49a9a38",
            "f3490c19f41445b48240d0f709844fc1",
            "a90f3ade3a414b04b9e0cd0cb5900e3b",
            "97cba74d8f294ba5abd5470038ddb6a7",
            "781e273cd4be47fbb9b47ee2de35427b",
            "2e48bed3ffa34fbcacc7dec59323b812",
            "dec19e176391414ea4ab2ccc5f2d463b",
            "dcbbc4c11b2041bda4a07bfcc5e8289c",
            "28ae155a4a7e4d63a3717d301543ce5e",
            "d639022bcc384ad88a9c2e0d2f2bf11c",
            "fa26418b38124340b93936c17cd2be5d",
            "896e4cf7c31847d0a2d30da8177f1ec6",
            "0c8ccbb4cda746b29dfcfedc37a85399",
            "7ac12e8d5c254dfd9979ca6c4bdebdd9",
            "6f9984cf059943b3a5e75eae1858d0ce",
            "f20d9cf4ccf54243a29f87402f7a166d",
            "e9faebfd14e3495fbfeebdeaa356f047",
            "9d37f3e7718b4c0d84cd7d15d547e459",
            "d6508d745c3e4bc2a244f67a6027a631",
            "f2917330828b49dd8a5197515969c663",
            "48287d97529b4cf09d62261b91ee94a4",
            "299d0bd58ec74160859c7f653c8b60f4",
            "8ec5c4019969406dba9ddeba56ebb33f",
            "ea9409d2e3c8494fa6035b861f388a77",
            "43a31d5fe545438b97dc1c1300d19d9e",
            "c81bf00fcbfa42eeb5a52f9520bf0532",
            "aa10ca246aeb481ea16c996ca75c5745",
            "962eccad21a44bc6b2d6c0f9418a1ec0",
            "60bd72fecf034ae5a347fc59043962ad",
            "0b4f48185dea46f5beb00314869eb15c",
            "f7a52ad6c7334d628506bba2e2eb84cf",
            "8d610311f26b4c6ea90c0063047b4e22",
            "ef45cb0544f149ec8e3c50bfc49bf08d",
            "284e9ae9d8e74ae48f28bb20772e0c6a",
            "cb7b9dc04e8b4cd1ae8776e5e907725b",
            "fb70204ec341483bb836299ac7f1ed47",
            "9ad9cbbd434249ea8b405de17cc380ff",
            "0f00ac0bf22b47f293e2c640c2a63364",
            "4118a73b2ec640ce84d6ddd753b15618",
            "632d369a72c94f3ba73080f3851ae559",
            "18a3056a6ea64286826a3afc56add186",
            "2e8d61bc2efc4dfe845141b1fbb22d50",
            "1cc487bffce94a148a223248509dcde9",
            "3125fdb19f04498fb8e8e63dd1d624b9",
            "c34c4a3d6a5a4efc9b4a100ca7130c0c",
            "61efc3e2bf0142eaa07adb0fd94e6d24",
            "73dcddfce36246d888f05cd3e76cddd0",
            "d4a0d6c3fad64f6c913c2c7e7060ca4d",
            "a94f5a15af6b41da836f4a1483070a9c",
            "917ad605a4a74b998868722a043f8ef0",
            "955e69c14d754139a6041b7b0fdaa935",
            "46e36b3cae2343d5993716151d3990ff",
            "0a67cb9a26544b64a71ae32d07c13b7b",
            "4513a31307844856a29c9b128586c73a",
            "1ba8972eb72c48b5bb88d57c7f17c53c",
            "39ae3c88984545e58886c565d20b1df6",
            "af6dad1da929414581a8bcc32ae11968",
            "127076aed02d45e9a831961f4b865c44",
            "dd4d7eb9cb7247dfb3b3b50680690a4e",
            "070c9ddacac8462c85d2c9af6ae07b0b",
            "92406a75e32f4bb180076e706787f3a6",
            "6070d82d367f436bbc4a50d750168f21",
            "3b0808a452904663a73ada839caf0984",
            "094ee7646d86499983f4eef515f6c8ea",
            "dd8ae2f7387d499cb22d7999e58e773f",
            "3480bd6c704440feaf23aea2974ce263",
            "aaca50a213074637af60d052b9581fa2",
            "a8925c0da5a540589a2229b991df63c5",
            "bd133b9d08ba49bd9f5cc1a5479b9fde",
            "a65eca6af86d4706bd6cbc90858b80f5",
            "ea4952cb69ff4b5f86a84360cfe38529",
            "a6256613007a4e2e8e7542e1fbc8bd53",
            "b7da521dc25f4b599467ed55511fd1af",
            "8832d707b4e04bb9ad36ae24f9c7e7e4",
            "8d16a6f846a5411fbd9e9576b145496a",
            "f77e5910f7694623ba23ea499edd1f61",
            "1adf1c2e50c84847953aee88d5d7566e",
            "f9ca5420060d4dcf9bf0dfb61cb73d45",
            "8de8e2973a9c469bbc193fee8ad2cede",
            "a4ae9bdde9664a6abda8da1a81007018",
            "c2690ecb2344402493df7eeb8c7ab73f",
            "0631426403144f98bd75fcfef573e217",
            "f71b45218df34ca48607e5737a7648f5",
            "3dbb2b6833a14fb684a9d679a46edd83",
            "c96b87e011c3458397001776a43a3649",
            "e77cbe7640844155b1600143347d8c18",
            "ea48145d9a2f49c0ad56f99411d9c043",
            "599144a6612642fb98dfe66d210c571b",
            "3851b7a2ceaa4c3c8609b6744187025a",
            "825509b73b8f47a994d69e083f8322ae"
          ]
        },
        "id": "b24b6wzKxvWD",
        "outputId": "b1f1b2aa-aa07-41bf-d29b-697bb2b21998"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-8-3a5d156374f2>:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embeddings_model = SentenceTransformerEmbeddings(model_name=model_name)\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2b7df0309786450aa7026bc6748c6710",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "463c3f9642fb42b28e12469a64233605",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b0964db1123547baaf30fd6da3066261",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/3.73k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a90f3ade3a414b04b9e0cd0cb5900e3b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7ac12e8d5c254dfd9979ca6c4bdebdd9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "43a31d5fe545438b97dc1c1300d19d9e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fb70204ec341483bb836299ac7f1ed47",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/314 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "73dcddfce36246d888f05cd3e76cddd0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "127076aed02d45e9a831961f4b865c44",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bd133b9d08ba49bd9f5cc1a5479b9fde",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a4ae9bdde9664a6abda8da1a81007018",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Instanciar el modelo de embedding\n",
        "model_name = \"paraphrase-MiniLM-L6-v2\" # \"all-MiniLM-L6-v2\"  \"paraphrase-MiniLM-L6-v2\"\n",
        "embeddings_model = SentenceTransformerEmbeddings(model_name=model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsbFtWiuhpK5"
      },
      "source": [
        "### Base de datos vectorial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdH5eXZphgJI"
      },
      "source": [
        "Ahora instanciaremos la base de datos vectorial (`vectorstore`), la cual será almacenada en el entorno actual.\n",
        "\n",
        "Usaremos **Chroma** ([documentación](https://docs.trychroma.com/integrations/langchain)), que es una de las herramientas más utilizadas para almacenar y recuperar información de manera eficiente.\n",
        "\n",
        "La función *Chroma.from_documents(...)* permite instanciar `vectorstore` pasando como parámetro los chunks obtenidos previamente (`splits`) y el *encoder* (`embedding_model`).\n",
        "\n",
        "*   Por defecto, esta función utiliza la función *Cosine* para comparar vectores y el algoritmo  *Hierarchical Navigable Small World* (HNSW) para almacenar los vectores; este último permite optimizar la creación de indices, búsqueda y recuperación de documentos.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1sylHC2dyLVX"
      },
      "outputs": [],
      "source": [
        "# Configura el path a la base de datos\n",
        "path_db = \"/content/db_vectorial\"\n",
        "!mkdir path_db\n",
        "db_name=path_db.split(\"/\")[-1]\n",
        "\n",
        "# Almacenamos los chunks en la base de datos\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=splits,\n",
        "    embedding=embeddings_model,\n",
        "    persist_directory=path_db,\n",
        "    collection_name=db_name,\n",
        "    )\n",
        "    # collection_metadata={\"hnsw:space\": \"cosine\"} -> por defecto usa cosine (sólo angulo, i.e., dirección)\n",
        "    # Otras métricas posibles:\n",
        "    #     collection_metadata={\"hnsw:space\": \"ip\"} -> producto interno (inner product)\n",
        "    #     collection_metadata={\"hnsw:space\": \"l2\"} -> distancia euclideana\n",
        "    # HNSW (Hierarchical navigable small world) -> Sirve para el almacenamiento de vectores,\n",
        "    #                                              Optimiza cómo se indexan y buscan los vectores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYbk4HdBqBNn"
      },
      "outputs": [],
      "source": [
        "# Eliminar BD (igual queda un recurso habilitado, lo mejor es cambiar de nombre de la BD y listo... o reinciciar el entorno)\n",
        "# del_op = \"y\" # @param {type:\"string\"}\n",
        "# if del_op.lower() == 'y':\n",
        "#   import shutil\n",
        "#   print(\"Eliminando BD\")\n",
        "#   # Elimina el directorio existente si es necesario\n",
        "#   shutil.rmtree(path_db, ignore_errors=True)\n",
        "#   del vectorstore\n",
        "\n",
        "# Listar funciones de vectorstore\n",
        "# dir(vectorstore)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_I4BfcLijpjE"
      },
      "source": [
        "Inspeccionemos la base de datos vectorial creada.\n",
        "\n",
        "Observaremos un diccionario formado por los indices (ids), embeddings, metadatos, documentos, entre otros elementos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6a_0qJXvxvne",
        "outputId": "369d8c2b-1bf1-4eb6-f77c-1ff5d520399e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['ids', 'embeddings', 'metadatas', 'documents', 'uris', 'data', 'included'])"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Ver el formato de vectorstore\n",
        "vectorstore.get().keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e541xnVj9xy"
      },
      "source": [
        "Podemos ver el contenido. Por defecto, se muestran los indices, metadatos y documentos almacenados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmpjjt-G14Tg",
        "outputId": "3b0d0394-ac13-408e-f7a1-1b715406a079"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'ids': ['f2bcc973-adca-4d8a-9de3-e91c00eb6a0e',\n",
              "  '272f7840-e978-4776-b45c-3d1704f9c26b',\n",
              "  '903a3185-d3f0-45e5-ac08-eea88261c9e2',\n",
              "  '3650e60e-20a1-4d69-bc5b-7cd73b7403de',\n",
              "  '7dd66b3a-4e0b-4551-a48c-eac35102f123',\n",
              "  '5c1f0c6e-5c13-4ad9-9497-f0e319481959',\n",
              "  '70fb7df7-ac23-42f8-8e86-9675178a4f65',\n",
              "  '1c1634c6-5ca4-4533-9e88-4d4c9b960dad',\n",
              "  'd803d230-b7ed-46b4-80fa-fc618ca2ce18',\n",
              "  '76f6151e-55e5-4aa2-82a5-cdefddbeeeaa',\n",
              "  'e0bef8e7-9bb9-4282-891a-0574367b82bb',\n",
              "  '6b932455-b420-42f7-a603-93c628088e5d',\n",
              "  '45ee964c-fd6d-4af8-8436-30c40358266e',\n",
              "  '9b2b93b6-4f73-4004-a795-685cec056b0c',\n",
              "  'b48d7681-9f04-4610-82a4-7b408aeef5ca',\n",
              "  '1eaec0a9-9b26-4cb1-bc7c-005333c1a146',\n",
              "  'a0bb99a2-68e2-49f8-86a0-55590fd9b152',\n",
              "  'ca93f828-181a-4688-a980-d5c5c582caa3',\n",
              "  'f6797a9d-45a3-4f95-a94e-fc15239bc7fe',\n",
              "  '0655f56b-6699-4591-a18b-e0e0c5f61f9a',\n",
              "  'd86cda39-8aca-4588-b3a3-d9c185243f3c',\n",
              "  '6159b42b-d783-44e2-a1d6-62ce4f36bdd7',\n",
              "  'ce1500ba-edc5-45ad-8778-f0c98f194521',\n",
              "  'ddef604d-b192-40ca-b576-2ad93e8693dc',\n",
              "  '4e8610cc-cede-4273-b1ec-556557bfe5dc',\n",
              "  '28b273dd-b3a7-4980-bc63-8fb5da7b43ff',\n",
              "  'bab4580b-7d2e-4c39-8e70-a251f0c5ec13',\n",
              "  '8c894f29-6c29-476e-9776-64b386ff73c7',\n",
              "  'bdeded0a-8863-4d00-adf8-e8de38378c7d',\n",
              "  '838a8454-38cc-4024-85e3-53e117c79b0e',\n",
              "  'f7ba51a4-8e5a-4bae-836e-a708c5b844f3',\n",
              "  'd7d00130-181c-4554-a018-0ade1e0322c6',\n",
              "  '16e5b375-0cb8-4c8d-9227-4f59754eefac',\n",
              "  '613cb1b2-9b79-4b02-be85-88ca6fb3c7ba',\n",
              "  '675ace4f-5885-46df-9da9-5c4386525b8b',\n",
              "  'f8d15738-5df9-4c81-9f71-227f273df4dd',\n",
              "  '0ab87dae-2dbc-4cba-ab69-db79815905bb',\n",
              "  'ce94e493-4486-410f-a458-c7c301c35fac',\n",
              "  'c2d24eea-cae6-419c-8686-5e93f6859f0f',\n",
              "  '35fd9048-5e47-428f-a3bb-31276f099a19',\n",
              "  'd961d737-81d1-43ac-8cb7-adb5150feb7c',\n",
              "  '62d8ab42-a1c2-4fe4-8127-bb1b2fcb1f9d',\n",
              "  'a68db237-f608-42ba-937b-04ced2b53a6b',\n",
              "  '2e62b2de-b355-416b-a038-35166c45b296',\n",
              "  '125ea20a-b236-4ef3-ae82-360e2da55559',\n",
              "  '2e60e5d2-6aa2-45b6-a301-660d914fc0f2',\n",
              "  '7fb4edc7-0b8e-474e-b9c6-4b86a3401143',\n",
              "  '6e65d6f9-70b0-45ec-a3ba-e6478c519c38',\n",
              "  '787c2596-7771-4eb0-b9c0-ac281e5fd9d8',\n",
              "  'caee7a2f-d0fe-4e17-a7c3-42dbed770722',\n",
              "  '79f2b66e-6f0a-49a7-a41b-ee3e324997e7',\n",
              "  '6085cb9f-4948-4661-b64a-30b3c4279719',\n",
              "  '941068b4-759a-4621-aed3-b4a606bf0aac',\n",
              "  'e5a51e58-e2e9-4d26-8662-71affb069d75',\n",
              "  '3298c7f8-ce91-4128-bd12-b69a5334336d',\n",
              "  'f15043ed-2d24-4259-8fb2-aa2ab509d6b0',\n",
              "  'e020ac7e-4410-40aa-bfe0-efb86aef14d8',\n",
              "  '78d5e9f8-bcca-44d9-9e24-c144e5582d3b',\n",
              "  'fbe0e87d-b80c-42e1-ba7d-45db6817b1dc',\n",
              "  '49d71362-9468-44d2-a997-86cb03b6e181',\n",
              "  '512eece7-5c2f-4d43-ac26-623dc5d090ce',\n",
              "  '0ede07ba-f3d9-4531-80a2-8a3fbc78c048',\n",
              "  'f3d06b8f-3c6a-4ebd-b015-e4e71c2cc38a',\n",
              "  '83b2a417-56ec-4a6b-9c9d-17bed87af416',\n",
              "  '6aa50ec8-7785-4844-8c6b-d6ca2a43e11b',\n",
              "  '155453ba-26bd-47e1-a565-0d2ad2231fce',\n",
              "  '94a71669-b76e-418d-92f6-2fa6724c2c8e',\n",
              "  '7d7fccfa-d2fa-4f8c-b5d1-e9579820411b',\n",
              "  '001e4834-2b9e-4b0c-a6f4-4792e7b47ab0',\n",
              "  '362acf07-90fd-4c01-9c57-576c89cd00a8',\n",
              "  '00492803-a03d-4f65-bb5a-e0b87d883163',\n",
              "  'ddfd6f09-55ce-4d0b-b09a-76b778db6aa9',\n",
              "  'be603419-0ee4-4617-848e-b04c0427ebf4',\n",
              "  '9ba32db6-b42e-4495-9481-d394ac5b5ebb',\n",
              "  '0eb28c6f-86da-4d5f-b0a1-18bc33684ccd',\n",
              "  '13bdd967-c198-484d-8d60-67059067d207',\n",
              "  'df0a6b61-87c9-4b7f-9dbc-20c6c2b90f44',\n",
              "  'b9c1c652-46fa-40ba-bcc1-949525cf1c37',\n",
              "  '3d25dd9e-a039-4ce3-b961-a11c933c13ea',\n",
              "  'ede18218-c5ed-4416-bc9e-0fa7dbbfb080',\n",
              "  '89de46a2-e92a-4542-bf19-96d3a5660354',\n",
              "  'bc215851-e0bd-40b0-a0af-047e17be5822',\n",
              "  '1e046953-e536-42f8-969f-f8e963e44148',\n",
              "  'b5225307-e70c-4a41-9c6e-133c5adbc9ac',\n",
              "  '323f6e06-b179-44c6-b5f7-b7711630311b',\n",
              "  '37422d28-3d52-4d88-b2e5-eeb2730edb75',\n",
              "  '38d6d7b8-7d7d-4e75-a485-e5e4f8886900',\n",
              "  '46a082a4-b722-4b23-8d29-440dd8b37c67',\n",
              "  '3cefb567-ef04-4a21-9dc7-8263c286c61e',\n",
              "  'a180d3c4-4042-4bb8-9aeb-c5bbb9a76aaa',\n",
              "  'd073875b-3c1e-404a-9542-d62f9d4060d4',\n",
              "  '10d24a69-04e5-45b7-b26b-2a4e843e6c65',\n",
              "  'ebcf168d-759e-4f0a-a5ff-fd93e77bb592',\n",
              "  '9ce005f4-5094-43f5-aecb-e6958956eddf',\n",
              "  '47addbc4-d25d-4912-a7ce-98a62c372f66',\n",
              "  'bdb16411-1962-49ba-841a-736b83838b60',\n",
              "  '021d71cb-c4fa-40ac-8bac-012347e1f704',\n",
              "  '14904b96-f71a-4f10-8720-04563650d443',\n",
              "  'cceed3d4-e377-4381-9863-978ba89552c8',\n",
              "  '981e59ac-31fc-4466-a5a7-2d4f95b73840',\n",
              "  'dd03eaec-a344-49c8-a0e9-3bcf6a0104d7',\n",
              "  'e207752d-8966-4b0c-b038-64e77710d008',\n",
              "  '55d57644-f229-43ab-9821-1ff5e64dbc4c',\n",
              "  '1041aece-c9b4-4a80-9b17-85704577532d',\n",
              "  '9d1d8b87-0d0c-465d-8e46-5dc57c44afd8',\n",
              "  'aa4f2ef8-08a4-458d-a113-3c716c34b528',\n",
              "  '7165fb06-fa86-44d3-98ce-1a62ce93269a',\n",
              "  '54b73a85-28ee-43ef-ba9f-d4672ba8a81a',\n",
              "  'f64521ba-679f-4cae-afe8-9bc7db77e9df',\n",
              "  'abdb248f-92a7-42b0-b1ad-6e8e3ecf9324',\n",
              "  '14a57fb0-757b-4c32-a764-e18ff3586e95',\n",
              "  '1f397539-38fc-4633-a0d9-b55b75bbe9b0',\n",
              "  '8eace4c2-531f-468d-ad0f-dcb9d50dd338',\n",
              "  '11eaa94f-513d-4fd4-ae11-2a59bd9dd187',\n",
              "  '7a13e798-d0c9-4373-8d74-cf0d91fcd734',\n",
              "  '8dca67c1-d4d2-45cb-ace3-827784cc2a4b',\n",
              "  '20359e60-4e13-49ca-994b-e88ffdf8dcc8',\n",
              "  'c84e1adc-4785-4175-911e-b0e3fc2a91a5',\n",
              "  '50d4344a-6360-47ce-81c3-5e7f771f5adc',\n",
              "  '470f3746-5fba-46ef-8961-54c52345c612',\n",
              "  'd5a5dd39-740b-43d3-9f48-d354c8c78e8a',\n",
              "  '30a745f0-16d2-4249-ac75-e389ae42c20e',\n",
              "  'c9a99770-e2a3-42fd-af69-3942a4aeb9ec',\n",
              "  'f7bd5584-8c55-4b5c-b455-96ebdcb92bb1',\n",
              "  'e63b219a-43a7-4907-b6dc-82437c97597d',\n",
              "  '2586652a-0175-4923-9995-b8878e40ddf1',\n",
              "  '51d6d852-e56d-4751-aa34-cf1b2dbf0cf8',\n",
              "  '8d21ed76-8b0d-458b-a967-e6ea1834738d',\n",
              "  'f6b18b0e-9a02-43c9-9f6b-aaf9b8bd9c03',\n",
              "  'd9f03953-c5fa-420f-9564-716c0e477bb2',\n",
              "  '2773fb26-f496-4a29-958f-382be0eebc8c',\n",
              "  'e598a39b-7e32-46bb-9ac8-cdf7a2fcd5c3',\n",
              "  '7510297c-2070-4586-8f20-85b4f2a4c434',\n",
              "  '0f99e198-620f-4c0b-8765-c9fe6fd8ac38',\n",
              "  '60fc0ecf-0895-46b8-befc-c22418ff4607',\n",
              "  'ed2ccedb-2a4b-421a-8822-c253fff40971',\n",
              "  'aab957f6-5b62-4c1a-9745-aa10fa2edf36',\n",
              "  'f8da2ce5-9bef-40be-bb0b-ace3d832e107',\n",
              "  '14b1844c-288b-426d-b590-b5a5c9f09aac',\n",
              "  '30afca6e-0948-4ca3-ae01-bac8030937d5',\n",
              "  '4e01bd39-4a02-40c9-88ea-0bcca113d98a',\n",
              "  'e8f9ded0-8a74-4dd2-9bc7-438248901a30',\n",
              "  'f19e684b-18b0-4a58-8cc2-f4969a1e6855',\n",
              "  '4ffb19ba-0345-4edf-9111-be295f3df541',\n",
              "  'b6cb7ed0-cc28-421f-b06b-f02e24eea51a',\n",
              "  '120535df-3fa2-4150-bff5-ffd15944ebdc',\n",
              "  '5d25156b-e9f5-46a1-95fa-d4fdf02d0f30',\n",
              "  'f530dbdd-d89b-4df4-878e-328012544be8',\n",
              "  'be907c53-df00-45e1-bd0d-7f855f7e7087',\n",
              "  '6f4a4050-843c-4bed-b175-d6670e4d02f3',\n",
              "  '7ca2dc59-16b9-43a4-9268-5799a686f8c5',\n",
              "  '0f9647a7-2ed1-4073-91d6-068bbc382316',\n",
              "  'dceff1fe-d93d-464a-93be-a912bfb8589a',\n",
              "  '194446e7-732b-4a40-9276-0bb59f600275',\n",
              "  'e10a4ce1-fc64-4447-ac57-f212f22c6357',\n",
              "  '796aeeaa-6345-445e-9cd7-ccd3a99694db',\n",
              "  '7facafe2-e968-45f6-af1c-abb33fb79079',\n",
              "  '3cd298d7-fe2c-4820-83b5-d84fcb69f217',\n",
              "  'e7adbfa2-0c84-43a8-a52d-22d2b9664c2f',\n",
              "  '132820c1-84e3-409b-8a43-6aa33329c37a',\n",
              "  'fe56ac88-5569-4a1d-b084-07528aa70e50',\n",
              "  '3040eb51-f75c-4743-9eff-a258130a7d3b',\n",
              "  '9e4e2ff9-8002-4e35-9ff8-0106e923dfd1',\n",
              "  '18a9a1a5-2701-48e9-bd6c-335d3f326c9b',\n",
              "  '99aa5fb7-85c7-4caf-82a4-8d4024ca106b',\n",
              "  'c8cd99f2-6f83-4b57-bbea-0a5039d5b5ec',\n",
              "  '2d41d54a-048f-485e-ae76-062129794d9c',\n",
              "  '8241bf7b-089e-4a9d-b79d-2d1643baaf35',\n",
              "  'f031b8e8-49d9-4b0a-bbea-ca42d6e1f6fa',\n",
              "  '7a104406-9a7e-4cb9-8be6-8f473fe21f17',\n",
              "  '7923f71e-c34d-4dde-b40d-67aa662e806a',\n",
              "  'f76836e2-a4f6-420f-af78-b013797fe38c',\n",
              "  'a245e87e-d6cf-4584-9e0d-1bc90a6f1b8f',\n",
              "  'ccc3fdaf-a377-460e-8137-3e4abdee2d57',\n",
              "  'cbffab72-96fd-47f8-a35b-6cea7c560d50',\n",
              "  '2ad231e9-453f-463b-8648-e3f5aae39380',\n",
              "  '3143b2d2-2ec5-4f9d-91ae-c4a9608385c5',\n",
              "  '0e74fc55-c6a3-4635-89c7-f26ce8773778',\n",
              "  '7ee068a8-cfcd-42d4-a084-60b263ea4d64',\n",
              "  'bcf0bdf8-04b7-42c9-af2f-972a3bee55c2',\n",
              "  '225bd966-7678-4d95-aa6f-d8030f9f507b',\n",
              "  'e2aa51b5-639e-40b5-9b93-9dc641036c4a',\n",
              "  '53d4a03d-5f3d-45d2-8a2c-b18d1512d445',\n",
              "  'c31383db-ab75-418a-bd6a-8f3043070174',\n",
              "  '33b03e81-acb9-45fb-a860-4def60f6f1a5',\n",
              "  '30334e2c-cc0c-4f96-a0e7-4904f592bac7',\n",
              "  '1762ace1-8f26-4558-a7a5-5e9298e1e00b',\n",
              "  '37337446-fbd4-484c-a3e7-97162432c77f',\n",
              "  '21143e7a-bdb2-48a3-b083-25d25e9f3297',\n",
              "  '5c36ab96-695c-4073-a663-96a05d055705',\n",
              "  '7bd98a14-8e70-47b0-b484-6811d0a4c0d2',\n",
              "  '579b83b8-7979-4585-8d99-ced65aa1b53e',\n",
              "  '3b823a1d-1d88-4e54-ba24-47d70d83c601',\n",
              "  'e9ea5af0-aef7-46a5-a97c-024aa3690eba',\n",
              "  'ed6ad45c-1bc7-47d3-adab-e49b33d1da41',\n",
              "  '64d92be6-1ba2-4452-a74f-27f6f5a5f126',\n",
              "  'b578b916-c6b4-4d46-bbc9-f1c3b0841ba0',\n",
              "  '770ab171-5a2b-4a6c-b3a2-52a41dc418fe',\n",
              "  '675b4c15-49fa-4b95-89c6-8724123e0b16',\n",
              "  '1610391e-cf56-4435-9fd8-dd64a9e8b487',\n",
              "  'c932096c-4c2d-4566-9b99-7a5f88c90101',\n",
              "  '99a6b7e7-2322-4deb-be20-32e03e792d63',\n",
              "  '6626a4d8-1aeb-400e-957d-19a6078765ec',\n",
              "  '7e246f64-8532-4e0f-8a16-33f89cec5b3d',\n",
              "  'e2b0c93b-3f78-4cfd-8a34-9ee3fadef40f',\n",
              "  'd6254949-78d9-4138-ba48-d3feda4a6c26',\n",
              "  'ce786456-0f16-44b3-a553-d185d775ef90',\n",
              "  '742a95cb-5b73-4399-ac8a-6737ea786b95',\n",
              "  '27dfbb60-25a8-4269-9559-32d26610afb3',\n",
              "  '7564f34f-bb93-4efc-b9c7-1d76e51adb5f',\n",
              "  '2cdd2377-a138-4cde-b23c-dc2aa70a1c35',\n",
              "  '3cdb9029-945a-4dcc-8eee-8c610fa306a9'],\n",
              " 'embeddings': None,\n",
              " 'metadatas': [{'page': 0,\n",
              "   'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 0, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 0, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 1, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 1, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 1, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 1, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 2, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 2, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 2, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 3, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 3, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 3, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 4, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 4, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 4, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 5, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 5, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 5, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 6, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 6, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 6, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 7, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 7, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 7, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 8, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 8, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 8, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 8, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 9, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 9, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 9, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 10, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 10, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 10, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 10, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 11, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 11, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 11, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 12, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 12, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 12, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 13, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 13, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 13, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 13, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 14, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 14, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 14, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 14, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 15, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 15, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 15, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 15, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 16, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 16, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 16, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 16, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 17, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 17, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 17, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 18, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 18, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 18, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 19, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 19, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 19, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 20, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 20, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 20, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 20, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 21, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 21, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 21, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 21, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 21, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 22, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 22, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 22, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 22, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 22, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 23, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 23, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 23, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 23, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 23, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 24, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 24, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 24, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 24, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 24, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 25, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 25, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 25, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 25, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 25, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 26, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 26, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 26, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 26, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 27, 'source': '/content/Unifying LLM and KG - 2024.pdf'},\n",
              "  {'page': 0, 'source': '/content/Liar liar - film.pdf'},\n",
              "  {'page': 1, 'source': '/content/Liar liar - film.pdf'},\n",
              "  {'page': 2, 'source': '/content/Liar liar - film.pdf'},\n",
              "  {'page': 3, 'source': '/content/Liar liar - film.pdf'},\n",
              "  {'page': 4, 'source': '/content/Liar liar - film.pdf'},\n",
              "  {'page': 5, 'source': '/content/Liar liar - film.pdf'},\n",
              "  {'page': 6, 'source': '/content/Liar liar - film.pdf'},\n",
              "  {'page': 7, 'source': '/content/Liar liar - film.pdf'},\n",
              "  {'page': 8, 'source': '/content/Liar liar - film.pdf'},\n",
              "  {'page': 9, 'source': '/content/Liar liar - film.pdf'},\n",
              "  {'page': 10, 'source': '/content/Liar liar - film.pdf'},\n",
              "  {'page': 11, 'source': '/content/Liar liar - film.pdf'},\n",
              "  {'page': 12, 'source': '/content/Liar liar - film.pdf'},\n",
              "  {'page': 13, 'source': '/content/Liar liar - film.pdf'},\n",
              "  {'page': 14, 'source': '/content/Liar liar - film.pdf'},\n",
              "  {'page': 15, 'source': '/content/Liar liar - film.pdf'},\n",
              "  {'page': 16, 'source': '/content/Liar liar - film.pdf'},\n",
              "  {'page': 17, 'source': '/content/Liar liar - film.pdf'},\n",
              "  {'page': 18, 'source': '/content/Liar liar - film.pdf'},\n",
              "  {'page': 19, 'source': '/content/Liar liar - film.pdf'},\n",
              "  {'page': 20, 'source': '/content/Liar liar - film.pdf'},\n",
              "  {'page': 21, 'source': '/content/Liar liar - film.pdf'},\n",
              "  {'page': 22, 'source': '/content/Liar liar - film.pdf'},\n",
              "  {'page': 23, 'source': '/content/Liar liar - film.pdf'},\n",
              "  {'page': 24, 'source': '/content/Liar liar - film.pdf'},\n",
              "  {'page': 25, 'source': '/content/Liar liar - film.pdf'},\n",
              "  {'page': 26, 'source': '/content/Liar liar - film.pdf'},\n",
              "  {'page': 27, 'source': '/content/Liar liar - film.pdf'},\n",
              "  {'page': 28, 'source': '/content/Liar liar - film.pdf'},\n",
              "  {'page': 29, 'source': '/content/Liar liar - film.pdf'},\n",
              "  {'page': 30, 'source': '/content/Liar liar - film.pdf'},\n",
              "  {'page': 31, 'source': '/content/Liar liar - film.pdf'},\n",
              "  {'page': 32, 'source': '/content/Liar liar - film.pdf'},\n",
              "  {'page': 33, 'source': '/content/Liar liar - film.pdf'},\n",
              "  {'page': 34, 'source': '/content/Liar liar - film.pdf'},\n",
              "  {'page': 35, 'source': '/content/Liar liar - film.pdf'},\n",
              "  {'page': 36, 'source': '/content/Liar liar - film.pdf'},\n",
              "  {'page': 37, 'source': '/content/Liar liar - film.pdf'},\n",
              "  {'page': 38, 'source': '/content/Liar liar - film.pdf'},\n",
              "  {'page': 39, 'source': '/content/Liar liar - film.pdf'},\n",
              "  {'page': 40, 'source': '/content/Liar liar - film.pdf'},\n",
              "  {'page': 41, 'source': '/content/Liar liar - film.pdf'},\n",
              "  {'page': 42, 'source': '/content/Liar liar - film.pdf'},\n",
              "  {'page': 43, 'source': '/content/Liar liar - film.pdf'},\n",
              "  {'page': 44, 'source': '/content/Liar liar - film.pdf'},\n",
              "  {'page': 45, 'source': '/content/Liar liar - film.pdf'},\n",
              "  {'page': 46, 'source': '/content/Liar liar - film.pdf'},\n",
              "  {'page': 47, 'source': '/content/Liar liar - film.pdf'},\n",
              "  {'page': 48, 'source': '/content/Liar liar - film.pdf'},\n",
              "  {'page': 49, 'source': '/content/Liar liar - film.pdf'},\n",
              "  {'page': 50, 'source': '/content/Liar liar - film.pdf'},\n",
              "  {'page': 51, 'source': '/content/Liar liar - film.pdf'},\n",
              "  {'page': 52, 'source': '/content/Liar liar - film.pdf'},\n",
              "  {'page': 53, 'source': '/content/Liar liar - film.pdf'},\n",
              "  {'page': 54, 'source': '/content/Liar liar - film.pdf'},\n",
              "  {'page': 55, 'source': '/content/Liar liar - film.pdf'},\n",
              "  {'page': 56, 'source': '/content/Liar liar - film.pdf'},\n",
              "  {'page': 57, 'source': '/content/Liar liar - film.pdf'},\n",
              "  {'page': 58, 'source': '/content/Liar liar - film.pdf'},\n",
              "  {'page': 59, 'source': '/content/Liar liar - film.pdf'},\n",
              "  {'page': 60, 'source': '/content/Liar liar - film.pdf'},\n",
              "  {'page': 61, 'source': '/content/Liar liar - film.pdf'},\n",
              "  {'page': 62, 'source': '/content/Liar liar - film.pdf'},\n",
              "  {'page': 63, 'source': '/content/Liar liar - film.pdf'},\n",
              "  {'page': 64, 'source': '/content/Liar liar - film.pdf'},\n",
              "  {'page': 65, 'source': '/content/Liar liar - film.pdf'},\n",
              "  {'page': 66, 'source': '/content/Liar liar - film.pdf'},\n",
              "  {'page': 67, 'source': '/content/Liar liar - film.pdf'},\n",
              "  {'page': 68, 'source': '/content/Liar liar - film.pdf'},\n",
              "  {'page': 69, 'source': '/content/Liar liar - film.pdf'},\n",
              "  {'page': 70, 'source': '/content/Liar liar - film.pdf'},\n",
              "  {'page': 71, 'source': '/content/Liar liar - film.pdf'},\n",
              "  {'page': 72, 'source': '/content/Liar liar - film.pdf'},\n",
              "  {'page': 73, 'source': '/content/Liar liar - film.pdf'},\n",
              "  {'page': 74, 'source': '/content/Liar liar - film.pdf'},\n",
              "  {'page': 75, 'source': '/content/Liar liar - film.pdf'},\n",
              "  {'page': 76, 'source': '/content/Liar liar - film.pdf'},\n",
              "  {'page': 77, 'source': '/content/Liar liar - film.pdf'},\n",
              "  {'page': 78, 'source': '/content/Liar liar - film.pdf'},\n",
              "  {'page': 79, 'source': '/content/Liar liar - film.pdf'},\n",
              "  {'page': 80, 'source': '/content/Liar liar - film.pdf'},\n",
              "  {'page': 81, 'source': '/content/Liar liar - film.pdf'},\n",
              "  {'page': 82, 'source': '/content/Liar liar - film.pdf'},\n",
              "  {'page': 83, 'source': '/content/Liar liar - film.pdf'},\n",
              "  {'page': 0, 'source': '/content/attention_is_all_you_need.pdf'},\n",
              "  {'page': 0, 'source': '/content/attention_is_all_you_need.pdf'},\n",
              "  {'page': 1, 'source': '/content/attention_is_all_you_need.pdf'},\n",
              "  {'page': 1, 'source': '/content/attention_is_all_you_need.pdf'},\n",
              "  {'page': 1, 'source': '/content/attention_is_all_you_need.pdf'},\n",
              "  {'page': 2, 'source': '/content/attention_is_all_you_need.pdf'},\n",
              "  {'page': 3, 'source': '/content/attention_is_all_you_need.pdf'},\n",
              "  {'page': 3, 'source': '/content/attention_is_all_you_need.pdf'},\n",
              "  {'page': 4, 'source': '/content/attention_is_all_you_need.pdf'},\n",
              "  {'page': 4, 'source': '/content/attention_is_all_you_need.pdf'},\n",
              "  {'page': 5, 'source': '/content/attention_is_all_you_need.pdf'},\n",
              "  {'page': 5, 'source': '/content/attention_is_all_you_need.pdf'},\n",
              "  {'page': 6, 'source': '/content/attention_is_all_you_need.pdf'},\n",
              "  {'page': 6, 'source': '/content/attention_is_all_you_need.pdf'},\n",
              "  {'page': 7, 'source': '/content/attention_is_all_you_need.pdf'},\n",
              "  {'page': 7, 'source': '/content/attention_is_all_you_need.pdf'},\n",
              "  {'page': 8, 'source': '/content/attention_is_all_you_need.pdf'},\n",
              "  {'page': 8, 'source': '/content/attention_is_all_you_need.pdf'},\n",
              "  {'page': 9, 'source': '/content/attention_is_all_you_need.pdf'},\n",
              "  {'page': 9, 'source': '/content/attention_is_all_you_need.pdf'},\n",
              "  {'page': 10, 'source': '/content/attention_is_all_you_need.pdf'},\n",
              "  {'page': 10, 'source': '/content/attention_is_all_you_need.pdf'},\n",
              "  {'page': 11, 'source': '/content/attention_is_all_you_need.pdf'},\n",
              "  {'page': 11, 'source': '/content/attention_is_all_you_need.pdf'},\n",
              "  {'page': 12, 'source': '/content/attention_is_all_you_need.pdf'},\n",
              "  {'page': 13, 'source': '/content/attention_is_all_you_need.pdf'},\n",
              "  {'page': 14, 'source': '/content/attention_is_all_you_need.pdf'}],\n",
              " 'documents': ['JOURNAL OF LATEX CLASS FILES, VOL. ??, NO. ??, MONTH 20YY 1 Unifying Large Language Models and Knowledge Graphs: A Roadmap Shirui Pan, Senior Member, IEEE , Linhao Luo, Yufei Wang, Chen Chen, Jiapu Wang, Xindong Wu, Fellow, IEEE Abstract —Large language models (LLMs), such as ChatGPT and GPT4, are making new waves in the field of natural language processing and artificial intelligence, due to their emergent ability and generalizability. However, LLMs are black-box models, which often fall short of capturing and accessing factual knowledge. In contrast, Knowledge Graphs (KGs), Wikipedia and Huapu for example, are structured knowledge models that explicitly store rich factual knowledge. KGs can enhance LLMs by providing external knowledge for inference and interpretability. Meanwhile, KGs are difficult to construct and evolve by nature, which challenges the existing methods in KGs to generate new facts and represent unseen knowledge. Therefore, it is complementary to unify LLMs and KGs together and simultaneously leverage their advantages. In this article, we present a forward-looking roadmap for the unification of LLMs and KGs. Our roadmap consists of three general frameworks, namely, 1) KG-enhanced LLMs, which incorporate KGs during the pre-training and inference phases of LLMs, or for the purpose of enhancing understanding of the knowledge learned by LLMs; 2) LLM-augmented KGs, that leverage LLMs for different KG tasks such as embedding, completion, construction, graph-to-text generation, and question answering; and 3) Synergized LLMs + KGs , in which LLMs and KGs play equal roles and work in a mutually beneficial way to enhance both LLMs and KGs for bidirectional reasoning driven by both data and knowledge. We review and summarize existing efforts within these three frameworks in our roadmap and pinpoint their future research directions. Index Terms —Natural Language Processing, Large Language Models, Generative Pre-Training, Knowledge Graphs, Roadmap,',\n",
              "  'Language Processing, Large Language Models, Generative Pre-Training, Knowledge Graphs, Roadmap, Bidirectional Reasoning. ✦ 1 I NTRODUCTION Large language models (LLMs)1(e.g., BERT [1], RoBERTA [2], and T5 [3]), pre-trained on the large-scale corpus, have shown great performance in various natural language processing (NLP) tasks, such as question answering [4], machine translation [5], and text generation [6]. Recently, the dramatically increasing model size further enables the LLMs with the emergent ability [7], paving the road for applying LLMs as Artificial General Intelligence (AGI). Advanced LLMs like ChatGPT2and PaLM23, with billions of parameters, exhibit great potential in many complex practical tasks, such as education [8], code generation [9] and recommendation [10]. •Shirui Pan is with the School of Information and Communication Tech- nology and Institute for Integrated and Intelligent Systems (IIIS), Griffith University, Queensland, Australia. Email: s.pan@griffith.edu.au; •Linhao Luo and Yufei Wang are with the Department of Data Sci- ence and AI, Monash University, Melbourne, Australia. E-mail: lin- hao.luo@monash.edu, garyyufei@gmail.com. •Chen Chen is with the Nanyang Technological University, Singapore. E- mail: s190009@ntu.edu.sg. •Jiapu Wang is with the Faculty of Information Technology, Beijing Uni- versity of Technology, Beijing, China. E-mail: jpwang@emails.bjut.edu.cn. •Xindong Wu is with the Key Laboratory of Knowledge Engineering with Big Data (the Ministry of Education of China), Hefei University of Tech- nology, Hefei, China, and also with the Research Center for Knowledge Engineering, Zhejiang Lab, Hangzhou, China. Email: xwu@hfut.edu.cn. •Shirui Pan and Linhao Luo contributed equally to this work. •Corresponding Author: Xindong Wu. 1. LLMs are also known as pre-trained language models (PLMs). 2. https://openai.com/blog/chatgpt 3. https://ai.google/discover/palm2 Fig. 1. Summarization of the pros and cons for LLMs and KGs. LLM pros:',\n",
              "  'Fig. 1. Summarization of the pros and cons for LLMs and KGs. LLM pros: General Knowledge [11], Language Processing [12], Generaliz- ability [13]; LLM cons: Implicit Knowledge [14], Hallucination [15], In- decisiveness [16], Black-box [17], Lacking Domain-specific/New Knowl- edge [18]. KG pros: Structural Knowledge [19], Accuracy [20], Decisive- ness [21], Interpretability [22], Domain-specific Knowledge [23], Evolv- ing Knowledge [24]; KG cons: Incompleteness [25], Lacking Language Understanding [26], Unseen Facts [27]. Pros. and Cons. are selected based on their representativeness. Detailed discussion can be found in Appendix A. Despite their success in many applications, LLMs have been criticized for their lack of factual knowledge. Specif- ically, LLMs memorize facts and knowledge contained in the training corpus [14]. However, further studies reveal that LLMs are not able to recall facts and often experience hallucinations by generating statements that are factually 0000–0000/00$00.00 © 2023 IEEEarXiv:2306.08302v3 [cs.CL] 25 Jan 2024',\n",
              "  'JOURNAL OF LATEX CLASS FILES, VOL. ??, NO. ??, MONTH 20YY 2 incorrect [15], [28]. For example, LLMs might say “Ein- stein discovered gravity in 1687” when asked, “When did Einstein discover gravity?”, which contradicts the fact that Isaac Newton formulated the gravitational theory. This issue severely impairs the trustworthiness of LLMs. As black-box models, LLMs are also criticized for their lack of interpretability. LLMs represent knowledge implic- itly in their parameters. It is difficult to interpret or validate the knowledge obtained by LLMs. Moreover, LLMs perform reasoning by a probability model, which is an indecisive process [16]. The specific patterns and functions LLMs used to arrive at predictions or decisions are not directly accessible or explainable to humans [17]. Even though some LLMs are equipped to explain their predictions by applying chain-of-thought [29], their reasoning explanations also suf- fer from the hallucination issue [30]. This severely impairs the application of LLMs in high-stakes scenarios, such as medical diagnosis and legal judgment. For instance, in a medical diagnosis scenario, LLMs may incorrectly diagnose a disease and provide explanations that contradict medical commonsense. This raises another issue that LLMs trained on general corpus might not be able to generalize well to specific domains or new knowledge due to the lack of domain-specific knowledge or new training data [18]. To address the above issues, a potential solution is to in- corporate knowledge graphs (KGs) into LLMs. Knowledge graphs (KGs), storing enormous facts in the way of triples, i.e.,(head entity, relation, tail entity ), are a structured and decisive manner of knowledge representation (e.g., Wiki- data [20], YAGO [31], and NELL [32]). KGs are crucial for various applications as they offer accurate explicit knowl- edge [19]. Besides, they are renowned for their symbolic reasoning ability [22], which generates interpretable results. KGs can also actively',\n",
              "  'their symbolic reasoning ability [22], which generates interpretable results. KGs can also actively evolve with new knowledge contin- uously added in [24]. Additionally, experts can construct domain-specific KGs to provide precise and dependable domain-specific knowledge [23]. Nevertheless, KGs are difficult to construct [25], and current approaches in KGs [27], [33], [34] are inadequate in handling the incomplete and dynamically changing na- ture of real-world KGs. These approaches fail to effectively model unseen entities and represent new facts. In addition, they often ignore the abundant textual information in KGs. Moreover, existing methods in KGs are often customized for specific KGs or tasks, which are not generalizable enough. Therefore, it is also necessary to utilize LLMs to address the challenges faced in KGs. We summarize the pros and cons of LLMs and KGs in Fig. 1, respectively. Recently, the possibility of unifying LLMs with KGs has attracted increasing attention from researchers and practi- tioners. LLMs and KGs are inherently interconnected and can mutually enhance each other. In KG-enhanced LLMs , KGs can not only be incorporated into the pre-training and inference stages of LLMs to provide external knowledge [35]–[37], but also used for analyzing LLMs and provid- ing interpretability [14], [38], [39]. In LLM-augmented KGs , LLMs have been used in various KG-related tasks, e.g., KG embedding [40], KG completion [26], KG construction [41], KG-to-text generation [42], and KGQA [43], to improve the performance and facilitate the application of KGs. In Syn- ergized LLM + KG , researchers marries the merits of LLMsand KGs to mutually enhance performance in knowledge representation [44] and reasoning [45], [46]. Although there are some surveys on knowledge-enhanced LLMs [47]–[49], which mainly focus on using KGs as an external knowledge to enhance LLMs, they ignore other possibilities of integrat- ing KGs for LLMs and the potential role of LLMs in KG',\n",
              "  'they ignore other possibilities of integrat- ing KGs for LLMs and the potential role of LLMs in KG applications. In this article, we present a forward-looking roadmap for unifying both LLMs and KGs, to leverage their respective strengths and overcome the limitations of each approach, for various downstream tasks. We propose detailed cate- gorization, conduct comprehensive reviews, and pinpoint emerging directions in these fast-growing fields. Our main contributions are summarized as follows: 1)Roadmap. We present a forward-looking roadmap for integrating LLMs and KGs. Our roadmap, consisting of three general frameworks to unify LLMs and KGs, namely, KG-enhanced LLMs ,LLM- augmented KGs , and Synergized LLMs + KGs , pro- vides guidelines for the unification of these two distinct but complementary technologies. 2)Categorization and review. For each integration framework of our roadmap, we present a detailed categorization and novel taxonomies of research on unifying LLMs and KGs. In each category, we review the research from the perspectives of differ- ent integration strategies and tasks, which provides more insights into each framework. 3)Coverage of emerging advances. We cover the advanced techniques in both LLMs and KGs. We include the discussion of state-of-the-art LLMs like ChatGPT and GPT-4 as well as the novel KGs e.g., multi-modal knowledge graphs. 4)Summary of challenges and future directions. We highlight the challenges in existing research and present several promising future research direc- tions. The rest of this article is organized as follows. Section 2 first explains the background of LLMs and KGs. Section 3 introduces the roadmap and the overall categorization of this article. Section 4 presents the different KGs-enhanced LLM approaches. Section 5 describes the possible LLM- augmented KG methods. Section 6 shows the approaches of synergizing LLMs and KGs. Section 7 discusses the challenges and future research directions. Finally, Section 8 concludes',\n",
              "  'Section 7 discusses the challenges and future research directions. Finally, Section 8 concludes this paper. 2 B ACKGROUND In this section, we will first briefly introduce a few rep- resentative large language models (LLMs) and discuss the prompt engineering that efficiently uses LLMs for varieties of applications. Then, we illustrate the concept of knowl- edge graphs (KGs) and present different categories of KGs. 2.1 Large Language models (LLMs) Large language models (LLMs) pre-trained on large-scale corpus have shown great potential in various NLP tasks [13]. As shown in Fig. 3, most LLMs derive from the Trans- former design [50], which contains the encoder and decoder',\n",
              "  'JOURNAL OF LATEX CLASS FILES, VOL. ??, NO. ??, MONTH 20YY 3 2018 2019 2020 2021 2023Encoder-onlyEncoder- decoderDecoder-only 2022Input TextDecoderOutput Text Input TextEncoderFeaturesDecoderOutput Text Input TextEncoderFeaturesBERT RoBER TA110M-340M 125M-355MALBER TDistillBert66M ERNIEELECTRA DeBER Ta 114M14M-1 10M 11M-223M 44M-304MBART T5mT5T0140M 80M-1 1B11B 300M-13BGLM Switch110M-10B 1.6TST-MoE4.1B-269BGLM-130B UL220B Flan-UL220B130B Flan-T5 80M-1 1BGPT-1110 M GPT-2117M-1.5B GPT-3175B ChatGPT GPT-4175B Unknown XLNet110M-340M GLaM1.2T Gopher 280BLaMDA PaLM 137B Flan PaLM540B 540BOPT175B OPT-IML175B Bard137B LLaMa 7B-65B AlpacaVicuna 7B 7B-13B Open-Source Closed-Source Fig. 2. Representative large language models (LLMs) in recent years. Open-source models are represented by solid squares, while closed source models are represented by hollow squares. Encoder Self-AttentionFeed Forward Self-AttentionFeed Forward Encoder-Decoder AttentionDecoder LinearMulti-head Dot-Product Attention VLinear Linear Q KLinear ConcatSelf-Attention Fig. 3. An illustration of the Transformer-based LLMs with self-attention mechanism. modules empowered by a self-attention mechanism. Based on the architecture structure, LLMs can be categorized into three groups: 1) encoder-only LLMs ,2) encoder-decoder LLMs , and 3) decoder-only LLMs . As shown in Fig. 2, we sum- marize several representative LLMs with different model architectures, model sizes, and open-source availabilities. 2.1.1 Encoder-only LLMs. Encoder-only large language models only use the encoder to encode the sentence and understand the relationships between words. The common training paradigm for these model is to predict the mask words in an input sentence. This method is unsupervised and can be trained on the large-scale corpus. Encoder-only LLMs like BERT [1], AL- BERT [51], RoBERTa [2], and ELECTRA [52] require adding an extra prediction head to resolve downstream tasks. These models are most effective for tasks that require',\n",
              "  'prediction head to resolve downstream tasks. These models are most effective for tasks that require understand- ing the entire sentence, such as text classification [26] and named entity recognition [53]. 2.1.2 Encoder-decoder LLMs. Encoder-decoder large language models adopt both the encoder and decoder module. The encoder module is re-sponsible for encoding the input sentence into a hidden- space, and the decoder is used to generate the target output text. The training strategies in encoder-decoder LLMs can be more flexible. For example, T5 [3] is pre-trained by masking and predicting spans of masking words. UL2 [54] unifies several training targets such as different masking spans and masking frequencies. Encoder-decoder LLMs (e.g., T0 [55], ST-MoE [56], and GLM-130B [57]) are able to directly resolve tasks that generate sentences based on some context, such as summariaztion, translation, and question answering [58]. 2.1.3 Decoder-only LLMs. Decoder-only large language models only adopt the de- coder module to generate target output text. The training paradigm for these models is to predict the next word in the sentence. Large-scale decoder-only LLMs can generally perform downstream tasks from a few examples or simple instructions, without adding prediction heads or finetun- ing [59]. Many state-of-the-art LLMs (e.g., Chat-GPT [60] and GPT-44) follow the decoder-only architecture. However, since these models are closed-source, it is challenging for academic researchers to conduct further research. Recently, Alpaca5and Vicuna6are released as open-source decoder- only LLMs. These models are finetuned based on LLaMA [61] and achieve comparable performance with ChatGPT and GPT-4. 2.1.4 Prompt Engineering Prompt engineering is a novel field that focuses on creating and refining prompts to maximize the effectiveness of large language models (LLMs) across various applications and re- search areas [62]. As shown in Fig. 4, a prompt is a sequence 4.',\n",
              "  'various applications and re- search areas [62]. As shown in Fig. 4, a prompt is a sequence 4. https://openai.com/product/gpt-4 5. https://github.com/tatsu-lab/stanford alpaca 6. https://lmsys.org/blog/2023-03-30-vicuna/',\n",
              "  'JOURNAL OF LATEX CLASS FILES, VOL. ??, NO. ??, MONTH 20YY 4 LLMs Classify the text into neutral, negative or positive. Text: This is awesome! Sentiment: Positive Text: This is bad! Sentiment: Negative Text: I think the vacation is okay . Sentiment:Instruction Context Input TextPromptPositive Output Fig. 4. An example of sentiment classification prompt. of natural language inputs for LLMs that are specified for the task, such as sentiment classification. A prompt could contain several elements, i.e., 1) Instruction ,2) Context , and 3) Input Text .Instruction is a short sentence that instructs the model to perform a specific task. Context provides the context for the input text or few-shot examples. Input Text is the text that needs to be processed by the model. Prompt engineering seeks to improve the capacity of large large language models (e.g., ChatGPT) in diverse complex tasks such as question answering, sentiment clas- sification, and common sense reasoning. Chain-of-thought (CoT) prompt [63] enables complex reasoning capabilities through intermediate reasoning steps. Prompt engineering also enables the integration of structural data like knowl- edge graphs (KGs) into LLMs. Li et al. [64] simply linearizes the KGs and uses templates to convert the KGs into pas- sages. Mindmap [65] designs a KG prompt to convert graph structure into a mind map that enables LLMs to perform reasoning on it. Prompt offers a simple way to utilize the potential of LLMs without finetuning. Proficiency in prompt engineering leads to a better understanding of the strengths and weaknesses of LLMs. 2.2 Knowledge Graphs (KGs) Knowledge graphs (KGs) store structured knowledge as a collection of triples KG={(h, r, t)⊆ E × R × E} , where E andRrespectively denote the set of entities and relations. Existing knowledge graphs (KGs) can be classified into four groups based on the stored information: 1) encyclopedic KGs , 2) commonsense KGs ,3) domain-specific KGs , and 4) multi- modal KGs . We',\n",
              "  \"1) encyclopedic KGs , 2) commonsense KGs ,3) domain-specific KGs , and 4) multi- modal KGs . We illustrate the examples of KGs of different categories in Fig. 5. 2.2.1 Encyclopedic Knowledge Graphs. Encyclopedic knowledge graphs are the most ubiquitous KGs, which represent the general knowledge in real-world. Encyclopedic knowledge graphs are often constructed by integrating information from diverse and extensive sources, including human experts, encyclopedias, and databases. Wikidata [20] is one of the most widely used encyclopedic knowledge graphs, which incorporates varieties of knowl- edge extracted from articles on Wikipedia. Other typical WikipediaMarriedT oPoliticianOf LiveInLocatedInBornIn CapitalOfEncyclopedic Knowledge GraphsCommonsense Knowledge GraphsBarack Obama Michelle ObamaUSAHonolulu W ashington D.C. Wake upBed LocatedAt Get out of bedOpen eyesSubeventOfSubeventOfDrink coffeeSubeventOfAwake Causes Make coffe SubeventOf Coffe IsFor DrinkIsKitchen LocatedAt Sugar CupNeed NeedConcept: W ake upDomain-speciﬁc Knowledge GraphsParkinson's DieaseSleeping DisorderCausePINK1Cause Motor SymptomLead TremorLeadAnxietyCause Pervasive Developmental DisorderLeadLanguage Undevelopment LeadMedical Knowledge GraphMulti-modal Knowledge GraphsFrance ParisEiffel TowerCapitalOf LocatedIn European UnionMemberOf Emmanuel MacronPoliticianOf LiveIn Fig. 5. Examples of different categories’ knowledge graphs, i.e., encyclo- pedic KGs ,commonsense KGs ,domain-specific KGs , and multi-modal KGs. encyclopedic knowledge graphs, like Freebase [66], Dbpedia [67], and YAGO [31] are also derived from Wikipedia. In ad- dition, NELL [32] is a continuously improving encyclopedic knowledge graph, which automatically extracts knowledge from the web, and uses that knowledge to improve its per- formance over time. There are several encyclopedic knowl- edge graphs available in languages other than English such as CN-DBpedia [68] and Vikidia [69]. The largest knowledge graph, named Knowledge\",\n",
              "  'than English such as CN-DBpedia [68] and Vikidia [69]. The largest knowledge graph, named Knowledge Occean (KO)7, currently contains 4,8784,3636 entities and 17,3115,8349 relations in both En- glish and Chinese. 2.2.2 Commonsense Knowledge Graphs. Commonsense knowledge graphs formulate the knowledge about daily concepts, e.g., objects, and events, as well as their relationships [70]. Compared with encyclopedic knowledge graphs, commonsense knowledge graphs often model the tacit knowledge extracted from text such as (Car, UsedFor, Drive) . ConceptNet [71] contains a wide range of commonsense concepts and relations, which can help computers understand the meanings of words people use. ATOMIC [72], [73] and ASER [74] focus on the causal effects between events, which can be used for commonsense rea- soning. Some other commonsense knowledge graphs, such as TransOMCS [75] and CausalBanK [76] are automatically constructed to provide commonsense knowledge. 7. https://ko.zhonghuapu.com/',\n",
              "  'JOURNAL OF LATEX CLASS FILES, VOL. ??, NO. ??, MONTH 20YY 5 LLMsKGs Text InputStructural Fact Domain-speciﬁc Knowledge Symbolic-reasoning .... Output KGsLLMs KG-related TasksGeneral Knowledge Language Processing Generalizability .... OutputKGs LLMs a. KG-enhanced LLMs b. LLM-augmented KGs c. Synergized LLMs + KGsFactual Knowledge Knowledge Representation Fig. 6. The general roadmap of unifying KGs and LLMs. (a.) KG-enhanced LLMs. (b.) LLM-augmented KGs. (c.) Synergized LLMs + KGs. TABLE 1 Representative applications of using LLMs and KGs. Name Category LLMs KGs URL ChatGPT/GPT-4 Chat Bot ✓ https://shorturl.at/cmsE0 ERNIE 3.0 Chat Bot ✓ ✓ https://shorturl.at/sCLV9 Bard Chat Bot ✓ ✓ https://shorturl.at/pDLY6 Firefly Photo Editing ✓ https://shorturl.at/fkzJV AutoGPT AI Assistant ✓ https://shorturl.at/bkoSY Copilot Coding Assistant ✓ https://shorturl.at/lKLUV New Bing Web Search ✓ https://shorturl.at/bimps Shop.ai Recommendation ✓ https://shorturl.at/alCY7 Wikidata Knowledge Base ✓ https://shorturl.at/lyMY5 KO Knowledge Base ✓ https://shorturl.at/sx238 OpenBG Recommendation ✓ https://shorturl.at/pDMV9 Doctor.ai Health Care Assistant ✓ ✓ https://shorturl.at/dhlK0 2.2.3 Domain-specific Knowledge Graphs Domain-specific knowledge graphs are often constructed to represent knowledge in a specific domain, e.g., medi- cal, biology, and finance [23]. Compared with encyclopedic knowledge graphs, domain-specific knowledge graphs are often smaller in size, but more accurate and reliable. For example, UMLS [77] is a domain-specific knowledge graph in the medical domain, which contains biomedical concepts and their relationships. In addition, there are some domain- specific knowledge graphs in other domains, such as finance [78], geology [79], biology [80], chemistry [81] and geneal- ogy [82]. 2.2.4 Multi-modal Knowledge Graphs. Unlike conventional knowledge graphs that only contain textual information, multi-modal knowledge graphs repre- sent facts in multiple modalities such as',\n",
              "  'textual information, multi-modal knowledge graphs repre- sent facts in multiple modalities such as images, sounds, and videos [83]. For example, IMGpedia [84], MMKG [85], and Richpedia [86] incorporate both the text and image information into the knowledge graphs. These knowledge graphs can be used for various multi-modal tasks such as image-text matching [87], visual question answering [88], and recommendation [89]. 2.3 Applications LLMs as KGs have been widely applied in various real-world applications. We summarize some representa- tive applications of using LLMs and KGs in Table 1. ChatGPT/GPT-4 are LLM-based chatbots that can commu- nicate with humans in a natural dialogue format. To im- prove knowledge awareness of LLMs, ERNIE 3.0 and Bard incorporate KGs into their chatbot applications. Instead ofChatbot. Firefly develops a photo editing application that allows users to edit photos by using natural language de- scriptions. Copilot, New Bing, and Shop.ai adopt LLMs to empower their applications in the areas of coding assistant, web search, and recommendation, respectively. Wikidata and KO are two representative knowledge graph applica- tions that are used to provide external knowledge. OpenBG [90] is a knowledge graph designed for recommendation. Doctor.ai develops a health care assistant that incorporates LLMs and KGs to provide medical advice. 3 R OADMAP & C ATEGORIZATION In this section, we first present a road map of explicit frameworks that unify LLMs and KGs. Then, we present the categorization of research on unifying LLMs and KGs. 3.1 Roadmap The roadmap of unifying KGs and LLMs is illustrated in Fig. 6. In the roadmap, we identify three frameworks for the unification of LLMs and KGs, including KG-enhanced LLMs, LLM-augmented KGs, and Synergized LLMs + KGs. The KG-enhanced LLMs and LLM-augmented KGs are two parallel frameworks that aim to enhance the capabilities of LLMs and KGs, respectively. Building upon these frame- works, Synergized LLMs + KGs is',\n",
              "  'of LLMs and KGs, respectively. Building upon these frame- works, Synergized LLMs + KGs is a unified framework that aims to synergize LLMs and KGs to mutually enhance each other. 3.1.1 KG-enhanced LLMs LLMs are renowned for their ability to learn knowledge from large-scale corpus and achieve state-of-the-art per- formance in various NLP tasks. However, LLMs are often criticized for their hallucination issues [15], and lacking of interpretability. To address these issues, researchers have proposed to enhance LLMs with knowledge graphs (KGs). KGs store enormous knowledge in an explicit and struc- tured way, which can be used to enhance the knowledge awareness of LLMs. Some researchers have proposed to incorporate KGs into LLMs during the pre-training stage, which can help LLMs learn knowledge from KGs [35], [91]. Other researchers have proposed to incorporate KGs into LLMs during the inference stage. By retrieving knowledge from KGs, it can significantly improve the performance of LLMs in accessing domain-specific knowledge [92]. To improve the interpretability of LLMs, researchers also utilize',\n",
              "  'JOURNAL OF LATEX CLASS FILES, VOL. ??, NO. ??, MONTH 20YY 6 Structural FactT ext CorpusImageKGs LLMsExplicit Knowledge Domain-speciﬁc Knowledge Decisiveness Interpretability General Knowledge Language Processing GeneralizabilityPrompt Engineering Graph Neural Network Representation Learning Neural-symbolic ReasoningIn-context Learning Few-shot LearningSearch EngineRecommender SystemDialogue SystemAI Assistant V ideo DataSynergized ModelT echniqueApplication Fig. 7. The general framework of the Synergized LLMs + KGs , which contains four layers: 1) Data ,2) Synergized Model ,3) Technique , and 4) Application . KGs to interpret the facts [14] and the reasoning process of LLMs [38]. 3.1.2 LLM-augmented KGs KGs store structure knowledge playing an essential role in many real-word applications [19]. Existing methods in KGs fall short of handling incomplete KGs [33] and processing text corpus to construct KGs [93]. With the generalizability of LLMs, many researchers are trying to harness the power of LLMs for addressing KG-related tasks. The most straightforward way to apply LLMs as text encoders for KG-related tasks. Researchers take advantage of LLMs to process the textual corpus in the KGs and then use the representations of the text to enrich KGs representa- tion [94]. Some studies also use LLMs to process the original corpus and extract relations and entities for KG construction [95]. Recent studies try to design a KG prompt that can effectively convert structural KGs into a format that can be comprehended by LLMs. In this way, LLMs can be directly applied to KG-related tasks, e.g., KG completion [96] and KG reasoning [97]. 3.1.3 Synergized LLMs + KGs The synergy of LLMs and KGs has attracted increasing attention from researchers these years [40], [42]. LLMs and KGs are two inherently complementary techniques, which should be unified into a general framework to mutually enhance each other. To further explore the unification, we propose a unified framework of the',\n",
              "  'enhance each other. To further explore the unification, we propose a unified framework of the synergized LLMs + KGs in Fig. 7. The unified framework contains four layers: 1) Data ,2) Syner- gized Model ,3) Technique , and 4) Application . In the Data layer, LLMs and KGs are used to process the textual and structural data, respectively. With the development of multi-modal LLMs [98] and KGs [99], this framework can be extended to process multi-modal data, such as video, audio, andimages. In the Synergized Model layer, LLMs and KGs could synergize with each other to improve their capabilities. In Technique layer, related techniques that have been used in LLMs and KGs can be incorporated into this framework to further enhance the performance. In the Application layer, LLMs and KGs can be integrated to address various real- world applications, such as search engines [100], recom- mender systems [10], and AI assistants [101]. 3.2 Categorization To better understand the research on unifying LLMs and KGs, we further provide a fine-grained categorization for each framework in the roadmap. Specifically, we focus on different ways of integrating KGs and LLMs, i.e., KG- enhanced LLMs, KG-augmented LLMs, and Synergized LLMs + KGs. The fine-grained categorization of the research is illustrated in Fig. 8. KG-enhanced LLMs. Integrating KGs can enhance the performance and interpretability of LLMs in various down- stream tasks. We categorize the research on KG-enhanced LLMs into three groups: 1) KG-enhanced LLM pre-training includes works that apply KGs during the pre-training stage and im- prove the knowledge expression of LLMs. 2) KG-enhanced LLM inference includes research that utilizes KGs during the inference stage of LLMs, which enables LLMs to access the latest knowledge without retraining. 3) KG-enhanced LLM interpretability includes works that use KGs to understand the knowledge learned by LLMs and interpret the reasoning process of LLMs. LLM-augmented KGs. LLMs can be',\n",
              "  'learned by LLMs and interpret the reasoning process of LLMs. LLM-augmented KGs. LLMs can be applied to augment various KG-related tasks. We categorize the research on LLM-augmented KGs into five groups based on the task types: 1) LLM-augmented KG embedding includes studies that apply LLMs to enrich representations of KGs by encoding the textual descriptions of entities and relations. 2) LLM-augmented KG completion includes papers that utilize LLMs to encode text or generate facts for better KGC performance. 3) LLM-augmented KG construction includes works that apply LLMs to address the entity discovery, corefer- ence resolution, and relation extraction tasks for KG construction. 4) LLM-augmented KG-to-text Generation includes re- search that utilizes LLMs to generate natural lan- guage that describes the facts from KGs. 5) LLM-augmented KG question answering includes stud- ies that apply LLMs to bridge the gap between natural language questions and retrieve answers from KGs. Synergized LLMs + KGs. The synergy of LLMs and KGs aims to integrate LLMs and KGs into a unified framework to mutually enhance each other. In this categorization, we review the recent attempts of Synergized LLMs + KGs from the perspectives of knowledge representation and reasoning . In the following sections (Sec 4, 5, and 6), we will provide details on these categorizations.',\n",
              "  'JOURNAL OF LATEX CLASS FILES, VOL. ??, NO. ??, MONTH 20YY 7 LLMs Meet K GsKG-enhanced LLMsKG-enhanced LLM pre-tr ainingIntegr ating K Gs into tr aining objective Integr ating K Gs into LLM inputs KGs Instruction-tuning KG-enhanced LLM inferenceRetrieval-augmented knowledge fusion KGs Prompting KG-enhanced LLM interpretabilityKGs for LLM probing KGs for LLM analysis LLM-augmented K GsLLM-augmented K G emebddingLLMs as text encoders LLMs for joint text and K G embedding LLM-augmented K G completionLLMs as encoders LLMs as gener ators LLM-augmented K G constructionEntity discovery Relation extr action Coreference resolution End-to-End K G construction Distilling K Gs from LLMs LLM-augmented K G to text gener ationLever aging knowledge from LLMs LLMs for constructing K G-text aligned Corpus LLM-augmented K G question answeringLLMs as entity/relation extr actors LLMs as answer reasoners Synergized LLMs + K Gs Synergized Knowledge Representation Synergized ReasoningLLM-K G fusion reasoning LLMs as agents reasoning Fig. 8. Fine-grained categorization of research on unifying large language models (LLMs) with knowledge graphs (KGs). 4 KG- ENHANCED LLM S Large language models (LLMs) achieve promising results in many natural language processing tasks. However, LLMs have been criticized for their lack of practical knowledge and tendency to generate factual errors during inference. To address this issue, researchers have proposed integrating knowledge graphs (KGs) to enhance LLMs. In this sec- tion, we first introduce the KG-enhanced LLM pre-training, which aims to inject knowledge into LLMs during the pre- training stage. Then, we introduce the KG-enhanced LLM inference, which enables LLMs to consider the latest knowl- edge while generating sentences. Finally, we introduce the KG-enhanced LLM interpretability, which aims to improve the interpretability of LLMs by using KGs. Table 2 summa- rizes the typical methods that integrate KGs for LLMs. 4.1 KG-enhanced LLM Pre-training',\n",
              "  '2 summa- rizes the typical methods that integrate KGs for LLMs. 4.1 KG-enhanced LLM Pre-training Existing large language models mostly rely on unsupervised training on the large-scale corpus. While these models may exhibit impressive performance on downstream tasks, they often lack practical knowledge relevant to the real world. Previous works that integrate KGs into large language mod- els can be categorized into three parts: 1) Integrating KGs into training objective ,2) Integrating KGs into LLM inputs , and 3) KGs Instruction-tuning . 4.1.1 Integrating KGs into Training Objective The research efforts in this category focus on designing novel knowledge-aware training objectives. An intuitive idea is to expose more knowledge entities in the pre-training objective. GLM [102] leverages the knowledge graph struc- ture to assign a masking probability. Specifically, entities that can be reached within a certain number of hops areTABLE 2 Summary of KG-enhanced LLM methods. Task Method Year KG Technique KG-enhanced LLM pre-trainingERNIE [35] 2019 E Integrating KGs into Training Objective GLM [102] 2020 C Integrating KGs into Training Objective Ebert [103] 2020 D Integrating KGs into Training Objective KEPLER [40] 2021 E Integrating KGs into Training Objective Deterministic LLM [104] 2022 E Integrating KGs into Training Objective KALA [105] 2022 D Integrating KGs into Training Objective WKLM [106] 2020 E Integrating KGs into Training Objective K-BERT [36] 2020 E+D Integrating KGs into Language Model Inputs CoLAKE [107] 2020 E Integrating KGs into Language Model Inputs ERNIE3.0 [101] 2021 E+D Integrating KGs into Language Model Inputs DkLLM [108] 2022 E Integrating KGs into Language Model Inputs KP-PLM [109] 2022 E KGs Instruction-tuning OntoPrompt [110] 2022 E+D KGs Instruction-tuning ChatKBQA [111] 2023 E KGs Instruction-tuning RoG [112] 2023 E KGs Instruction-tuning KG-enhanced LLM inferenceKGLM [113] 2019 E Retrival-augmented knowledge fusion REALM [114] 2020 E',\n",
              "  'KG-enhanced LLM inferenceKGLM [113] 2019 E Retrival-augmented knowledge fusion REALM [114] 2020 E Retrival-augmented knowledge fusion RAG [92] 2020 E Retrival-augmented knowledge fusion EMAT [115] 2022 E Retrival-augmented knowledge fusion Li et al. [64] 2023 C KGs Prompting Mindmap [65] 2023 E+D KGs Prompting ChatRule [116] 2023 E+D KGs Prompting CoK [117] 2023 E+C+D KGs Prompting KG-enhanced LLM interpretabilityLAMA [14] 2019 E KGs for LLM probing LPAQA [118] 2020 E KGs for LLM probing Autoprompt [119] 2020 E KGs for LLM probing MedLAMA [120] 2022 D KGs for LLM probing LLM-facteval [121] 2023 E+D KGs for LLM probing KagNet [38] 2019 C KGs for LLM analysis Interpret-lm [122] 2021 E KGs for LLM analysis knowledge-neurons [39] 2021 E KGs for LLM analysis Shaobo et al. [123] 2022 E KGs for LLM analysis E: Encyclopedic Knowledge Graphs, C: Commonsense Knowledge Graphs, D: Domain-Specific Knowledge Graphs. considered to be the most important entities for learning, and they are given a higher masking probability during pre-training. Furthermore, E-BERT [103] further controls the balance between the token-level and entity-level training losses. The training loss values are used as indications of the learning process for token and entity, which dynamically de- termines their ratio for the next training epochs. SKEP [124] also follows a similar fusion to inject sentiment knowledge during LLMs pre-training. SKEP first determines words with positive and negative sentiment by utilizing PMI along with a predefined set of seed sentiment words. Then, it assigns a higher masking probability to those identified',\n",
              "  'JOURNAL OF LATEX CLASS FILES, VOL. ??, NO. ??, MONTH 20YY 8 LLMs Bob Dylan wrote blowin ... 1962Bob DylanBlowin’ in the W ind Input T ext: Bob Dylan wrote Blowin’ in the W ind in 1962...Text RepresentationsKnowledge Graph Representations Text Sequence EntitiyText-knowledge Alignment Fig. 9. Injecting KG information into LLMs training objective via text- knowledge alignment loss, where hdenotes the hidden representation generated by LLMs. sentiment words in the word masking objective. The other line of work explicitly leverages the connec- tions with knowledge and input text. As shown in Fig. 9, ERNIE [35] proposes a novel word-entity alignment training objective as a pre-training objective. Specifically, ERNIE feeds both sentences and corresponding entities mentioned in the text into LLMs, and then trains the LLMs to pre- dict alignment links between textual tokens and entities in knowledge graphs. Similarly, KALM [91] enhances the input tokens by incorporating entity embeddings and includes an entity prediction pre-training task in addition to the token-only pre-training objective. This approach aims to improve the ability of LLMs to capture knowledge related to entities. Finally, KEPLER [40] directly employs both knowledge graph embedding training objective and Masked token pre-training objective into a shared transformer-based encoder. Deterministic LLM [104] focuses on pre-training language models to capture deterministic factual knowledge. It only masks the span that has a deterministic entity as the question and introduces additional clue contrast learning and clue classification objective. WKLM [106] first replaces entities in the text with other same-type entities and then feeds them into LLMs. The model is further pre-trained to distinguish whether the entities have been replaced or not. 4.1.2 Integrating KGs into LLM Inputs As shown in Fig. 10, this kind of research focus on in- troducing relevant knowledge sub-graph into the inputs of LLMs. Given a',\n",
              "  'of research focus on in- troducing relevant knowledge sub-graph into the inputs of LLMs. Given a knowledge graph triple and the corre- sponding sentences, ERNIE 3.0 [101] represents the triple as a sequence of tokens and directly concatenates them with the sentences. It further randomly masks either the relation token in the triple or tokens in the sentences to better combine knowledge with textual representations. However, such direct knowledge triple concatenation method allows the tokens in the sentence to intensively interact with the tokens in the knowledge sub-graph, which could result in Knowledge Noise [36]. To solve this issue, K-BERT [36] takes the first step to inject the knowledge triple into the sentence via a visible matrix where only the knowledge entities have access to the knowledge triple information, while the tokens in the sentences can only see each other in the self-attention module. To further reduce Knowledge Noise , Colake [107] proposes a unified word-knowledge graph (shown in Fig. 10) where the tokens in the input sentences form a fully Input T ext: Mr. Darcy gives Elizabeth a letterMr. Darcy Elizabeth gives a letterBeloved FatherMr. BennetMother JaneLLMs Mr. BennetFatherBelovedMother JaneText GraphKnowledge GraphMr. Darcy... [MASK] Mother [MASK] ... Text SequenceEntity SequenceletterMr. BennetMask Text PredictionMask Entity PredictionFig. 10. Injecting KG information into LLMs inputs using graph structure. connected word graph where tokens aligned with knowl- edge entities are connected with their neighboring entities. The above methods can indeed inject a large amount of knowledge into LLMs. However, they mostly focus on popular entities and overlook the low-frequent and long- tail ones. DkLLM [108] aims to improve the LLMs repre- sentations towards those entities. DkLLM first proposes a novel measurement to determine long-tail entities and then replaces these selected entities in the text with pseudo token embedding as new input to the',\n",
              "  'then replaces these selected entities in the text with pseudo token embedding as new input to the large language models. Furthermore, Dict-BERT [125] proposes to leverage exter- nal dictionaries to solve this issue. Specifically, Dict-BERT improves the representation quality of rare words by ap- pending their definitions from the dictionary at the end of input text and trains the language model to locally align rare word representations in input sentences and dictionary definitions as well as to discriminate whether the input text and definition are correctly mapped. 4.1.3 KGs Instruction-tuning Instead of injecting factual knowledge into LLMs, the KGs Instruction-tuning aims to fine-tune LLMs to better com- prehend the structure of KGs and effectively follow user instructions to conduct complex tasks. KGs Instruction- tuning utilizes both facts and the structure of KGs to cre- ate instruction-tuning datasets. LLMs finetuned on these datasets can extract both factual and structural knowledge from KGs, enhancing the reasoning ability of LLMs. KP- PLM [109] first designs several prompt templates to transfer structural graphs into natural language text. Then, two self- supervised tasks are proposed to finetune LLMs to further leverage the knowledge from these prompts. OntoPrompt [110] proposes an ontology-enhanced prompt-tuning that can place knowledge of entities into the context of LLMs, which are further finetuned on several downstream tasks. ChatKBQA [111] finetunes LLMs on KG structure to gener- ate logical queries, which can be executed on KGs to obtain answers. To better reason on graphs, RoG [112] presents a planning-retrieval-reasoning framework. RoG is finetuned on KG structure to generate relation paths grounded by KGs as faithful plans. These plans are then used to retrieve valid',\n",
              "  'JOURNAL OF LATEX CLASS FILES, VOL. ??, NO. ??, MONTH 20YY 9 reasoning paths from the KGs for LLMs to conduct faithful reasoning and generate interpretable results. KGs Instruction-tuning can better leverage the knowl- edge from KGs for downstream tasks. However, it requires retraining the models, which is time-consuming and re- quires lots of resources. 4.2 KG-enhanced LLM Inference The above methods could effectively fuse knowledge into LLMs. However, real-world knowledge is subject to change and the limitation of these approaches is that they do not permit updates to the incorporated knowledge without retraining the model. As a result, they may not generalize well to the unseen knowledge during inference [126]. There- fore, considerable research has been devoted to keeping the knowledge space and text space separate and injecting the knowledge while inference. These methods mostly focus on the Question Answering (QA) tasks, because QA requires the model to capture both textual semantic meanings and up-to-date real-world knowledge. 4.2.1 Retrieval-Augmented Knowledge Fusion Retrieval-Augmented Knowledge Fusion is a popular method to inject knowledge into LLMs during inference. The key idea is to retrieve relevant knowledge from a large corpus and then fuse the retrieved knowledge into LLMs. As shown in Fig. 11, RAG [92] proposes to combine non- parametric and parametric modules to handle the external knowledge. Given the input text, RAG first searches for rel- evant KG in the non-parametric module via MIPS to obtain several documents. RAG then treats these documents as hidden variables zand feeds them into the output generator, empowered by Seq2Seq LLMs, as additional context infor- mation. The research indicates that using different retrieved documents as conditions at different generation steps per- forms better than only using a single document to guide the whole generation process. The experimental results show that RAG outperforms other parametric-only and',\n",
              "  'generation process. The experimental results show that RAG outperforms other parametric-only and non-parametric-only baseline models in open-domain QA. RAG can also generate more specific, diverse, and factual text than other parameter-only baselines. Story-fragments [127] further improves architecture by adding an additional module to determine salient knowledge entities and fuse them into the generator to improve the quality of generated long stories. EMAT [115] further improves the efficiency of such a system by encoding external knowledge into a key- value memory and exploiting the fast maximum inner prod- uct search for memory querying. REALM [114] proposes a novel knowledge retriever to help the model to retrieve and attend over documents from a large corpus during the pre- training stage and successfully improves the performance of open-domain question answering. KGLM [113] selects the facts from a knowledge graph using the current context to generate factual sentences. With the help of an external knowledge graph, KGLM could describe facts using out-of- domain words or phrases. 4.2.2 KGs Prompting To better feed the KG structure into the LLM during infer- ence, KGs prompting aims to design a crafted prompt that Knowledge RetrieverLLMs A: USA BackpropagationKGs Q: Which country is Obama from?Retrieved Facts (Obama, BornIn, Honolulu) (Honolulu, LocatedIn, USA)Fig. 11. Retrieving external knowledge to enhance the LLM generation. converts structured KGs into text sequences, which can be fed as context into LLMs. In this way, LLMs can better take advantage of the structure of KGs to perform reasoning. Li et al. [64] adopt the pre-defined template to convert each triple into a short sentence, which can be understood by LLMs for reasoning. Mindmap [65] designs a KG prompt to convert graph structure into a mind map that enables LLMs to perform reasoning by consolidating the facts in KGs and the implicit knowledge from LLMs. ChatRule [116] sam- ples several relation',\n",
              "  'the facts in KGs and the implicit knowledge from LLMs. ChatRule [116] sam- ples several relation paths from KGs, which are verbalized and fed into LLMs. Then, LLMs are prompted to generate meaningful logical rules that can be used for reasoning. CoK [117] proposes a chain-of-knowledge prompting that uses a sequence of triples to elicit the reasoning ability of LLMs to reach the final answer. KGs prompting presents a simple way to synergize LLMs and KGs. By using the prompt, we can easily harness the power of LLMs to perform reasoning based on KGs without retraining the models. However, the prompt is usually designed manually, which requires lots of human effort. 4.3 Comparison between KG-enhanced LLM Pre- training and Inference KG-enhanced LLM Pre-training methods commonly en- rich large-amount of unlabeled corpus with semantically relevant real-world knowledge. These methods allow the knowledge representations to be aligned with appropri- ate linguistic context and explicitly train LLMs to lever- age those knowledge from scratch. When applying the resulting LLMs to downstream knowledge-intensive tasks, they should achieve optimal performance. In contrast, KG- enhanced LLM inference methods only present the knowl- edge to LLMs in the inference stage and the underlying LLMs may not be trained to fully leverage these knowledge when conducting downstream tasks, potentially resulting in sub-optimal model performance. However, real-world knowledge is dynamic and requires frequent updates. Despite being effective, the KG-enhanced LLM Pre-training methods never permit knowledge up- dates or editing without model re-training. As a result, the KG-enhanced LLM Pre-training methods could generalize poorly to recent or unseen knowledge. KG-enhanced LLM inference methods can easily maintain knowledge updates by changing the inference inputs. These methods help im- prove LLMs performance on new knowledge and domains. In summary, when to use these methods depends on the',\n",
              "  'LLMs performance on new knowledge and domains. In summary, when to use these methods depends on the application scenarios. If one wishes to apply LLMs to han-',\n",
              "  'JOURNAL OF LATEX CLASS FILES, VOL. ??, NO. ??, MONTH 20YY 10 Fig. 12. The general framework of using knowledge graph for language model probing. dle time-insensitive knowledge in particular domains (e.g., commonsense and reasoning knowledge), KG-enhanced LLM Pre-training methods should be considered. Other- wise, KG-enhanced LLM inference methods can be used to handle open-domain knowledge with frequent updates. 4.4 KG-enhanced LLM Interpretability Although LLMs have achieved remarkable success in many NLP tasks, they are still criticized for their lack of inter- pretability. The large language model (LLM) interpretability refers to the understanding and explanation of the inner workings and decision-making processes of a large lan- guage model [17]. This can improve the trustworthiness of LLMs and facilitate their applications in high-stakes scenar- ios such as medical diagnosis and legal judgment. Knowl- edge graphs (KGs) represent the knowledge structurally and can provide good interpretability for the reasoning results. Therefore, researchers try to utilize KGs to improve the interpretability of LLMs, which can be roughly grouped into two categories: 1) KGs for language model probing , and 2) KGs for language model analysis . 4.4.1 KGs for LLM Probing The large language model (LLM) probing aims to under- stand the knowledge stored in LLMs. LLMs, trained on large-scale corpus, are often known as containing enor- mous knowledge. However, LLMs store the knowledge in a hidden way, making it hard to figure out the stored knowledge. Moreover, LLMs suffer from the hallucination problem [15], which results in generating statements that contradict facts. This issue significantly affects the reliability of LLMs. Therefore, it is necessary to probe and verify the knowledge stored in LLMs. LAMA [14] is the first work to probe the knowledge in LLMs by using KGs. As shown in Fig. 12, LAMA first converts the facts in KGs into cloze statements by a pre- defined prompt template',\n",
              "  '12, LAMA first converts the facts in KGs into cloze statements by a pre- defined prompt template and then uses LLMs to predict the missing entity. The prediction results are used to evaluate the knowledge stored in LLMs. For example, we try to probe whether LLMs know the fact (Obama, profession, pres- ident) . We first convert the fact triple into a cloze question “Obama’s profession is .” with the object masked. Then, we test if the LLMs can predict the object “president” correctly. However, LAMA ignores the fact that the prompts are inappropriate. For example, the prompt “Obama worked as a”may be more favorable to the prediction of the blank by the language models than “Obama is a by profession” . Fig. 13. The general framework of using knowledge graph for language model analysis. Thus, LPAQA [118] proposes a mining and paraphrasing- based method to automatically generate high-quality and diverse prompts for a more accurate assessment of the knowledge contained in the language model. Moreover, Adolphs et al. [128] attempt to use examples to make the language model understand the query, and experiments obtain substantial improvements for BERT-large on the T- REx data. Unlike using manually defined prompt templates, Autoprompt [119] proposes an automated method, which is based on the gradient-guided search to create prompts. LLM-facteval [121] designs a systematic framework that automatically generates probing questions from KGs. The generated questions are then used to evaluate the factual knowledge stored in LLMs. Instead of probing the general knowledge by using the encyclopedic and commonsense knowledge graphs, BioLAMA [129] and MedLAMA [120] probe the medical knowledge in LLMs by using medical knowledge graphs. Alex et al. [130] investigate the capacity of LLMs to re- tain less popular factual knowledge. They select unpopular facts from Wikidata knowledge graphs which have low- frequency clicked entities. These facts are then used for the evaluation, where the',\n",
              "  'which have low- frequency clicked entities. These facts are then used for the evaluation, where the results indicate that LLMs encounter difficulties with such knowledge, and that scaling fails to appreciably improve memorization of factual knowledge in the tail. 4.4.2 KGs for LLM Analysis Knowledge graphs (KGs) for pre-train language models (LLMs) analysis aims to answer the following questions such as “how do LLMs generate the results?”, and “how do the function and structure work in LLMs?”. To analyze the inference process of LLMs, as shown in Fig. 13, KagNet [38] and QA-GNN [131] make the results generated by LLMs at each reasoning step grounded by knowledge graphs. In this way, the reasoning process of LLMs can be explained by extracting the graph structure from KGs. Shaobo et al. [123] investigate how LLMs generate the results correctly. They adopt the causal-inspired analysis from facts extracted from KGs. This analysis quantitatively measures the word patterns that LLMs depend on to generate the results. The results show that LLMs generate the missing factual more by the positionally closed words rather than the knowledge- dependent words. Thus, they claim that LLMs are inade- quate to memorize factual knowledge because of the inaccu- rate dependence. To interpret the training of LLMs, Swamy',\n",
              "  'JOURNAL OF LATEX CLASS FILES, VOL. ??, NO. ??, MONTH 20YY 11 TABLE 3 Summary of representative LLM-augmented KG methods. Task Method Year LLM Technique LLM-augmented KG embeddingPretrain-KGE [94] 2020 E LLMs as Text Encoders KEPLER [40] 2020 E LLMs as Text Encoders Nayyeri et al. [132] 2022 E LLMs as Text Encoders Huang et al. [133] 2022 E LLMs as Text Encoders CoDEx [134] 2022 E LLMs as Text Encoders LMKE [135] 2022 E LLMs for Joint Text and KG Embedding kNN-KGE [136] 2022 E LLMs for Joint Text and KG Embedding LambdaKG [137] 2023 E+D+ED LLMs for Joint Text and KG Embedding LLM-augmented KG completionKG-BERT [26] 2019 E Joint Encoding MTL-KGC [138] 2020 E Joint Encoding PKGC [139] 2022 E Joint Encoding LASS [140] 2022 E Joint Encoding MEM-KGC [141] 2021 E MLM Encoding OpenWorld KGC [142] 2023 E MLM Encoding StAR [143] 2021 E Separated Encoding SimKGC [144] 2022 E Separated Encoding LP-BERT [145] 2022 E Separated Encoding GenKGC [96] 2022 ED LLM as decoders KGT5 [146] 2022 ED LLM as decoders KG-S2S [147] 2022 ED LLM as decoders AutoKG [93] 2023 D LLM as decoders LLM-augmented KG constructionELMO [148] 2018 E Named Entity Recognition GenerativeNER [149] 2021 ED Named Entity Recognition LDET [150] 2019 E Entity Typing BOX4Types [151] 2021 E Entity Typing ELQ [152] 2020 E Entity Linking ReFinED [153] 2022 E Entity Linking BertCR [154] 2019 E CR (Within-document) Spanbert [155] 2020 E CR (Within-document) CDLM [156] 2021 E CR (Cross-document) CrossCR [157] 2021 E CR (Cross-document) CR-RL [158] 2021 E CR (Cross-document) SentRE [159] 2019 E RE (Sentence-level) Curriculum-RE [160] 2021 E RE (Sentence-level) DREEAM [161] 2023 E RE (Document-level) Kumar et al. [95] 2020 E End-to-End Construction Guo et al. [162] 2021 E End-to-End Construction Grapher [41] 2021 ED End-to-End Construction PiVE [163] 2023 D+ED End-to-End Construction COMET [164] 2019 D Distilling KGs from LLMs BertNet [165] 2022 E Distilling KGs from LLMs West et al. [166] 2022 D Distilling KGs from LLMs',\n",
              "  'BertNet [165] 2022 E Distilling KGs from LLMs West et al. [166] 2022 D Distilling KGs from LLMs LLM-augmented KG-to-text GenerationRibeiro et al [167] 2021 ED Leveraging Knowledge from LLMs JointGT [42] 2021 ED Leveraging Knowledge from LLMs FSKG2Text [168] 2021 D+ED Leveraging Knowledge from LLMs GAP [169] 2022 ED Leveraging Knowledge from LLMs GenWiki [170] 2020 - Constructing KG-text aligned Corpus KGPT [171] 2020 ED Constructing KG-text aligned Corpus LLM-augmented KGQALukovnikov et al. [172] 2019 E Entity/Relation Extractor Luo et al. [173] 2020 E Entity/Relation Extractor QA-GNN [131] 2021 E Entity/Relation Extractor Nan et al. [174] 2023 E+D+ED Entity/Relation Extractor DEKCOR [175] 2021 E Answer Reasoner DRLK [176] 2022 E Answer Reasoner OreoLM [177] 2022 E Answer Reasoner GreaseLM [178] 2022 E Answer Reasoner ReLMKG [179] 2022 E Answer Reasoner UniKGQA [43] 2023 E Answer Reasoner E: Encoder-only LLMs, D: Decoder-only LLMs, ED: Encoder-decoder LLMs. et al. [122] adopt the language model during pre-training to generate knowledge graphs. The knowledge acquired by LLMs during training can be unveiled by the facts in KGs explicitly. To explore how implicit knowledge is stored in parameters of LLMs, Dai et al. [39] propose the concept of knowledge neurons . Specifically, activation of the identified knowledge neurons is highly correlated with knowledge expression. Thus, they explore the knowledge and facts represented by each neuron by suppressing and amplifying knowledge neurons. 5 LLM- AUGMENTED KGS Knowledge graphs are famous for representing knowledge in a structural manner. They have been applied in many downstream tasks such as question answering, recommen- dation, and web search. However, the conventional KGs are often incomplete and existing methods often lack con- sidering textual information. To address these issues, re- cent research has explored integrating LLMs to augment KGs to consider the textual information and improve the performance in',\n",
              "  'integrating LLMs to augment KGs to consider the textual information and improve the performance in downstream tasks. In this section, we will introduce the recent research on LLM-augmented KGs. We will introduce the methods that integrate LLMs for KG embedding, KG completion, KG construction, KG-to-text generation, and KG question answering, respectively. Rep- resentative works are summarized in Table 3. KGsNeil Armstrong Wapakoneta , ( )An American astronaut and aeronautical engineer .A small city in Ohio, USA.LLMs Text TextKGE ModelsKGE T raining Text BornIn ,Fig. 14. LLMs as text encoder for knowledge graph embedding (KGE). 5.1 LLM-augmented KG Embedding Knowledge graph embedding (KGE) aims to map each entity and relation into a low-dimensional vector (embed- ding) space. These embeddings contain both semantic and structural information of KGs, which can be utilized for various tasks such as question answering [180], reasoning [38], and recommendation [181]. Conventional knowledge graph embedding methods mainly rely on the structural information of KGs to optimize a scoring function de- fined on embeddings (e.g., TransE [33], and DisMult [182]). However, these approaches often fall short in representing unseen entities and long-tailed relations due to their limited structural connectivity [183], [184]. To address this issue, as shown in Fig. 14, recent research adopts LLMs to enrich representations of KGs by encoding the textual descriptions of entities and relations [40], [94]. 5.1.1 LLMs as Text Encoders Pretrain-KGE [94] is a representative method that follows the framework shown in Fig. 14. Given a triple (h, r, t)from KGs, it firsts uses a LLM encoder to encode the textual de- scriptions of entities h,t, and relations rinto representations as eh=LLM(Texth), et=LLM(Textt), er=LLM(Textr),(1) where eh, er,andetdenotes the initial embeddings of enti- tiesh,t, and relations r, respectively. Pretrain-KGE uses the BERT as the LLM encoder in experiments. Then, the',\n",
              "  'relations r, respectively. Pretrain-KGE uses the BERT as the LLM encoder in experiments. Then, the initial embeddings are fed into a KGE model to generate the final embeddings vh, vr, and vt. During the KGE training phase, they optimize the KGE model by following the standard KGE loss function as L= [γ+f(vh, vr, vt)−f(v′ h, v′ r, v′ t)], (2) where fis the KGE scoring function, γis a margin hy- perparameter, and v′ h, v′ r, and v′ tare the negative samples. In this way, the KGE model could learn adequate struc- ture information, while reserving partial knowledge from LLM enabling better knowledge graph embedding. KEPLER [40] offers a unified model for knowledge embedding and pre-trained language representation. This model not only generates effective text-enhanced knowledge embedding',\n",
              "  'JOURNAL OF LATEX CLASS FILES, VOL. ??, NO. ??, MONTH 20YY 12 Neil Armstrong BornIn [CLS] [SEP] [MASK]LLMsWapakoneta ( Neil Armstrong, BornIn, W apakoneta)Text [SEP]Mask Entity Prediction KGsText Fig. 15. LLMs for joint text and knowledge graph embedding. using powerful LLMs but also seamlessly integrates factual knowledge into LLMs. Nayyeri et al. [132] use LLMs to gen- erate the world-level, sentence-level, and document-level representations. They are integrated with graph structure embeddings into a unified vector by Dihedron and Quater- nion representations of 4D hypercomplex numbers. Huang et al. [133] combine LLMs with other vision and graph encoders to learn multi-modal knowledge graph embedding that enhances the performance of downstream tasks. CoDEx [134] presents a novel loss function empowered by LLMs that guides the KGE models in measuring the likelihood of triples by considering the textual information. The proposed loss function is agnostic to model structure that can be incorporated with any KGE model. 5.1.2 LLMs for Joint Text and KG Embedding Instead of using KGE model to consider graph structure, another line of methods directly employs LLMs to incorpo- rate both the graph structure and textual information into the embedding space simultaneously. As shown in Fig. 15, kNN-KGE [136] treats the entities and relations as special tokens in the LLM. During training, it transfers each triple (h, r, t)and corresponding text descriptions into a sentence xas x=[CLS] hTexth[SEP] r[SEP] [MASK] Textt[SEP] , (3) where the tailed entities are replaced by [MASK] . The sen- tence is fed into a LLM, which then finetunes the model to predict the masked entity, formulated as PLLM(t|h, r) =P([MASK]=t |x,Θ), (4) where Θdenotes the parameters of the LLM. The LLM is optimized to maximize the probability of the correct entity t. After training, the corresponding token representations in LLMs are used as embeddings for entities and rela- tions. Similarly, LMKE [135] proposes',\n",
              "  'in LLMs are used as embeddings for entities and rela- tions. Similarly, LMKE [135] proposes a contrastive learning method to improve the learning of embeddings generated by LLMs for KGE. Meanwhile, to better capture graph structure, LambdaKG [137] samples 1-hop neighbor entities and concatenates their tokens with the triple as a sentence feeding into LLMs. 5.2 LLM-augmented KG Completion Knowledge Graph Completion (KGC) refers to the task of inferring missing facts in a given knowledge graph. Similar to KGE, conventional KGC methods mainly focused onthe structure of the KG, without considering the exten- sive textual information. However, the recent integration of LLMs enables KGC methods to encode text or generate facts for better KGC performance. These methods fall into two distinct categories based on their utilization styles: 1) LLM as Encoders (PaE) , and 2) LLM as Generators (PaG) . 5.2.1 LLM as Encoders (PaE). As shown in Fig. 16 (a), (b), and (c), this line of work first uses encoder-only LLMs to encode textual information as well as KG facts. Then, they predict the plausibility of the triples or masked entities by feeding the encoded representation into a prediction head, which could be a simple MLP or conventional KG score function (e.g., TransE [33] and TransR [185]). Joint Encoding. Since the encoder-only LLMs (e.g., Bert [1]) are well at encoding text sequences, KG-BERT [26] represents a triple (h, r, t)as a text sequence and encodes it with LLM Fig. 16(a). x=[CLS] Texth[SEP] Textr[SEP] Textt[SEP] ,(5) The final hidden state of the [CLS] token is fed into a classifier to predict the possibility of the triple, formulated as s=σ(MLP(e[CLS])), (6) where σ(·)denotes the sigmoid function and e[CLS] de- notes the representation encoded by LLMs. To improve the efficacy of KG-BERT, MTL-KGC [138] proposed a Multi- Task Learning for the KGC framework which incorporates additional auxiliary tasks into the model’s training, i.e. prediction (RP) and relevance',\n",
              "  'additional auxiliary tasks into the model’s training, i.e. prediction (RP) and relevance ranking (RR). PKGC [139] assesses the validity of a triplet (h, r, t)by transforming the triple and its supporting information into natural language sentences with pre-defined templates. These sentences are then processed by LLMs for binary classification. The sup- porting information of the triplet is derived from the at- tributes of handtwith a verbalizing function. For instance, if the triple is (Lebron James, member of sports team, Lakers) , the information regarding Lebron James is verbalized as ”Lebron James: American basketball player”. LASS [140] observes that language semantics and graph structures are equally vital to KGC. As a result, LASS is proposed to jointly learn two types of embeddings: semantic embedding and structure embedding. In this method, the full text of a triple is forwarded to the LLM, and the mean pooling of the corresponding LLM outputs for h,r, and tare separately calculated. These embeddings are then passed to a graph- based method, i.e. TransE, to reconstruct the KG structures. MLM Encoding. Instead of encoding the full text of a triple, many works introduce the concept of Masked Lan- guage Model (MLM) to encode KG text (Fig. 16(b)). MEM- KGC [141] uses Masked Entity Model (MEM) classification mechanism to predict the masked entities of the triple. The input text is in the form of x=[CLS] Texth[SEP] Textr[SEP] [MASK] [SEP] ,(7) Similar to Eq. 4, it tries to maximize the probability that the masked entity is the correct entity t. Additionally, to enable the model to learn unseen entities, MEM-KGC integrates',\n",
              "  'JOURNAL OF LATEX CLASS FILES, VOL. ??, NO. ??, MONTH 20YY 13 [CLS] Text [SEP] Text [SEP] [MASK] [SEP][CLS] Text [SEP] Text [SEP] Text [SEP] [CLS] Text [SEP] Text [SEP] [CLS] Text [SEP] LLMs LLMs LLMsLLMs (a) Joint Encoding (b) MLM Encoding (c) Separated EncodingMLP 0/1 MLP Entity Score Function ScoreTriple: Text Sequence: [CLS] Text [SEP] Text [SEP] Text [SEP] Fig. 16. The general framework of adopting LLMs as encoders (PaE) for KG Completion. multitask learning for entities and super-class prediction based on the text description of entities: x=[CLS] [MASK] [SEP] Texth[SEP] . (8) OpenWorld KGC [142] expands the MEM-KGC model to address the challenges of open-world KGC with a pipeline framework, where two sequential MLM-based modules are defined: Entity Description Prediction (EDP), an auxiliary module that predicts a corresponding entity with a given textual description; Incomplete Triple Prediction (ITP), the target module that predicts a plausible entity for a given incomplete triple (h, r,?). EDP first encodes the triple with Eq. 8 and generates the final hidden state, which is then forwarded into ITP as an embedding of the head entity in Eq. 7 to predict target entities. Separated Encoding. As shown in Fig. 16(c), these meth- ods involve partitioning a triple (h, r, t)into two distinct parts, i.e. (h, r)andt, which can be expressed as x(h,r)=[CLS] Texth[SEP] Textr[SEP] , (9) xt=[CLS] Textt[SEP] . (10) Then the two parts are encoded separately by LLMs, and the final hidden states of the [CLS] tokens are used as the rep- resentations of (h, r)andt, respectively. The representations are then fed into a scoring function to predict the possibility of the triple, formulated as s=fscore(e(h,r), et), (11) where fscore denotes the score function like TransE. StAR [143] applies Siamese-style textual encoders on their text, encoding them into separate contextualized rep- resentations. To avoid the combinatorial explosion of textual encoding approaches, e.g., KG-BERT, StAR',\n",
              "  'To avoid the combinatorial explosion of textual encoding approaches, e.g., KG-BERT, StAR employs a scor- ing module that involves both deterministic classifier and spatial measurement for representation and structure learn- ing respectively, which also enhances structured knowledge by exploring the spatial characteristics. SimKGC [144] is LLMs (En.) [SEP] Text [SEP] Text [SEP][SEP] Text [SEP] (a) Encoder-Decoder PaG LLMs (De.) (a) Decoder-Only PaG [SEP] Text [SEP] Text [SEP]LLMs (De.)[SEP] Text [SEP] Query T riple: Text Sequence: [CLS] Text [SEP] Text [SEP]Fig. 17. The general framework of adopting LLMs as decoders (PaG) for KG Completion. The En. and De. denote the encoder and decoder, respectively. another instance of leveraging a Siamese textual encoder to encode textual representations. Following the encoding process, SimKGC applies contrastive learning techniques to these representations. This process involves computing the similarity between the encoded representations of a given triple and its positive and negative samples. In particular, the similarity between the encoded representation of the triple and the positive sample is maximized, while the sim- ilarity between the encoded representation of the triple and the negative sample is minimized. This enables SimKGC to learn a representation space that separates plausible and implausible triples. To avoid overfitting textural in- formation, CSPromp-KG [186] employs parameter-efficient prompt learning for KGC. LP-BERT [145] is a hybrid KGC method that combines both MLM Encoding and Separated Encoding. This ap- proach consists of two stages, namely pre-training and fine-tuning. During pre-training, the method utilizes the standard MLM mechanism to pre-train a LLM with KGC data. During the fine-tuning stage, the LLM encodes both parts and is optimized using a contrastive learning strategy (similar to SimKGC [144]). 5.2.2 LLM as Generators (PaG). Recent works use LLMs as sequence-to-sequence generators in KGC.',\n",
              "  '5.2.2 LLM as Generators (PaG). Recent works use LLMs as sequence-to-sequence generators in KGC. As presented in Fig. 17 (a) and (b), these approaches involve encoder-decoder or decoder-only LLMs. The LLMs receive a sequence text input of the query triple (h, r,?), and generate the text of tail entity tdirectly. GenKGC [96] uses the large language model BART [5] as the backbone model. Inspired by the in-context learning approach used in GPT-3 [59], where the model concatenates relevant samples to learn correct output answers, GenKGC proposes a relation-guided demonstration technique that includes triples with the same relation to facilitating the model’s learning process. In addition, during generation, an entity-aware hierarchical decoding method is proposed to reduce the time complexity. KGT5 [146] introduces a',\n",
              "  \"JOURNAL OF LATEX CLASS FILES, VOL. ??, NO. ??, MONTH 20YY 14 LLMs Given head entity and relation, predict the tail entity from the candidates: [ 100 candidates ] Head: Charlie's Angels Relation: genre of Tail: Comedy-GB Head: Charlie's Angels Relation: prequel of Tail:Charlie's Angels: Full Throttle Fig. 18. The framework of prompt-based PaG for KG Completion. novel KGC model that fulfils four key requirements of such models: scalability, quality, versatility, and simplicity. To address these objectives, the proposed model employs a straightforward T5 small architecture. The model is distinct from previous KGC methods, in which it is randomly ini- tialized rather than using pre-trained models. KG-S2S [147] is a comprehensive framework that can be applied to var- ious types of KGC tasks, including Static KGC, Temporal KGC, and Few-shot KGC. To achieve this objective, KG-S2S reformulates the standard triple KG fact by introducing an additional element, forming a quadruple (h, r, t, m ), where mrepresents the additional ”condition” element. Although different KGC tasks may refer to different conditions, they typically have a similar textual format, which enables uni- fication across different KGC tasks. The KG-S2S approach incorporates various techniques such as entity description, soft prompt, and Seq2Seq Dropout to improve the model’s performance. In addition, it utilizes constrained decoding to ensure the generated entities are valid. For closed-source LLMs (e.g., ChatGPT and GPT-4), AutoKG adopts prompt engineering to design customized prompts [93]. As shown in Fig. 18, these prompts contain the task description, few- shot examples, and test input, which instruct LLMs to predict the tail entity for KG completion. Comparison between PaE and PaG. LLMs as Encoders (PaE) applies an additional prediction head on the top of the representation encoded by LLMs. Therefore, the PaE framework is much easier to finetune since we can only optimize the prediction heads and\",\n",
              "  'the PaE framework is much easier to finetune since we can only optimize the prediction heads and freeze the LLMs. More- over, the output of the prediction can be easily specified and integrated with existing KGC functions for different KGC tasks. However, during the inference stage, the PaE requires to compute a score for every candidate in KGs, which could be computationally expensive. Besides, they cannot generalize to unseen entities. Furthermore, the PaE requires the representation output of the LLMs, whereas some state-of-the-art LLMs (e.g. GPT-41) are closed sources and do not grant access to the representation output. LLMs as Generators (PaG), on the other hand, which does not need the prediction head, can be used without finetuning or access to representations. Therefore, the frame- work of PaG is suitable for all kinds of LLMs. In addition, PaG directly generates the tail entity, making it efficientin inference without ranking all the candidates and easily generalizing to unseen entities. But, the challenge of PaG is that the generated entities could be diverse and not lie in KGs. What is more, the time of a single inference is longer due to the auto-regressive generation. Last, how to design a powerful prompt that feeds KGs into LLMs is still an open question. Consequently, while PaG has demonstrated promising results for KGC tasks, the trade-off between model complexity and computational efficiency must be carefully considered when selecting an appropriate LLM- based KGC framework. 5.2.3 Model Analysis Justin et al. [187] provide a comprehensive analysis of KGC methods integrated with LLMs. Their research investigates the quality of LLM embeddings and finds that they are suboptimal for effective entity ranking. In response, they propose several techniques for processing embeddings to improve their suitability for candidate retrieval. The study also compares different model selection dimensions, such as Embedding Extraction, Query Entity Extraction, and',\n",
              "  'different model selection dimensions, such as Embedding Extraction, Query Entity Extraction, and Lan- guage Model Selection. Lastly, the authors propose a frame- work that effectively adapts LLM for knowledge graph completion. 5.3 LLM-augmented KG Construction Knowledge graph construction involves creating a struc- tured representation of knowledge within a specific domain. This includes identifying entities and their relationships with each other. The process of knowledge graph construc- tion typically involves multiple stages, including 1) entity discovery ,2) coreference resolution , and 3) relation extraction . Fig 19 presents the general framework of applying LLMs for each stage in KG construction. More recent approaches have explored 4) end-to-end knowledge graph construction , which involves constructing a complete knowledge graph in one step or directly 5) distilling knowledge graphs from LLMs . 5.3.1 Entity Discovery Entity discovery in KG construction refers to the process of identifying and extracting entities from unstructured data sources, such as text documents, web pages, or social me- dia posts, and incorporating them to construct knowledge graphs. Named Entity Recognition (NER) involves identifying and tagging named entities in text data with their positions and classifications. The named entities include people, or- ganizations, locations, and other types of entities. The state- of-the-art NER methods usually employ LLMs to leverage their contextual understanding and linguistic knowledge for accurate entity recognition and classification. There are three NER sub-tasks based on the types of NER spans identified, i.e., flat NER, nested NER, and discontinuous NER. 1) Flat NER is to identify non-overlapping named entities from input text. It is usually conceptualized as a sequence labelling problem where each token in the text is assigned a unique label based on its position in the sequence [1], [148], [188], [189]. 2) Nested NER considers complex',\n",
              "  'based on its position in the sequence [1], [148], [188], [189]. 2) Nested NER considers complex scenarios which allow a token to belong to multiple entities. The span- based method [190]–[194] is a popular branch of nested',\n",
              "  'JOURNAL OF LATEX CLASS FILES, VOL. ??, NO. ??, MONTH 20YY 15 Text: Joe Biden was born in Pennsylvania. He serves as the 46th President of the United States.... ... United StatesPennsylvania Joe Biden politician state countryBornIn PresidentOfIsA politician countryIsAstateKnowledge Graph IsA LLM-based Knowledge Graph Construction Named Entity RecognitionEntity Typing Entity LinkingCoreference ResolutionRelation ExtractionJoe Biden was born in Pennsylvania. He serves as the 46th President of the United States. Fig. 19. The general framework of LLM-based KG construction. NER which involves enumerating all candidate spans and classifying them into entity types (including a non-entity type). Parsing-based methods [195]–[197] reveal similarities between nested NER and constituency parsing tasks (pre- dicting nested and non-overlapping spans), and propose to integrate the insights of constituency parsing into nested NER. 3) Discontinuous NER identifies named entities that may not be contiguous in the text. To address this challenge, [198] uses the LLM output to identify entity fragments and deter- mine whether they are overlapped or in succession. Unlike the task-specific methods, GenerativeNER [149] uses a sequence-to-sequence LLM with a pointer mecha- nism to generate an entity sequence, which is capable of solving all three types of NER sub-tasks. Entity Typing (ET) aims to provide fine-grained and ultra-grained type information for a given entity men- tioned in context. These methods usually utilize LLM to encode mentions, context and types. LDET [150] applies pre- trained ELMo embeddings [148] for word representation and adopts LSTM as its sentence and mention encoders. BOX4Types [151] recognizes the importance of type depen- dency and uses BERT to represent the hidden vector and each type in a hyperrectangular (box) space. LRN [199] considers extrinsic and intrinsic dependencies between la- bels. It encodes the context and entity with BERT and employs these output',\n",
              "  'dependencies between la- bels. It encodes the context and entity with BERT and employs these output embeddings to conduct deductive and inductive reasoning. MLMET [200] uses predefined patterns to construct input samples for the BERT MLM and employs [MASK] to predict context-dependent hypernyms of the mention, which can be viewed as type labels. PL [201] and DFET [202] utilize prompt learning for entity typing. LITE [203] formulates entity typing as textual inference and uses RoBERTa-large-MNLI as the backbone network. Entity Linking (EL) , as known as entity disambiguation, involves linking entity mentions appearing in the text to their corresponding entities in a knowledge graph. [204] proposed BERT-based end-to-end EL systems that jointly discover and link entities. ELQ [152] employs a fast bi- encoder architecture to jointly perform mention detectionand linking in one pass for downstream question answering systems. Unlike previous models that frame EL as matching in vector space, GENRE [205] formulates it as a sequence-to- sequence problem, autoregressively generating a version of the input markup-annotated with the unique identifiers of an entity expressed in natural language. GENRE is extended to its multilingual version mGENRE [206]. Considering the efficiency challenges of generative EL approaches, [207] par- allelizes autoregressive linking across all potential mentions and relies on a shallow and efficient decoder. ReFinED [153] proposes an efficient zero-shot-capable EL approach by taking advantage of fine-grained entity types and entity descriptions which are processed by a LLM-based encoder. 5.3.2 Coreference Resolution (CR) Coreference resolution is to find all expressions (i.e., men- tions) that refer to the same entity or event in a text. Within-document CR refers to the CR sub-task where all these mentions are in a single document. Mandar et al. [154] initialize LLM-based coreferences resolution by replacing the previous LSTM encoder [208] with',\n",
              "  'initialize LLM-based coreferences resolution by replacing the previous LSTM encoder [208] with BERT. This work is followed by the introduction of SpanBERT [155] which is pre-trained on BERT architecture with a span-based masked language model (MLM). Inspired by these works, Tuan Manh et al. [209] present a strong baseline by incorporat- ing the SpanBERT encoder into a non-LLM approach e2e- coref [208]. CorefBERT leverages Mention Reference Predic- tion (MRP) task which masks one or several mentions and requires the model to predict the masked mention’s corre- sponding referents. CorefQA [210] formulates coreference resolution as a question answering task, where contextual queries are generated for each candidate mention and the coreferent spans are extracted from the document using the queries. Tuan Manh et al. [211] introduce a gating mech- anism and a noisy training method to extract information from event mentions using the SpanBERT encoder. In order to reduce the large memory footprint faced by large LLM-based NER models, Yuval et al. [212] and Raghuveer el al. [213] proposed start-to-end and approxima- tion models, respectively, both utilizing bilinear functions to calculate mention and antecedent scores with reduced reliance on span-level representations. Cross-document CR refers to the sub-task where the mentions refer to the same entity or event might be across multiple documents. CDML [156] proposes a cross docu- ment language modeling method which pre-trains a Long- former [214] encoder on concatenated related documents and employs an MLP for binary classification to determine whether a pair of mentions is coreferent or not. CrossCR [157] utilizes an end-to-end model for cross-document coref- erence resolution which pre-trained the mention scorer on gold mention spans and uses a pairwise scorer to compare mentions with all spans across all documents. CR-RL [158] proposes an actor-critic deep reinforcement learning-based coreference resolver for',\n",
              "  'CR-RL [158] proposes an actor-critic deep reinforcement learning-based coreference resolver for cross-document CR. 5.3.3 Relation Extraction (RE) Relation extraction involves identifying semantic relation- ships between entities mentioned in natural language text. There are two types of relation extraction methods, i.e.',\n",
              "  'JOURNAL OF LATEX CLASS FILES, VOL. ??, NO. ??, MONTH 20YY 16 sentence-level RE and document-level RE, according to the scope of the text analyzed. Sentence-level RE focuses on identifying relations be- tween entities within a single sentence. Peng et al. [159] and TRE [215] introduce LLM to improve the performance of relation extraction models. BERT-MTB [216] learns relation representations based on BERT by performing the matching- the-blanks task and incorporating designed objectives for relation extraction. Curriculum-RE [160] utilizes curriculum learning to improve relation extraction models by gradu- ally increasing the difficulty of the data during training. RECENT [217] introduces SpanBERT and exploits entity type restriction to reduce the noisy candidate relation types. Jiewen [218] extends RECENT by combining both the entity information and the label information into sentence-level embeddings, which enables the embedding to be entity- label aware. Document-level RE (DocRE) aims to extract relations between entities across multiple sentences within a docu- ment. Hong et al. [219] propose a strong baseline for DocRE by replacing the BiLSTM backbone with LLMs. HIN [220] use LLM to encode and aggregate entity representation at different levels, including entity, sentence, and document levels. GLRE [221] is a global-to-local network, which uses LLM to encode the document information in terms of entity global and local representations as well as context relation representations. SIRE [222] uses two LLM-based encoders to extract intra-sentence and inter-sentence relations. LSR [223] and GAIN [224] propose graph-based approaches which induce graph structures on top of LLM to better extract relations. DocuNet [225] formulates DocRE as a semantic segmentation task and introduces a U-Net [226] on the LLM encoder to capture local and global dependencies between entities. ATLOP [227] focuses on the multi-label problems in DocRE, which could be handled with two',\n",
              "  \"entities. ATLOP [227] focuses on the multi-label problems in DocRE, which could be handled with two techniques, i.e., adaptive thresholding for classifier and localized con- text pooling for LLM. DREEAM [161] further extends and improves ATLOP by incorporating evidence information. End-to-End KG Construction. Currently, researchers are exploring the use of LLMs for end-to-end KG construction. Kumar et al. [95] propose a unified approach to build KGs from raw text, which contains two LLMs powered components. They first finetune a LLM on named entity recognition tasks to make it capable of recognizing entities in raw text. Then, they propose another “2-model BERT” for solving the relation extraction task, which contains two BERT-based classifiers. The first classifier learns the relation class whereas the second binary classifier learns the direc- tion of the relations between the two entities. The predicted triples and relations are then used to construct the KG. Guo et al. [162] propose an end-to-end knowledge extraction model based on BERT, which can be applied to construct KGs from Classical Chinese text. Grapher [41] presents a novel end-to-end multi-stage system. It first utilizes LLMs to generate KG entities, followed by a simple relation con- struction head, enabling efficient KG construction from the textual description. PiVE [163] proposes a prompting with an iterative verification framework that utilizes a smaller LLM like T5 to correct the errors in KGs generated by a larger LLM (e.g., ChatGPT). To further explore advanced LLMs, AutoKG design several prompts for different KG Brarck Obama PoliticianOf USAHonolulu BornIn LocatedIn CapitalOfMarriedT o Michelle ObamaLiveInConstruct KGs LLMsObama born in [MASK] Honolulu is located in [MASK] USA's capital is [MASK]Cloze Question (Obama, BornIn, Honolulu) (Honolulu, LocatedIn, USA) (Washingto D.C., CapitalOf, USA) Washingto D.C.Distilled T riplesFig. 20. The general framework of distilling KGs from LLMs.\",\n",
              "  'USA) Washingto D.C.Distilled T riplesFig. 20. The general framework of distilling KGs from LLMs. construction tasks (e.g., entity typing, entity linking, and relation extraction). Then, it adopts the prompt to perform KG construction using ChatGPT and GPT-4. 5.3.4 Distilling Knowledge Graphs from LLMs LLMs have been shown to implicitly encode massive knowl- edge [14]. As shown in Fig. 20, some research aims to distill knowledge from LLMs to construct KGs. COMET [164] proposes a commonsense transformer model that constructs commonsense KGs by using existing tuples as a seed set of knowledge on which to train. Using this seed set, a LLM learns to adapt its learned representations to knowledge generation, and produces novel tuples that are high quality. Experimental results reveal that implicit knowledge from LLMs is transferred to generate explicit knowledge in com- monsense KGs. BertNet [165] proposes a novel framework for automatic KG construction empowered by LLMs. It re- quires only the minimal definition of relations as inputs and automatically generates diverse prompts, and performs an efficient knowledge search within a given LLM for consis- tent outputs. The constructed KGs show competitive quality, diversity, and novelty with a richer set of new and complex relations, which cannot be extracted by previous methods. West et al. [166] propose a symbolic knowledge distillation framework that distills symbolic knowledge from LLMs. They first finetune a small student LLM by distilling com- monsense facts from a large LLM like GPT-3. Then, the student LLM is utilized to generate commonsense KGs. 5.4 LLM-augmented KG-to-text Generation The goal of Knowledge-graph-to-text (KG-to-text) genera- tion is to generate high-quality texts that accurately and consistently describe the input knowledge graph infor- mation [228]. KG-to-text generation connects knowledge graphs and texts, significantly improving the applicability of KG in more realistic NLG scenarios, including',\n",
              "  'texts, significantly improving the applicability of KG in more realistic NLG scenarios, including story- telling [229] and knowledge-grounded dialogue [230]. How- ever, it is challenging and costly to collect large amounts of graph-text parallel data, resulting in insufficient training and poor generation quality. Thus, many research efforts re- sort to either: 1) leverage knowledge from LLMs or2) construct large-scale weakly-supervised KG-text corpus to solve this issue. 5.4.1 Leveraging Knowledge from LLMs As pioneering research efforts in using LLMs for KG-to-Text generation, Ribeiro et al. [167] and Kale and Rastogi [231] directly fine-tune various LLMs, including BART and T5, with the goal of transferring LLMs knowledge for this',\n",
              "  'JOURNAL OF LATEX CLASS FILES, VOL. ??, NO. ??, MONTH 20YY 17 Brarck Obama PoliticianOf USAHonolulu BornIn LocatedIn CapitalOf Washingto D.C.MarriedT o Michelle ObamaLiveInKGs LLMsBrack Obama is a politician of USA . He was born in Honolulu , and married to Michelle Obama . Graph Linearization Brack Obama [SEP] PoliticianOf [SEP] USA [SEP] ..... [SEP] Michelle Obama Description T ext Fig. 21. The general framework of KG-to-text generation. task. As shown in Fig. 21, both works simply represent the input graph as a linear traversal and find that such a naive approach successfully outperforms many existing state-of-the-art KG-to-text generation systems. Interestingly, Ribeiro et al. [167] also find that continue pre-training could further improve model performance. However, these meth- ods are unable to explicitly incorporate rich graph semantics in KGs. To enhance LLMs with KG structure information, JointGT [42] proposes to inject KG structure-preserving representations into the Seq2Seq large language models. Given input sub-KGs and corresponding text, JointGT first represents the KG entities and their relations as a sequence of tokens, then concatenate them with the textual tokens which are fed into LLM. After the standard self-attention module, JointGT then uses a pooling layer to obtain the contextual semantic representations of knowledge entities and relations. Finally, these pooled KG representations are then aggregated in another structure-aware self-attention layer. JointGT also deploys additional pre-training objec- tives, including KG and text reconstruction tasks given masked inputs, to improve the alignment between text and graph information. Li et al. [168] focus on the few-shot scenario. It first employs a novel breadth-first search (BFS) strategy to better traverse the input KG structure and feed the enhanced linearized graph representations into LLMs for high-quality generated outputs, then aligns the GCN- based and LLM-based KG entity representation.',\n",
              "  'high-quality generated outputs, then aligns the GCN- based and LLM-based KG entity representation. Colas et al. [169] first transform the graph into its appropriate repre- sentation before linearizing the graph. Next, each KG node is encoded via a global attention mechanism, followed by a graph-aware attention module, ultimately being decoded into a sequence of tokens. Different from these works, KG- BART [37] keeps the structure of KGs and leverages the graph attention to aggregate the rich concept semantics in the sub-KG, which enhances the model generalization on unseen concept sets. 5.4.2 Constructing large weakly KG-text aligned Corpus Although LLMs have achieved remarkable empirical suc- cess, their unsupervised pre-training objectives are not nec- essarily aligned well with the task of KG-to-text genera- tion, motivating researchers to develop large-scale KG-text aligned corpus. Jin et al. [170] propose a 1.3M unsupervised KG-to-graph training data from Wikipedia. Specifically, they first detect the entities appearing in the text via hyperlinks and named entity detectors, and then only add text that shares a common set of entities with the correspondingknowledge graph, similar to the idea of distance supervision in the relation extraction task [232]. They also provide a 1,000+ human annotated KG-to-Text test data to verify the effectiveness of the pre-trained KG-to-Text models. Simi- larly, Chen et al. [171] also propose a KG-grounded text corpus collected from the English Wikidump. To ensure the connection between KG and text, they only extract sentences with at least two Wikipedia anchor links. Then, they use the entities from those links to query their surrounding neighbors in WikiData and calculate the lexical overlapping between these neighbors and the original sentences. Finally, only highly overlapped pairs are selected. The authors ex- plore both graph-based and sequence-based encoders and identify their advantages in various different tasks and',\n",
              "  'and sequence-based encoders and identify their advantages in various different tasks and settings. 5.5 LLM-augmented KG Question Answering Knowledge graph question answering (KGQA) aims to find answers to natural language questions based on the struc- tured facts stored in knowledge graphs [233], [234]. The inevitable challenge in KGQA is to retrieve related facts and extend the reasoning advantage of KGs to QA. Therefore, recent studies adopt LLMs to bridge the gap between nat- ural language questions and structured knowledge graphs [174], [175], [235]. The general framework of applying LLMs for KGQA is illustrated in Fig. 22, where LLMs can be used as 1) entity/relation extractors, and 2) answer reasoners. 5.5.1 LLMs as Entity/relation Extractors Entity/relation extractors are designed to identify entities and relationships mentioned in natural language questions and retrieve related facts in KGs. Given the proficiency in language comprehension, LLMs can be effectively utilized for this purpose. Lukovnikov et al. [172] are the first to uti- lize LLMs as classifiers for relation prediction, resulting in a notable improvement in performance compared to shallow neural networks. Nan et al. [174] introduce two LLM-based KGQA frameworks that adopt LLMs to detect mentioned entities and relations. Then, they query the answer in KGs using the extracted entity-relation pairs. QA-GNN [131] uses LLMs to encode the question and candidate answer pairs, which are adopted to estimate the importance of relative KG entities. The entities are retrieved to form a subgraph, where an answer reasoning is conducted by a graph neural network. Luo et al. [173] use LLMs to calculate the similarities between relations and questions to retrieve related facts, formulated as s(r, q) =LLM(r)⊤LLM(q), (12) where qdenotes the question, rdenotes the relation, and LLM(·)would generate representation for qandr, respec- tively. Furthermore, Zhang et al. [236] propose a LLM-based path retriever to',\n",
              "  'for qandr, respec- tively. Furthermore, Zhang et al. [236] propose a LLM-based path retriever to retrieve question-related relations hop-by- hop and construct several paths. The probability of each path can be calculated as P(p|q) =|p|Y t=1s(rt, q), (13) where pdenotes the path, and rtdenotes the relation at the t-th hop of p. The retrieved relations and paths can be used',\n",
              "  'JOURNAL OF LATEX CLASS FILES, VOL. ??, NO. ??, MONTH 20YY 18 Question: Where was Neil Armstrong born in?LLMsRelation/entity ExtractorNeil Armstrong BornIn Entity RelationKGsRetrieve in KGsQuestion [CLS] [SEP] Related Facts Candidates [SEP] [SEP]LLMsAnswer ReasonerScore Fig. 22. The general framework of applying LLMs for knowledge graph question answering (KGQA). as context knowledge to improve the performance of answer reasoners as P(a|q) =X p∈PP(a|p)P(p|q), (14) where Pdenotes retrieved paths and adenotes the answer. 5.5.2 LLMs as Answer Reasoners Answer reasoners are designed to reason over the retrieved facts and generate answers. LLMs can be used as answer reasoners to generate answers directly. For example, as shown in Fig. 3 22, DEKCOR [175] concatenates the re- trieved facts with questions and candidate answers as x=[CLS] q[SEP] Related Facts [SEP] a[SEP] ,(15) where adenotes candidate answers. Then, it feeds them into LLMs to predict answer scores. After utilizing LLMs to generate the representation of xas QA context, DRLK [176] proposes a Dynamic Hierarchical Reasoner to capture the interactions between QA context and answers for answer prediction. Yan et al. [235] propose a LLM-based KGQA framework consisting of two stages: (1) retrieve related facts from KGs and (2) generate answers based on the retrieved facts. The first stage is similar to the entity/relation extractors. Given a candidate answer entity a, it extracts a series of paths p1, . . . , p nfrom KGs. But the second stage is a LLM-based answer reasoner. It first verbalizes the paths by using the entity names and relation names in KGs. Then, it concatenates the question qand all paths p1, . . . , p nto make an input sample as x=[CLS] q[SEP] p1[SEP] ···[SEP] pn[SEP] .(16) These paths are regarded as the related facts for the can- didate answer a. Finally, it uses LLMs to predict whether the hypothesis: “ ais the answer of q” is supported by those facts, which is formulated as e[CLS] =LLM(x), (17)',\n",
              "  '“ ais the answer of q” is supported by those facts, which is formulated as e[CLS] =LLM(x), (17) s=σ(MLP(e[CLS])), (18) where it encodes xusing a LLM and feeds representation corresponding to [CLS] token for binary classification, and σ(·)denotes the sigmoid function.TABLE 4 Summary of methods that synergize KGs and LLMs. Task Method Year Synergized Knowledge representationJointGT [42] 2021 KEPLER [40] 2021 DRAGON [44] 2022 HKLM [238] 2023 Synergized ReasoningLARK [45] 2023 Siyuan et al. [46] 2023 KSL [239] 2023 StructGPT [237] 2023 Think-on-graph [240] 2023 To better guide LLMs reason through KGs, OreoLM [177] proposes a Knowledge Interaction Layer (KIL) which is in- serted amid LLM layers. KIL interacts with a KG reasoning module, where it discovers different reasoning paths, and then the reasoning module can reason over the paths to generate answers. GreaseLM [178] fuses the representations from LLMs and graph neural networks to effectively reason over KG facts and language context. UniKGQA [43] unifies the facts retrieval and reasoning into a unified framework. UniKGQA consists of two modules. The first module is a semantic matching module that uses a LLM to match questions with their corresponding relations semantically. The second module is a matching information propagation module, which propagates the matching information along directed edges on KGs for answer reasoning. Similarly, ReLMKG [179] performs joint reasoning on a large language model and the associated knowledge graph. The question and verbalized paths are encoded by the language model, and different layers of the language model produce outputs that guide a graph neural network to perform message pass- ing. This process utilizes the explicit knowledge contained in the structured knowledge graph for reasoning purposes. StructGPT [237] adopts a customized interface to allow large language models (e.g., ChatGPT) directly reasoning on KGs to perform multi-step question answering. 6 S YNERGIZED LLM S+',\n",
              "  'ChatGPT) directly reasoning on KGs to perform multi-step question answering. 6 S YNERGIZED LLM S+ KG S The synergy of LLMs and KGs has attracted increasing attention these years, which marries the merits of LLMs and KGs to mutually enhance performance in various down- stream applications. For example, LLMs can be used to understand natural language, while KGs are treated as a knowledge base, which provides factual knowledge. The unification of LLMs and KGs could result in a powerful model for knowledge representation and reasoning. In this section, we will discuss the state-of-the-art Syn- ergized LLMs + KGs from two perspectives: 1) Synergized Knowledge Representation , and 2)Synergized Reasoning. Rep- resentative works are summarized in Table 4. 6.1 Synergized Knowledge Representation Text corpus and knowledge graphs both contain enormous knowledge. However, the knowledge in text corpus is usually implicit and unstructured, while the knowledge in KGs is explicit and structured. Synergized Knowledge Representation aims to design a synergized model that can',\n",
              "  'JOURNAL OF LATEX CLASS FILES, VOL. ??, NO. ??, MONTH 20YY 19 Text-Knowledge Fusion Module Self-Attention Input T extSelf-Attention Self-AttentionT-encoderN LayersText Outputs Knowledge Graph Outputs M LayersK-encoder Knowledge Graph Fig. 23. Synergized knowledge representation by additional KG fusion modules. effectively represent knowledge from both LLMs and KGs. The synergized model can provide a better understanding of the knowledge from both sources, making it valuable for many downstream tasks. To jointly represent the knowledge, researchers propose the synergized models by introducing additional KG fu- sion modules, which are jointly trained with LLMs. As shown in Fig. 23, ERNIE [35] proposes a textual-knowledge dual encoder architecture where a T-encoder first encodes the input sentences, then a K-encoder processes knowledge graphs which are fused them with the textual representation from the T-encoder . BERT-MK [241] employs a similar dual- encoder architecture but it introduces additional informa- tion of neighboring entities in the knowledge encoder com- ponent during the pre-training of LLMs. However, some of the neighboring entities in KGs may not be relevant to the input text, resulting in extra redundancy and noise. Coke- BERT [242] focuses on this issue and proposes a GNN-based module to filter out irrelevant KG entities using the input text. JAKET [243] proposes to fuse the entity information in the middle of the large language model. KEPLER [40] presents a unified model for knowledge embedding and pre-trained language representation. In KE- PLER, they encode textual entity descriptions with a LLM as their embeddings, and then jointly optimize the knowledge embedding and language modeling objectives. JointGT [42] proposes a graph-text joint representation learning model, which proposes three pre-training tasks to align represen- tations of graph and text. DRAGON [44] presents a self- supervised method to pre-train a joint language-knowledge',\n",
              "  'and text. DRAGON [44] presents a self- supervised method to pre-train a joint language-knowledge foundation model from text and KG. It takes text segments and relevant KG subgraphs as input and bidirectionally fuses information from both modalities. Then, DRAGON utilizes two self-supervised reasoning tasks, i.e., masked language modeling and KG link prediction to optimize the model parameters. HKLM [238] introduces a unified LLM which incorporates KGs to learn representations of domain- specific knowledge. 6.2 Synergized Reasoning To better utilize the knowledge from text corpus and knowl- edge graph reasoning, Synergized Reasoning aims to design a synergized model that can effectively conduct reasoning with both LLMs and KGs. Question <SEP> Option LLM EncoderKG EncoderJoint Reasoning LayerDynamic Pruning LM Rep. KG Rep.LM to KG Att. KG to LM Att.LM to KG AttentionAnswer Answer InferenceFig. 24. The framework of LLM-KG Fusion Reasoning. LLM-KG Fusion Reasoning. LLM-KG Fusion Reasoning leverages two separated LLM and KG encoders to process the text and relevant KG inputs [244]. These two encoders are equally important and jointly fusing the knowledge from two sources for reasoning. To improve the interac- tion between text and knowledge, KagNet [38] proposes to first encode the input KG, and then augment the input textual representation. In contrast, MHGRN [234] uses the final LLM outputs of the input text to guide the reasoning process on the KGs. Yet, both of them only design a single- direction interaction between the text and KGs. To tackle this issue, QA-GNN [131] proposes to use a GNN-based model to jointly reason over input context and KG information via message passing. Specifically, QA-GNN represents the input textual information as a special node via a pooling operation and connects this node with other entities in KG. However, the textual inputs are only pooled into a single dense vector, limiting the information fusion performance. JointLK [245] then',\n",
              "  'pooled into a single dense vector, limiting the information fusion performance. JointLK [245] then proposes a framework with fine-grained interaction between any tokens in the textual inputs and any KG entities through LM-to-KG and KG-to-LM bi-directional attention mechanism. As shown in Fig. 24, pairwise dot- product scores are calculated over all textual tokens and KG entities, the bi-directional attentive scores are computed sep- arately. In addition, at each jointLK layer, the KGs are also dynamically pruned based on the attention score to allow later layers to focus on more important sub-KG structures. Despite being effective, in JointLK, the fusion process be- tween the input text and KG still uses the final LLM outputs as the input text representations. GreaseLM [178] designs deep and rich interaction between the input text tokens and KG entities at each layer of the LLMs. The architecture and fusion approach is mostly similar to ERNIE [35] discussed in Section 6.1, except that GreaseLM does not use the text-only T-encoder to handle input text. LLMs as Agents Reasoning. Instead using two encoders to fuse the knowledge, LLMs can also be treated as agents to interact with the KGs to conduct reasoning [246], as illustrated in Fig. 25. KD-CoT [247] iteratively retrieves facts from KGs and produces faithful reasoning traces, which guide LLMs to generate answers. KSL [239] teaches LLMs to search on KGs to retrieve relevant facts and then generate answers. StructGPT [237] designs several API interfaces to allow LLMs to access the structural data and perform rea- soning by traversing on KGs. Think-on-graph [240] provides a flexible plug-and-play framework where LLM agents it- eratively execute beam searches on KGs to discover the',\n",
              "  'JOURNAL OF LATEX CLASS FILES, VOL. ??, NO. ??, MONTH 20YY 20 Born_inKnowledge Graph Question: Which country is Barack Obama from ?Marry_toMichelle Obama Barack ObamaHonolulu City_of Located_in HawaiiUSA1776 Founded_in LLM agent Answer: USA KGsLLMsReasoning-on-Graphs Fig. 25. Using LLMs as agents for reasoning on KGs. reasoning paths and generate answers. To enhance the agent abilities, AgentTuning [248] presents several instruction- tuning datasets to guide LLM agents to perform reasoning on KGs. Comparison and Discussion. LLM-KG Fusion Reasoning combines the LLM encoder and KG encoder to represent knowledge in a unified manner. It then employs a syner- gized reasoning module to jointly reason the results. This framework allows for different encoders and reasoning modules, which are trained end-to-end to effectively utilize the knowledge and reasoning capabilities of LLMs and KGs. However, these additional modules may introduce extra parameters and computational costs while lacking inter- pretability. LLMs as Agents for KG reasoning provides a flexible framework for reasoning on KGs without additional training cost, which can be generalized to different LLMs and KGs. Meanwhile, the reasoning process is interpretable, which can be used to explain the results. Nevertheless, defining the actions and policies for LLM agents is also chal- lenging. The synergy of LLMs and KGs is still an ongoing research topic, with the potential to have more powerful frameworks in the future. 7 F UTURE DIRECTIONS AND MILESTONES In this section, we discuss the future directions and several milestones in the research area of unifying KGs and LLMs. 7.1 KGs for Hallucination Detection in LLMs The hallucination problem in LLMs, which generates fac- tually incorrect content, significantly hinders the reliability of LLMs. As discussed in Section 4, existing studies try to utilize KGs to obtain more reliable LLMs through pre- training or KG-enhanced inference. Despite the efforts, the issue of',\n",
              "  'reliable LLMs through pre- training or KG-enhanced inference. Despite the efforts, the issue of hallucination may continue to persist in the realm of LLMs for the foreseeable future. Consequently, in order to gain the public’s trust and border applications, it is impera- tive to detect and assess instances of hallucination within LLMs and other forms of AI-generated content (AIGC). Existing methods strive to detect hallucination by training a neural classifier on a small set of documents [249], which are neither robust nor powerful to handle ever-growing LLMs. Recently, researchers try to use KGs as an external source to validate LLMs [250]. Further studies combine LLMs and KGs to achieve a generalized fact-checking model that can detect hallucinations across domains [251]. Therefore,it opens a new door to utilizing KGs for hallucination detection. 7.2 KGs for Editing Knowledge in LLMs Although LLMs are capable of storing massive real-world knowledge, they cannot quickly update their internal knowledge updated as real-world situations change. There are some research efforts proposed for editing knowledge in LLMs [252] without re-training the whole LLMs. Yet, such solutions still suffer from poor performance or computa- tional overhead [253]. Existing studies [254] also reveal that edit a single fact would cause a ripple effect for other related knowledge. Therefore, it is necessary to develop a more efficient and effective method to edit knowledge in LLMs. Recently, researchers try to leverage KGs to edit knowledge in LLMs efficiently. 7.3 KGs for Black-box LLMs Knowledge Injection Although pre-training and knowledge editing could update LLMs to catch up with the latest knowledge, they still need to access the internal structures and parameters of LLMs. However, many state-of-the-art large LLMs (e.g., ChatGPT) only provide APIs for users and developers to access, mak- ing themselves black-box to the public. Consequently, it is impossible to follow conventional KG',\n",
              "  'ing themselves black-box to the public. Consequently, it is impossible to follow conventional KG injection approaches described [38], [244] that change LLM structure by adding additional knowledge fusion modules. Converting various types of knowledge into different text prompts seems to be a feasible solution. However, it is unclear whether these prompts can generalize well to new LLMs. Moreover, the prompt-based approach is limited to the length of input to- kens of LLMs. Therefore, how to enable effective knowledge injection for black-box LLMs is still an open question for us to explore [255], [256]. 7.4 Multi-Modal LLMs for KGs Current knowledge graphs typically rely on textual and graph structure to handle KG-related applications. How- ever, real-world knowledge graphs are often constructed by data from diverse modalities [99], [257], [258]. Therefore, effectively leveraging representations from multiple modal- ities would be a significant challenge for future research in KGs [259]. One potential solution is to develop methods that can accurately encode and align entities across different modalities. Recently, with the development of multi-modal LLMs [98], [260], leveraging LLMs for modality alignment holds promise in this regard. But, bridging the gap between multi-modal LLMs and KG structure remains a crucial challenge in this field, demanding further investigation and advancements. 7.5 LLMs for Understanding KG Structure Conventional LLMs trained on plain text data are not designed to understand structured data like knowledge graphs. Thus, LLMs might not fully grasp or understand the information conveyed by the KG structure. A straightfor- ward way is to linearize the structured data into a sentence that LLMs can understand. However, the scale of the KGs',\n",
              "  'JOURNAL OF LATEX CLASS FILES, VOL. ??, NO. ??, MONTH 20YY 21 KG-enhanced LLMs LLM-augmented KGs Synergized LLMs + KGsGraph Structure Understanding Multi-modality Knowledge UpdatingStage 1 Stage 2 Stage 3 Fig. 26. The milestones of unifying KGs and LLMs. makes it impossible to linearize the whole KGs as input. Moreover, the linearization process may lose some underly- ing information in KGs. Therefore, it is necessary to develop LLMs that can directly understand the KG structure and reason over it [237]. 7.6 Synergized LLMs and KGs for Birectional Reason- ing KGs and LLMs are two complementary technologies that can synergize each other. However, the synergy of LLMs and KGs is less explored by existing researchers. A desired synergy of LLMs and KGs would involve leveraging the strengths of both technologies to overcome their individual limitations. LLMs, such as ChatGPT, excel in generating human-like text and understanding natural language, while KGs are structured databases that capture and represent knowledge in a structured manner. By combining their capa- bilities, we can create a powerful system that benefits from the contextual understanding of LLMs and the structured knowledge representation of KGs. To better unify LLMs and KGs, many advanced techniques need to be incorporated, such as multi-modal learning [261], graph neural network [262], and continuous learning [263]. Last, the synergy of LLMs and KGs can be applied to many real-world applica- tions, such as search engines [100], recommender systems [10], [89], and drug discovery. With a given application problem, we can apply a KG to perform a knowledge-driven search for potential goals and unseen data, and simultaneously start with LLMs to perform a data/text-driven inference to see what new data/goal items can be derived. When the knowledge-based search is combined with data/text-driven inference, they can mutually validate each other, resulting in efficient and effective solutions powered by',\n",
              "  'they can mutually validate each other, resulting in efficient and effective solutions powered by dual-driving wheels. There- fore, we can anticipate increasing attention to unlock the po- tential of integrating KGs and LLMs for diverse downstream applications with both generative and reasoning capabilities in the near future. 8 C ONCLUSION Unifying large language models (LLMs) and knowledge graphs (KGs) is an active research direction that has at- tracted increasing attention from both academia and in- dustry. In this article, we provide a thorough overview of the recent research in this field. We first introduce different manners that integrate KGs to enhance LLMs. Then, we introduce existing methods that apply LLMs for KGs and establish taxonomy based on varieties of KG tasks. Finally, we discuss the challenges and future directions in this field.We envision that there will be multiple stages (milestones) in the roadmap of unifying KGs and LLMs, as shown in Fig. 26. In particular, we will anticipate increasing research on three stages: Stage 1 : KG-enhanced LLMs, LLM-augmented KGs, Stage 2: Synergized LLMs + KGs, and Stage 3: Graph Structure Understanding, Multi-modality, Knowledge Up- dating. We hope that this article will provide a guideline to advance future research. ACKNOWLEDGMENTS This research was supported by the Australian Research Council (ARC) under grants FT210100097 and DP240101547 and the National Natural Science Foundation of China (NSFC) under grant 62120106008. REFERENCES [1] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre- training of deep bidirectional transformers for language under- standing,” arXiv preprint arXiv:1810.04805 , 2018. [2] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V . Stoyanov, “Roberta: A ro- bustly optimized bert pretraining approach,” arXiv preprint arXiv:1907.11692 , 2019. [3] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P .',\n",
              "  '2019. [3] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P . J. Liu, “Exploring the limits of transfer learning with a unified text-to-text transformer,” The Journal of Machine Learning Research , vol. 21, no. 1, pp. 5485–5551, 2020. [4] D. Su, Y. Xu, G. I. Winata, P . Xu, H. Kim, Z. Liu, and P . Fung, “Generalizing question answering system with pre-trained lan- guage model fine-tuning,” in Proceedings of the 2nd Workshop on Machine Reading for Question Answering , 2019, pp. 203–211. [5] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy, V . Stoyanov, and L. Zettlemoyer, “Bart: Denoising sequence-to-sequence pre-training for natural language genera- tion, translation, and comprehension,” in ACL , 2020, pp. 7871– 7880. [6] J. Li, T. Tang, W. X. Zhao, and J.-R. Wen, “Pretrained lan- guage models for text generation: A survey,” arXiv preprint arXiv:2105.10311 , 2021. [7] J. Wei, Y. Tay, R. Bommasani, C. Raffel, B. Zoph, S. Borgeaud, D. Yogatama, M. Bosma, D. Zhou, D. Metzler et al. , “Emergent abilities of large language models,” Transactions on Machine Learn- ing Research . [8] K. Malinka, M. Pere ˇs´ıni, A. Firc, O. Huj ˇn´ak, and F. Janu ˇs, “On the educational impact of chatgpt: Is artificial intelligence ready to obtain a university degree?” arXiv preprint arXiv:2303.11146 , 2023. [9] Z. Li, C. Wang, Z. Liu, H. Wang, S. Wang, and C. Gao, “Cctest: Testing and repairing code completion systems,” ICSE , 2023. [10] J. Liu, C. Liu, R. Lv, K. Zhou, and Y. Zhang, “Is chatgpt a good rec- ommender? a preliminary study,” arXiv preprint arXiv:2304.10149 , 2023. [11] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min, B. Zhang, J. Zhang, Z. Dong et al. , “A survey of large language models,” arXiv preprint arXiv:2303.18223 , 2023. [12] X. Qiu, T. Sun, Y. Xu, Y. Shao, N. Dai, and X. Huang, “Pre-trained models for natural language processing: A survey,” Science China Technological Sciences , vol. 63, no. 10, pp.',\n",
              "  'natural language processing: A survey,” Science China Technological Sciences , vol. 63, no. 10, pp. 1872–1897, 2020. [13] J. Yang, H. Jin, R. Tang, X. Han, Q. Feng, H. Jiang, B. Yin, and X. Hu, “Harnessing the power of llms in practice: A survey on chatgpt and beyond,” arXiv preprint arXiv:2304.13712 , 2023. [14] F. Petroni, T. Rockt ¨aschel, S. Riedel, P . Lewis, A. Bakhtin, Y. Wu, and A. Miller, “Language models as knowledge bases?” in EMNLP-IJCNLP , 2019, pp. 2463–2473. [15] Z. Ji, N. Lee, R. Frieske, T. Yu, D. Su, Y. Xu, E. Ishii, Y. J. Bang, A. Madotto, and P . Fung, “Survey of hallucination in natural language generation,” ACM Computing Surveys , vol. 55, no. 12, pp. 1–38, 2023. [16] H. Zhang, H. Song, S. Li, M. Zhou, and D. Song, “A survey of controllable text generation using transformer-based pre-trained language models,” arXiv preprint arXiv:2201.05337 , 2022.',\n",
              "  'JOURNAL OF LATEX CLASS FILES, VOL. ??, NO. ??, MONTH 20YY 22 [17] M. Danilevsky, K. Qian, R. Aharonov, Y. Katsis, B. Kawas, and P . Sen, “A survey of the state of explainable ai for natural language processing,” arXiv preprint arXiv:2010.00711 , 2020. [18] J. Wang, X. Hu, W. Hou, H. Chen, R. Zheng, Y. Wang, L. Yang, H. Huang, W. Ye, X. Geng et al. , “On the robustness of chatgpt: An adversarial and out-of-distribution perspective,” arXiv preprint arXiv:2302.12095 , 2023. [19] S. Ji, S. Pan, E. Cambria, P . Marttinen, and S. Y. Philip, “A survey on knowledge graphs: Representation, acquisition, and applications,” IEEE TNNLS , vol. 33, no. 2, pp. 494–514, 2021. [20] D. Vrande ˇci´c and M. Kr ¨otzsch, “Wikidata: a free collaborative knowledgebase,” Communications of the ACM , vol. 57, no. 10, pp. 78–85, 2014. [21] S. Hu, L. Zou, and X. Zhang, “A state-transition framework to answer complex questions over knowledge base,” in EMNLP , 2018, pp. 2098–2108. [22] J. Zhang, B. Chen, L. Zhang, X. Ke, and H. Ding, “Neural, symbolic and neural-symbolic reasoning on knowledge graphs,” AI Open , vol. 2, pp. 14–35, 2021. [23] B. Abu-Salih, “Domain-specific knowledge graphs: A survey,” Journal of Network and Computer Applications , vol. 185, p. 103076, 2021. [24] T. Mitchell, W. Cohen, E. Hruschka, P . Talukdar, B. Yang, J. Bet- teridge, A. Carlson, B. Dalvi, M. Gardner, B. Kisiel, K. Jayant, L. Ni, M. Kathryn, M. Thahir, N. Ndapandula, P . Emmanouil, R. Alan, S. Mehdi, S. Burr, W. Derry, G. Abhinav, C. Xi, S. Abul- hair, and W. Joel, “Never-ending learning,” Communications of the ACM , vol. 61, no. 5, pp. 103–115, 2018. [25] L. Zhong, J. Wu, Q. Li, H. Peng, and X. Wu, “A comprehen- sive survey on automatic knowledge graph construction,” arXiv preprint arXiv:2302.05019 , 2023. [26] L. Yao, C. Mao, and Y. Luo, “Kg-bert: Bert for knowledge graph completion,” arXiv preprint arXiv:1909.03193 , 2019. [27] L. Luo, Y.-F. Li, G. Haffari, and S. Pan, “Normalizing flow- based neural process',\n",
              "  ', 2019. [27] L. Luo, Y.-F. Li, G. Haffari, and S. Pan, “Normalizing flow- based neural process for few-shot knowledge graph completion,” SIGIR , 2023. [28] Y. Bang, S. Cahyawijaya, N. Lee, W. Dai, D. Su, B. Wilie, H. Love- nia, Z. Ji, T. Yu, W. Chung et al. , “A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity,” arXiv preprint arXiv:2302.04023 , 2023. [29] X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, and D. Zhou, “Self- consistency improves chain of thought reasoning in language models,” arXiv preprint arXiv:2203.11171 , 2022. [30] O. Golovneva, M. Chen, S. Poff, M. Corredor, L. Zettlemoyer, M. Fazel-Zarandi, and A. Celikyilmaz, “Roscoe: A suite of metrics for scoring step-by-step reasoning,” ICLR , 2023. [31] F. M. Suchanek, G. Kasneci, and G. Weikum, “Yago: a core of semantic knowledge,” in WWW , 2007, pp. 697–706. [32] A. Carlson, J. Betteridge, B. Kisiel, B. Settles, E. Hruschka, and T. Mitchell, “Toward an architecture for never-ending language learning,” in Proceedings of the AAAI conference on artificial intelli- gence , vol. 24, no. 1, 2010, pp. 1306–1313. [33] A. Bordes, N. Usunier, A. Garcia-Duran, J. Weston, and O. Yakhnenko, “Translating embeddings for modeling multi- relational data,” NeurIPS , vol. 26, 2013. [34] G. Wan, S. Pan, C. Gong, C. Zhou, and G. Haffari, “Reasoning like human: Hierarchical reinforcement learning for knowledge graph reasoning,” in AAAI , 2021, pp. 1926–1932. [35] Z. Zhang, X. Han, Z. Liu, X. Jiang, M. Sun, and Q. Liu, “ERNIE: Enhanced language representation with informative entities,” in ACL , 2019, pp. 1441–1451. [36] W. Liu, P . Zhou, Z. Zhao, Z. Wang, Q. Ju, H. Deng, and P . Wang, “K-BERT: enabling language representation with knowledge graph,” in AAAI , 2020, pp. 2901–2908. [37] Y. Liu, Y. Wan, L. He, H. Peng, and P . S. Yu, “KG-BART: knowl- edge graph-augmented BART for generative commonsense rea- soning,” in AAAI , 2021, pp. 6418–6425. [38] B. Y. Lin, X. Chen, J.',\n",
              "  'for generative commonsense rea- soning,” in AAAI , 2021, pp. 6418–6425. [38] B. Y. Lin, X. Chen, J. Chen, and X. Ren, “KagNet: Knowledge- aware graph networks for commonsense reasoning,” in EMNLP- IJCNLP , 2019, pp. 2829–2839. [39] D. Dai, L. Dong, Y. Hao, Z. Sui, B. Chang, and F. Wei, “Knowledge neurons in pretrained transformers,” arXiv preprint arXiv:2104.08696 , 2021. [40] X. Wang, T. Gao, Z. Zhu, Z. Zhang, Z. Liu, J. Li, and J. Tang, “KEPLER: A unified model for knowledge embedding and pre- trained language representation,” Transactions of the Association for Computational Linguistics , vol. 9, pp. 176–194, 2021.[41] I. Melnyk, P . Dognin, and P . Das, “Grapher: Multi-stage knowl- edge graph construction using pretrained language models,” in NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications , 2021. [42] P . Ke, H. Ji, Y. Ran, X. Cui, L. Wang, L. Song, X. Zhu, and M. Huang, “JointGT: Graph-text joint representation learning for text generation from knowledge graphs,” in ACL Finding , 2021, pp. 2526–2538. [43] J. Jiang, K. Zhou, W. X. Zhao, and J.-R. Wen, “Unikgqa: Unified retrieval and reasoning for solving multi-hop question answering over knowledge graph,” ICLR 2023 , 2023. [44] M. Yasunaga, A. Bosselut, H. Ren, X. Zhang, C. D. Manning, P . S. Liang, and J. Leskovec, “Deep bidirectional language-knowledge graph pretraining,” NeurIPS , vol. 35, pp. 37 309–37 323, 2022. [45] N. Choudhary and C. K. Reddy, “Complex logical reasoning over knowledge graphs using large language models,” arXiv preprint arXiv:2305.01157 , 2023. [46] S. Wang, Z. Wei, J. Xu, and Z. Fan, “Unifying structure reasoning and language model pre-training for complex reasoning,” arXiv preprint arXiv:2301.08913 , 2023. [47] C. Zhen, Y. Shang, X. Liu, Y. Li, Y. Chen, and D. Zhang, “A survey on knowledge-enhanced pre-trained language models,” arXiv preprint arXiv:2212.13428 , 2022. [48] X. Wei, S. Wang, D. Zhang, P . Bhatia, and A. Arnold, “Knowl- edge enhanced pretrained',\n",
              "  ', 2022. [48] X. Wei, S. Wang, D. Zhang, P . Bhatia, and A. Arnold, “Knowl- edge enhanced pretrained language models: A compreshensive survey,” arXiv preprint arXiv:2110.08455 , 2021. [49] D. Yin, L. Dong, H. Cheng, X. Liu, K.-W. Chang, F. Wei, and J. Gao, “A survey of knowledge-intensive nlp with pre-trained language models,” arXiv preprint arXiv:2202.08772 , 2022. [50] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” NeurIPS , vol. 30, 2017. [51] Z. Lan, M. Chen, S. Goodman, K. Gimpel, P . Sharma, and R. Sori- cut, “Albert: A lite bert for self-supervised learning of language representations,” in ICLR , 2019. [52] K. Clark, M.-T. Luong, Q. V . Le, and C. D. Manning, “Electra: Pre- training text encoders as discriminators rather than generators,” arXiv preprint arXiv:2003.10555 , 2020. [53] K. Hakala and S. Pyysalo, “Biomedical named entity recognition with multilingual bert,” in Proceedings of the 5th workshop on BioNLP open shared tasks , 2019, pp. 56–61. [54] Y. Tay, M. Dehghani, V . Q. Tran, X. Garcia, J. Wei, X. Wang, H. W. Chung, D. Bahri, T. Schuster, S. Zheng et al. , “Ul2: Unifying language learning paradigms,” in ICLR , 2022. [55] V . Sanh, A. Webson, C. Raffel, S. Bach, L. Sutawika, Z. Alyafeai, A. Chaffin, A. Stiegler, A. Raja, M. Dey et al. , “Multitask prompted training enables zero-shot task generalization,” in ICLR , 2022. [56] B. Zoph, I. Bello, S. Kumar, N. Du, Y. Huang, J. Dean, N. Shazeer, and W. Fedus, “St-moe: Designing stable and transferable sparse expert models,” URL https://arxiv. org/abs/2202.08906 , 2022. [57] A. Zeng, X. Liu, Z. Du, Z. Wang, H. Lai, M. Ding, Z. Yang, Y. Xu, W. Zheng, X. Xia, W. L. Tam, Z. Ma, Y. Xue, J. Zhai, W. Chen, Z. Liu, P . Zhang, Y. Dong, and J. Tang, “GLM-130b: An open bilingual pre-trained model,” in ICLR , 2023. [58] L. Xue, N. Constant, A. Roberts, M. Kale, R. Al-Rfou, A. Siddhant, A. Barua, and C. Raffel, “mt5: A massively',\n",
              "  'Constant, A. Roberts, M. Kale, R. Al-Rfou, A. Siddhant, A. Barua, and C. Raffel, “mt5: A massively multilingual pre- trained text-to-text transformer,” in NAACL , 2021, pp. 483–498. [59] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P . Dhari- wal, A. Neelakantan, P . Shyam, G. Sastry, A. Askell et al. , “Language models are few-shot learners,” Advances in neural information processing systems , vol. 33, pp. 1877–1901, 2020. [60] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P . Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray et al. , “Training language models to follow instructions with human feedback,” NeurIPS , vol. 35, pp. 27 730–27 744, 2022. [61] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi `ere, N. Goyal, E. Hambro, F. Azhar et al. , “Llama: Open and efficient foundation language models,” arXiv preprint arXiv:2302.13971 , 2023. [62] E. Saravia, “Prompt Engineering Guide,” https://github.com/ dair-ai/Prompt-Engineering-Guide, 2022, accessed: 2022-12. [63] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. H. Chi, Q. V . Le, D. Zhou et al. , “Chain-of-thought prompting elicits reasoning in large language models,” in NeurIPS . [64] S. Li, Y. Gao, H. Jiang, Q. Yin, Z. Li, X. Yan, C. Zhang, and B. Yin, “Graph reasoning for question answering with triplet retrieval,” inACL , 2023.',\n",
              "  'JOURNAL OF LATEX CLASS FILES, VOL. ??, NO. ??, MONTH 20YY 23 [65] Y. Wen, Z. Wang, and J. Sun, “Mindmap: Knowledge graph prompting sparks graph of thoughts in large language models,” arXiv preprint arXiv:2308.09729 , 2023. [66] K. Bollacker, C. Evans, P . Paritosh, T. Sturge, and J. Taylor, “Free- base: A collaboratively created graph database for structuring human knowledge,” in SIGMOD , 2008, pp. 1247–1250. [67] S. Auer, C. Bizer, G. Kobilarov, J. Lehmann, R. Cyganiak, and Z. Ives, “Dbpedia: A nucleus for a web of open data,” in The Semantic Web: 6th International Semantic Web Conference . Springer, 2007, pp. 722–735. [68] B. Xu, Y. Xu, J. Liang, C. Xie, B. Liang, W. Cui, and Y. Xiao, “Cn- dbpedia: A never-ending chinese knowledge extraction system,” in30th International Conference on Industrial Engineering and Other Applications of Applied Intelligent Systems . Springer, 2017, pp. 428–438. [69] P . Hai-Nyzhnyk, “Vikidia as a universal multilingual online encyclopedia for children,” The Encyclopedia Herald of Ukraine , vol. 14, 2022. [70] F. Ilievski, P . Szekely, and B. Zhang, “Cskg: The commonsense knowledge graph,” Extended Semantic Web Conference (ESWC) , 2021. [71] R. Speer, J. Chin, and C. Havasi, “Conceptnet 5.5: An open multilingual graph of general knowledge,” in Proceedings of the AAAI conference on artificial intelligence , vol. 31, no. 1, 2017. [72] H. Ji, P . Ke, S. Huang, F. Wei, X. Zhu, and M. Huang, “Language generation with multi-hop reasoning on commonsense knowl- edge graph,” in EMNLP , 2020, pp. 725–736. [73] J. D. Hwang, C. Bhagavatula, R. Le Bras, J. Da, K. Sakaguchi, A. Bosselut, and Y. Choi, “(comet-) atomic 2020: On symbolic and neural commonsense knowledge graphs,” in AAAI , vol. 35, no. 7, 2021, pp. 6384–6392. [74] H. Zhang, X. Liu, H. Pan, Y. Song, and C. W.-K. Leung, “Aser: A large-scale eventuality knowledge graph,” in Proceedings of the web conference 2020 , 2020, pp. 201–211. [75] H. Zhang, D. Khashabi, Y. Song, and D. Roth,',\n",
              "  'of the web conference 2020 , 2020, pp. 201–211. [75] H. Zhang, D. Khashabi, Y. Song, and D. Roth, “Transomcs: from linguistic graphs to commonsense knowledge,” in IJCAI , 2021, pp. 4004–4010. [76] Z. Li, X. Ding, T. Liu, J. E. Hu, and B. Van Durme, “Guided generation of cause and effect,” in IJCAI , 2020. [77] O. Bodenreider, “The unified medical language system (umls): in- tegrating biomedical terminology,” Nucleic acids research , vol. 32, no. suppl 1, pp. D267–D270, 2004. [78] Y. Liu, Q. Zeng, J. Ordieres Mer ´e, and H. Yang, “Anticipating stock market of the renowned companies: a knowledge graph approach,” Complexity , vol. 2019, 2019. [79] Y. Zhu, W. Zhou, Y. Xu, J. Liu, Y. Tan et al. , “Intelligent learning for knowledge graph towards geological data,” Scientific Program- ming , vol. 2017, 2017. [80] W. Choi and H. Lee, “Inference of biomedical relations among chemicals, genes, diseases, and symptoms using knowledge rep- resentation learning,” IEEE Access , vol. 7, pp. 179 373–179 384, 2019. [81] F. Farazi, M. Salamanca, S. Mosbach, J. Akroyd, A. Eibeck, L. K. Aditya, A. Chadzynski, K. Pan, X. Zhou, S. Zhang et al. , “Knowledge graph approach to combustion chemistry and inter- operability,” ACS omega , vol. 5, no. 29, pp. 18 342–18 348, 2020. [82] X. Wu, T. Jiang, Y. Zhu, and C. Bu, “Knowledge graph for china’s genealogy,” IEEE TKDE , vol. 35, no. 1, pp. 634–646, 2023. [83] X. Zhu, Z. Li, X. Wang, X. Jiang, P . Sun, X. Wang, Y. Xiao, and N. J. Yuan, “Multi-modal knowledge graph construction and application: A survey,” IEEE TKDE , 2022. [84] S. Ferrada, B. Bustos, and A. Hogan, “Imgpedia: a linked dataset with content-based analysis of wikimedia images,” in The Seman- tic Web–ISWC 2017 . Springer, 2017, pp. 84–93. [85] Y. Liu, H. Li, A. Garcia-Duran, M. Niepert, D. Onoro-Rubio, and D. S. Rosenblum, “Mmkg: multi-modal knowledge graphs,” inThe Semantic Web: 16th International Conference, ESWC 2019, Portoroˇ z, Slovenia, June 2–6, 2019, Proceedings 16 . Springer,',\n",
              "  'Conference, ESWC 2019, Portoroˇ z, Slovenia, June 2–6, 2019, Proceedings 16 . Springer, 2019, pp. 459–474. [86] M. Wang, H. Wang, G. Qi, and Q. Zheng, “Richpedia: a large- scale, comprehensive multi-modal knowledge graph,” Big Data Research , vol. 22, p. 100159, 2020. [87] B. Shi, L. Ji, P . Lu, Z. Niu, and N. Duan, “Knowledge aware semantic concept expansion for image-text matching.” in IJCAI , vol. 1, 2019, p. 2. [88] S. Shah, A. Mishra, N. Yadati, and P . P . Talukdar, “Kvqa: Knowledge-aware visual question answering,” in AAAI , vol. 33, no. 01, 2019, pp. 8876–8884.[89] R. Sun, X. Cao, Y. Zhao, J. Wan, K. Zhou, F. Zhang, Z. Wang, and K. Zheng, “Multi-modal knowledge graphs for recommender systems,” in CIKM , 2020, pp. 1405–1414. [90] S. Deng, C. Wang, Z. Li, N. Zhang, Z. Dai, H. Chen, F. Xiong, M. Yan, Q. Chen, M. Chen, J. Chen, J. Z. Pan, B. Hooi, and H. Chen, “Construction and applications of billion-scale pre- trained multimodal business knowledge graph,” in ICDE , 2023. [91] C. Rosset, C. Xiong, M. Phan, X. Song, P . Bennett, and S. Tiwary, “Knowledge-aware language model pretraining,” arXiv preprint arXiv:2007.00655 , 2020. [92] P . Lewis, E. Perez, A. Piktus, F. Petroni, V . Karpukhin, N. Goyal, H. K ¨uttler, M. Lewis, W.-t. Yih, T. Rockt ¨aschel, S. Riedel, and D. Kiela, “Retrieval-augmented generation for knowledge- intensive nlp tasks,” in NeurIPS , vol. 33, 2020, pp. 9459–9474. [93] Y. Zhu, X. Wang, J. Chen, S. Qiao, Y. Ou, Y. Yao, S. Deng, H. Chen, and N. Zhang, “Llms for knowledge graph construction and reasoning: Recent capabilities and future opportunities,” arXiv preprint arXiv:2305.13168 , 2023. [94] Z. Zhang, X. Liu, Y. Zhang, Q. Su, X. Sun, and B. He, “Pretrain- kge: learning knowledge representation from pretrained lan- guage models,” in EMNLP Finding , 2020, pp. 259–266. [95] A. Kumar, A. Pandey, R. Gadia, and M. Mishra, “Building knowledge graph using pre-trained language model for learning entity-aware relationships,” in 2020 IEEE',\n",
              "  'graph using pre-trained language model for learning entity-aware relationships,” in 2020 IEEE International Conference on Computing, Power and Communication Technologies (GUCON) . IEEE, 2020, pp. 310–315. [96] X. Xie, N. Zhang, Z. Li, S. Deng, H. Chen, F. Xiong, M. Chen, and H. Chen, “From discrimination to generation: Knowledge graph completion with generative transformer,” in WWW , 2022, pp. 162–165. [97] Z. Chen, C. Xu, F. Su, Z. Huang, and Y. Dou, “Incorporating structured sentences with time-enhanced bert for fully-inductive temporal relation prediction,” SIGIR , 2023. [98] D. Zhu, J. Chen, X. Shen, X. Li, and M. Elhoseiny, “Minigpt-4: Enhancing vision-language understanding with advanced large language models,” arXiv preprint arXiv:2304.10592 , 2023. [99] M. Warren, D. A. Shamma, and P . J. Hayes, “Knowledge engi- neering with image data in real-world settings,” in AAAI , ser. CEUR Workshop Proceedings, vol. 2846, 2021. [100] R. Thoppilan, D. De Freitas, J. Hall, N. Shazeer, A. Kul- shreshtha, H.-T. Cheng, A. Jin, T. Bos, L. Baker, Y. Du et al. , “Lamda: Language models for dialog applications,” arXiv preprint arXiv:2201.08239 , 2022. [101] Y. Sun, S. Wang, S. Feng, S. Ding, C. Pang, J. Shang, J. Liu, X. Chen, Y. Zhao, Y. Lu et al. , “Ernie 3.0: Large-scale knowledge enhanced pre-training for language understanding and genera- tion,” arXiv preprint arXiv:2107.02137 , 2021. [102] T. Shen, Y. Mao, P . He, G. Long, A. Trischler, and W. Chen, “Exploiting structured knowledge in text via graph-guided rep- resentation learning,” in EMNLP , 2020, pp. 8980–8994. [103] D. Zhang, Z. Yuan, Y. Liu, F. Zhuang, H. Chen, and H. Xiong, “E-bert: A phrase and product knowledge enhanced language model for e-commerce,” arXiv preprint arXiv:2009.02835 , 2020. [104] S. Li, X. Li, L. Shang, C. Sun, B. Liu, Z. Ji, X. Jiang, and Q. Liu, “Pre-training language models with deterministic factual knowl- edge,” in EMNLP , 2022, pp. 11 118–11 131. [105] M. Kang, J. Baek, and S. J. Hwang,',\n",
              "  'factual knowl- edge,” in EMNLP , 2022, pp. 11 118–11 131. [105] M. Kang, J. Baek, and S. J. Hwang, “Kala: Knowledge-augmented language model adaptation,” in NAACL , 2022, pp. 5144–5167. [106] W. Xiong, J. Du, W. Y. Wang, and V . Stoyanov, “Pretrained en- cyclopedia: Weakly supervised knowledge-pretrained language model,” in ICLR , 2020. [107] T. Sun, Y. Shao, X. Qiu, Q. Guo, Y. Hu, X. Huang, and Z. Zhang, “CoLAKE: Contextualized language and knowledge embed- ding,” in Proceedings of the 28th International Conference on Com- putational Linguistics , 2020, pp. 3660–3670. [108] T. Zhang, C. Wang, N. Hu, M. Qiu, C. Tang, X. He, and J. Huang, “DKPLM: decomposable knowledge-enhanced pre-trained lan- guage model for natural language understanding,” in AAAI , 2022, pp. 11 703–11 711. [109] J. Wang, W. Huang, M. Qiu, Q. Shi, H. Wang, X. Li, and M. Gao, “Knowledge prompting in pre-trained language model for natu- ral language understanding,” in Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing , 2022, pp. 3164–3177. [110] H. Ye, N. Zhang, S. Deng, X. Chen, H. Chen, F. Xiong, X. Chen, and H. Chen, “Ontology-enhanced prompt-tuning for few-shot learning,” in Proceedings of the ACM Web Conference 2022 , 2022, pp. 778–787.',\n",
              "  'JOURNAL OF LATEX CLASS FILES, VOL. ??, NO. ??, MONTH 20YY 24 [111] H. Luo, Z. Tang, S. Peng, Y. Guo, W. Zhang, C. Ma, G. Dong, M. Song, W. Lin et al. , “Chatkbqa: A generate-then-retrieve frame- work for knowledge base question answering with fine-tuned large language models,” arXiv preprint arXiv:2310.08975 , 2023. [112] L. Luo, Y.-F. Li, G. Haffari, and S. Pan, “Reasoning on graphs: Faithful and interpretable large language model reasoning,” arXiv preprint arxiv:2310.01061 , 2023. [113] R. Logan, N. F. Liu, M. E. Peters, M. Gardner, and S. Singh, “Barack’s wife hillary: Using knowledge graphs for fact-aware language modeling,” in ACL , 2019, pp. 5962–5971. [114] K. Guu, K. Lee, Z. Tung, P . Pasupat, and M.-W. Chang, “Realm: Retrieval-augmented language model pre-training,” in ICML , 2020. [115] Y. Wu, Y. Zhao, B. Hu, P . Minervini, P . Stenetorp, and S. Riedel, “An efficient memory-augmented transformer for knowledge- intensive NLP tasks,” in EMNLP , 2022, pp. 5184–5196. [116] L. Luo, J. Ju, B. Xiong, Y.-F. Li, G. Haffari, and S. Pan, “Chatrule: Mining logical rules with large language models for knowledge graph reasoning,” arXiv preprint arXiv:2309.01538 , 2023. [117] J. Wang, Q. Sun, N. Chen, X. Li, and M. Gao, “Boosting language models reasoning with chain-of-knowledge prompting,” arXiv preprint arXiv:2306.06427 , 2023. [118] Z. Jiang, F. F. Xu, J. Araki, and G. Neubig, “How can we know what language models know?” Transactions of the Association for Computational Linguistics , vol. 8, pp. 423–438, 2020. [119] T. Shin, Y. Razeghi, R. L. Logan IV , E. Wallace, and S. Singh, “Au- toprompt: Eliciting knowledge from language models with au- tomatically generated prompts,” arXiv preprint arXiv:2010.15980 , 2020. [120] Z. Meng, F. Liu, E. Shareghi, Y. Su, C. Collins, and N. Collier, “Rewire-then-probe: A contrastive recipe for probing biomedi- cal knowledge of pre-trained language models,” arXiv preprint arXiv:2110.08173 , 2021. [121] L. Luo, T.-T. Vu, D. Phung, and',\n",
              "  'language models,” arXiv preprint arXiv:2110.08173 , 2021. [121] L. Luo, T.-T. Vu, D. Phung, and G. Haffari, “Systematic assess- ment of factual knowledge in large language models,” in EMNLP , 2023. [122] V . Swamy, A. Romanou, and M. Jaggi, “Interpreting language models through knowledge graph extraction,” arXiv preprint arXiv:2111.08546 , 2021. [123] S. Li, X. Li, L. Shang, Z. Dong, C. Sun, B. Liu, Z. Ji, X. Jiang, and Q. Liu, “How pre-trained language models capture fac- tual knowledge? a causal-inspired analysis,” arXiv preprint arXiv:2203.16747 , 2022. [124] H. Tian, C. Gao, X. Xiao, H. Liu, B. He, H. Wu, H. Wang, and F. Wu, “SKEP: Sentiment knowledge enhanced pre-training for sentiment analysis,” in ACL , 2020, pp. 4067–4076. [125] W. Yu, C. Zhu, Y. Fang, D. Yu, S. Wang, Y. Xu, M. Zeng, and M. Jiang, “Dict-BERT: Enhancing language model pre-training with dictionary,” in ACL , 2022, pp. 1907–1918. [126] T. McCoy, E. Pavlick, and T. Linzen, “Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference,” in ACL , 2019, pp. 3428–3448. [127] D. Wilmot and F. Keller, “Memory and knowledge augmented language models for inferring salience in long-form stories,” in EMNLP , 2021, pp. 851–865. [128] L. Adolphs, S. Dhuliawala, and T. Hofmann, “How to query language models?” arXiv preprint arXiv:2108.01928 , 2021. [129] M. Sung, J. Lee, S. Yi, M. Jeon, S. Kim, and J. Kang, “Can language models be biomedical knowledge bases?” in EMNLP , 2021, pp. 4723–4734. [130] A. Mallen, A. Asai, V . Zhong, R. Das, H. Hajishirzi, and D. Khashabi, “When not to trust language models: Investigating effectiveness and limitations of parametric and non-parametric memories,” arXiv preprint arXiv:2212.10511 , 2022. [131] M. Yasunaga, H. Ren, A. Bosselut, P . Liang, and J. Leskovec, “QA- GNN: Reasoning with language models and knowledge graphs for question answering,” in NAACL , 2021, pp. 535–546. [132] M. Nayyeri, Z. Wang, M. Akter, M. M. Alam, M. R. A. H. Rony, J.',\n",
              "  'in NAACL , 2021, pp. 535–546. [132] M. Nayyeri, Z. Wang, M. Akter, M. M. Alam, M. R. A. H. Rony, J. Lehmann, S. Staab et al. , “Integrating knowledge graph embedding and pretrained language models in hypercomplex spaces,” arXiv preprint arXiv:2208.02743 , 2022. [133] N. Huang, Y. R. Deshpande, Y. Liu, H. Alberts, K. Cho, C. Vania, and I. Calixto, “Endowing language models with multimodal knowledge graph representations,” arXiv preprint arXiv:2206.13163 , 2022. [134] M. M. Alam, M. R. A. H. Rony, M. Nayyeri, K. Mohiuddin, M. M. Akter, S. Vahdati, and J. Lehmann, “Language model guided knowledge graph embeddings,” IEEE Access , vol. 10, pp. 76 008– 76 020, 2022.[135] X. Wang, Q. He, J. Liang, and Y. Xiao, “Language models as knowledge embeddings,” arXiv preprint arXiv:2206.12617 , 2022. [136] N. Zhang, X. Xie, X. Chen, S. Deng, C. Tan, F. Huang, X. Cheng, and H. Chen, “Reasoning through memorization: Nearest neighbor knowledge graph embeddings,” arXiv preprint arXiv:2201.05575 , 2022. [137] X. Xie, Z. Li, X. Wang, Y. Zhu, N. Zhang, J. Zhang, S. Cheng, B. Tian, S. Deng, F. Xiong, and H. Chen, “Lambdakg: A library for pre-trained language model-based knowledge graph embed- dings,” 2022. [138] B. Kim, T. Hong, Y. Ko, and J. Seo, “Multi-task learning for knowl- edge graph completion with pre-trained language models,” in COLING , 2020, pp. 1737–1743. [139] X. Lv, Y. Lin, Y. Cao, L. Hou, J. Li, Z. Liu, P . Li, and J. Zhou, “Do pre-trained models benefit knowledge graph completion? A reliable evaluation and a reasonable approach,” in ACL , 2022, pp. 3570–3581. [140] J. Shen, C. Wang, L. Gong, and D. Song, “Joint language semantic and structure embedding for knowledge graph completion,” in COLING , 2022, pp. 1965–1978. [141] B. Choi, D. Jang, and Y. Ko, “MEM-KGC: masked entity model for knowledge graph completion with pre-trained language model,” IEEE Access , vol. 9, pp. 132 025–132 032, 2021. [142] B. Choi and Y. Ko, “Knowledge graph extension with a pre- trained language',\n",
              "  '025–132 032, 2021. [142] B. Choi and Y. Ko, “Knowledge graph extension with a pre- trained language model via unified learning method,” Knowl. Based Syst. , vol. 262, p. 110245, 2023. [143] B. Wang, T. Shen, G. Long, T. Zhou, Y. Wang, and Y. Chang, “Structure-augmented text representation learning for efficient knowledge graph completion,” in WWW , 2021, pp. 1737–1748. [144] L. Wang, W. Zhao, Z. Wei, and J. Liu, “Simkgc: Simple contrastive knowledge graph completion with pre-trained language mod- els,” in ACL , 2022, pp. 4281–4294. [145] D. Li, M. Yi, and Y. He, “Lp-bert: Multi-task pre-training knowledge graph bert for link prediction,” arXiv preprint arXiv:2201.04843 , 2022. [146] A. Saxena, A. Kochsiek, and R. Gemulla, “Sequence-to-sequence knowledge graph completion and question answering,” in ACL , 2022, pp. 2814–2828. [147] C. Chen, Y. Wang, B. Li, and K. Lam, “Knowledge is flat: A seq2seq generative framework for various knowledge graph completion,” in COLING , 2022, pp. 4005–4017. [148] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and L. Zettlemoyer, “Deep contextualized word representations,” inNAACL , 2018, pp. 2227–2237. [149] H. Yan, T. Gui, J. Dai, Q. Guo, Z. Zhang, and X. Qiu, “A unified generative framework for various NER subtasks,” in ACL , 2021, pp. 5808–5822. [150] Y. Onoe and G. Durrett, “Learning to denoise distantly-labeled data for entity typing,” in NAACL , 2019, pp. 2407–2417. [151] Y. Onoe, M. Boratko, A. McCallum, and G. Durrett, “Modeling fine-grained entity types with box embeddings,” in ACL , 2021, pp. 2051–2064. [152] B. Z. Li, S. Min, S. Iyer, Y. Mehdad, and W. Yih, “Efficient one- pass end-to-end entity linking for questions,” in EMNLP , 2020, pp. 6433–6441. [153] T. Ayoola, S. Tyagi, J. Fisher, C. Christodoulopoulos, and A. Pier- leoni, “Refined: An efficient zero-shot-capable approach to end- to-end entity linking,” in NAACL , 2022, pp. 209–220. [154] M. Joshi, O. Levy, L. Zettlemoyer, and D. S. Weld, “BERT',\n",
              "  'in NAACL , 2022, pp. 209–220. [154] M. Joshi, O. Levy, L. Zettlemoyer, and D. S. Weld, “BERT for coreference resolution: Baselines and analysis,” in EMNLP , 2019, pp. 5802–5807. [155] M. Joshi, D. Chen, Y. Liu, D. S. Weld, L. Zettlemoyer, and O. Levy, “Spanbert: Improving pre-training by representing and predicting spans,” Trans. Assoc. Comput. Linguistics , vol. 8, pp. 64–77, 2020. [156] A. Caciularu, A. Cohan, I. Beltagy, M. E. Peters, A. Cattan, and I. Dagan, “CDLM: cross-document language modeling,” in EMNLP , 2021, pp. 2648–2662. [157] A. Cattan, A. Eirew, G. Stanovsky, M. Joshi, and I. Dagan, “Cross- document coreference resolution over predicted mentions,” in ACL , 2021, pp. 5100–5107. [158] Y. Wang, Y. Shen, and H. Jin, “An end-to-end actor-critic-based neural coreference resolution system,” in IEEE International Con- ference on Acoustics, Speech and Signal Processing, ICASSP 2021, Toronto, ON, Canada, June 6-11, 2021 , 2021, pp. 7848–7852. [159] P . Shi and J. Lin, “Simple BERT models for relation extraction and semantic role labeling,” CoRR , vol. abs/1904.05255, 2019.',\n",
              "  'JOURNAL OF LATEX CLASS FILES, VOL. ??, NO. ??, MONTH 20YY 25 [160] S. Park and H. Kim, “Improving sentence-level relation extraction through curriculum learning,” CoRR , vol. abs/2107.09332, 2021. [161] Y. Ma, A. Wang, and N. Okazaki, “DREEAM: guiding attention with evidence for improving document-level relation extraction,” inEACL , 2023, pp. 1963–1975. [162] Q. Guo, Y. Sun, G. Liu, Z. Wang, Z. Ji, Y. Shen, and X. Wang, “Con- structing chinese historical literature knowledge graph based on bert,” in Web Information Systems and Applications: 18th Inter- national Conference, WISA 2021, Kaifeng, China, September 24–26, 2021, Proceedings 18 . Springer, 2021, pp. 323–334. [163] J. Han, N. Collier, W. Buntine, and E. Shareghi, “Pive: Prompt- ing with iterative verification improving graph-based generative capability of llms,” arXiv preprint arXiv:2305.12392 , 2023. [164] A. Bosselut, H. Rashkin, M. Sap, C. Malaviya, A. Celikyilmaz, and Y. Choi, “Comet: Commonsense transformers for knowledge graph construction,” in ACL , 2019. [165] S. Hao, B. Tan, K. Tang, H. Zhang, E. P . Xing, and Z. Hu, “Bertnet: Harvesting knowledge graphs from pretrained language mod- els,” arXiv preprint arXiv:2206.14268 , 2022. [166] P . West, C. Bhagavatula, J. Hessel, J. Hwang, L. Jiang, R. Le Bras, X. Lu, S. Welleck, and Y. Choi, “Symbolic knowledge distillation: from general language models to commonsense models,” in NAACL , 2022, pp. 4602–4625. [167] L. F. R. Ribeiro, M. Schmitt, H. Sch ¨utze, and I. Gurevych, “Investi- gating pretrained language models for graph-to-text generation,” inProceedings of the 3rd Workshop on Natural Language Processing for Conversational AI , 2021, pp. 211–227. [168] J. Li, T. Tang, W. X. Zhao, Z. Wei, N. J. Yuan, and J.-R. Wen, “Few-shot knowledge graph-to-text generation with pretrained language models,” in ACL , 2021, pp. 1558–1568. [169] A. Colas, M. Alvandipour, and D. Z. Wang, “GAP: A graph- aware language model framework for knowledge graph-to-text',\n",
              "  'and D. Z. Wang, “GAP: A graph- aware language model framework for knowledge graph-to-text generation,” in Proceedings of the 29th International Conference on Computational Linguistics , 2022, pp. 5755–5769. [170] Z. Jin, Q. Guo, X. Qiu, and Z. Zhang, “GenWiki: A dataset of 1.3 million content-sharing text and graphs for unsupervised graph-to-text generation,” in Proceedings of the 28th International Conference on Computational Linguistics , 2020, pp. 2398–2409. [171] W. Chen, Y. Su, X. Yan, and W. Y. Wang, “KGPT: Knowledge- grounded pre-training for data-to-text generation,” in EMNLP , 2020, pp. 8635–8648. [172] D. Lukovnikov, A. Fischer, and J. Lehmann, “Pretrained trans- formers for simple question answering over knowledge graphs,” inThe Semantic Web–ISWC 2019: 18th International Semantic Web Conference, Auckland, New Zealand, October 26–30, 2019, Proceed- ings, Part I 18 . Springer, 2019, pp. 470–486. [173] D. Luo, J. Su, and S. Yu, “A bert-based approach with relation- aware attention for knowledge base question answering,” in IJCNN . IEEE, 2020, pp. 1–8. [174] N. Hu, Y. Wu, G. Qi, D. Min, J. Chen, J. Z. Pan, and Z. Ali, “An empirical study of pre-trained language models in simple knowl- edge graph question answering,” arXiv preprint arXiv:2303.10368 , 2023. [175] Y. Xu, C. Zhu, R. Xu, Y. Liu, M. Zeng, and X. Huang, “Fusing context into knowledge graph for commonsense question an- swering,” in ACL , 2021, pp. 1201–1207. [176] M. Zhang, R. Dai, M. Dong, and T. He, “Drlk: Dynamic hierar- chical reasoning with language model and knowledge graph for question answering,” in EMNLP , 2022, pp. 5123–5133. [177] Z. Hu, Y. Xu, W. Yu, S. Wang, Z. Yang, C. Zhu, K.-W. Chang, and Y. Sun, “Empowering language models with knowledge graph reasoning for open-domain question answering,” in EMNLP , 2022, pp. 9562–9581. [178] X. Zhang, A. Bosselut, M. Yasunaga, H. Ren, P . Liang, C. D. Man- ning, and J. Leskovec, “Greaselm: Graph reasoning enhanced language models,” in ICLR , 2022.',\n",
              "  'Man- ning, and J. Leskovec, “Greaselm: Graph reasoning enhanced language models,” in ICLR , 2022. [179] X. Cao and Y. Liu, “Relmkg: reasoning with pre-trained language models and knowledge graphs for complex question answering,” Applied Intelligence , pp. 1–15, 2022. [180] X. Huang, J. Zhang, D. Li, and P . Li, “Knowledge graph embed- ding based question answering,” in WSDM , 2019, pp. 105–113. [181] H. Wang, F. Zhang, X. Xie, and M. Guo, “Dkn: Deep knowledge- aware network for news recommendation,” in WWW , 2018, pp. 1835–1844. [182] B. Yang, S. W.-t. Yih, X. He, J. Gao, and L. Deng, “Embedding entities and relations for learning and inference in knowledge bases,” in ICLR , 2015.[183] W. Xiong, M. Yu, S. Chang, X. Guo, and W. Y. Wang, “One-shot relational learning for knowledge graphs,” in EMNLP , 2018, pp. 1980–1990. [184] P . Wang, J. Han, C. Li, and R. Pan, “Logic attention based neighborhood aggregation for inductive knowledge graph em- bedding,” in AAAI , vol. 33, no. 01, 2019, pp. 7152–7159. [185] Y. Lin, Z. Liu, M. Sun, Y. Liu, and X. Zhu, “Learning entity and relation embeddings for knowledge graph completion,” in Proceedings of the AAAI conference on artificial intelligence , vol. 29, no. 1, 2015. [186] C. Chen, Y. Wang, A. Sun, B. Li, and L. Kwok-Yan, “Dipping plms sauce: Bridging structure and text for effective knowledge graph completion via conditional soft prompting,” in ACL , 2023. [187] J. Lovelace and C. P . Ros ´e, “A framework for adapting pre- trained language models to knowledge graph completion,” in Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emi- rates, December 7-11, 2022 , 2022, pp. 5937–5955. [188] J. Fu, L. Feng, Q. Zhang, X. Huang, and P . Liu, “Larger-context tagging: When and why does it work?” in Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL- HLT 2021,',\n",
              "  'of the Association for Computational Linguistics: Human Language Technologies, NAACL- HLT 2021, Online, June 6-11, 2021 , 2021, pp. 1463–1475. [189] X. Liu, K. Ji, Y. Fu, Z. Du, Z. Yang, and J. Tang, “P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks,” CoRR , vol. abs/2110.07602, 2021. [190] J. Yu, B. Bohnet, and M. Poesio, “Named entity recognition as dependency parsing,” in ACL , 2020, pp. 6470–6476. [191] F. Li, Z. Lin, M. Zhang, and D. Ji, “A span-based model for joint overlapped and discontinuous named entity recognition,” inACL , 2021, pp. 4814–4828. [192] C. Tan, W. Qiu, M. Chen, R. Wang, and F. Huang, “Boundary enhanced neural span classification for nested named entity recognition,” in The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020 , 2020, pp. 9016–9023. [193] Y. Xu, H. Huang, C. Feng, and Y. Hu, “A supervised multi-head self-attention network for nested named entity recognition,” in Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intel- ligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021 , 2021, pp. 14 185–14 193. [194] J. Yu, B. Ji, S. Li, J. Ma, H. Liu, and H. Xu, “S-NER: A concise and efficient span-based model for named entity recognition,” Sensors , vol. 22, no. 8, p. 2852, 2022. [195] Y. Fu, C. Tan, M. Chen, S. Huang, and F. Huang, “Nested named entity recognition with partially-observed treecrfs,” in AAAI , 2021, pp. 12 839–12 847. [196] C. Lou, S. Yang, and K. Tu, “Nested named entity recognition as latent lexicalized constituency parsing,” in Proceedings of the 60th Annual Meeting of the',\n",
              "  'as latent lexicalized constituency parsing,” in Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022 , 2022, pp. 6183–6198. [197] S. Yang and K. Tu, “Bottom-up constituency parsing and nested named entity recognition with pointer networks,” in Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022 , 2022, pp. 2403–2416. [198] F. Li, Z. Lin, M. Zhang, and D. Ji, “A span-based model for joint overlapped and discontinuous named entity recognition,” inProceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021 , 2021, pp. 4814–4828. [199] Q. Liu, H. Lin, X. Xiao, X. Han, L. Sun, and H. Wu, “Fine-grained entity typing via label reasoning,” in Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021 , 2021, pp. 4611–4622. [200] H. Dai, Y. Song, and H. Wang, “Ultra-fine entity typing with weak supervision from a masked language model,” in Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021 , 2021, pp. 1790–1799.',\n",
              "  'JOURNAL OF LATEX CLASS FILES, VOL. ??, NO. ??, MONTH 20YY 26 [201] N. Ding, Y. Chen, X. Han, G. Xu, X. Wang, P . Xie, H. Zheng, Z. Liu, J. Li, and H. Kim, “Prompt-learning for fine-grained entity typing,” in Findings of the Association for Computational Linguistics: EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022 , 2022, pp. 6888–6901. [202] W. Pan, W. Wei, and F. Zhu, “Automatic noisy label correction for fine-grained entity typing,” in Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI 2022, Vienna, Austria, 23-29 July 2022 , 2022, pp. 4317–4323. [203] B. Li, W. Yin, and M. Chen, “Ultra-fine entity typing with indi- rect supervision from natural language inference,” Trans. Assoc. Comput. Linguistics , vol. 10, pp. 607–622, 2022. [204] S. Broscheit, “Investigating entity knowledge in BERT with sim- ple neural end-to-end entity linking,” CoRR , vol. abs/2003.05473, 2020. [205] N. D. Cao, G. Izacard, S. Riedel, and F. Petroni, “Autoregressive entity retrieval,” in 9th ICLR, ICLR 2021, Virtual Event, Austria, May 3-7, 2021 , 2021. [206] N. D. Cao, L. Wu, K. Popat, M. Artetxe, N. Goyal, M. Plekhanov, L. Zettlemoyer, N. Cancedda, S. Riedel, and F. Petroni, “Mul- tilingual autoregressive entity linking,” Trans. Assoc. Comput. Linguistics , vol. 10, pp. 274–290, 2022. [207] N. D. Cao, W. Aziz, and I. Titov, “Highly parallel autoregressive entity linking with discriminative correction,” in Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021 , 2021, pp. 7662–7669. [208] K. Lee, L. He, and L. Zettlemoyer, “Higher-order coreference resolution with coarse-to-fine inference,” in NAACL , 2018, pp. 687–692. [209] T. M. Lai, T. Bui, and D. S. Kim, “End-to-end neural coreference resolution revisited: A simple yet effective baseline,” in IEEE International Conference on Acoustics, Speech and Signal',\n",
              "  'A simple yet effective baseline,” in IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2022, Virtual and Singapore, 23-27 May 2022 , 2022, pp. 8147–8151. [210] W. Wu, F. Wang, A. Yuan, F. Wu, and J. Li, “Corefqa: Coreference resolution as query-based span prediction,” in Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020 , 2020, pp. 6953–6963. [211] T. M. Lai, H. Ji, T. Bui, Q. H. Tran, F. Dernoncourt, and W. Chang, “A context-dependent gated module for incorporating symbolic semantics into event coreference resolution,” in Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL- HLT 2021, Online, June 6-11, 2021 , 2021, pp. 3491–3499. [212] Y. Kirstain, O. Ram, and O. Levy, “Coreference resolution without span representations,” in Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th Interna- tional Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 2: Short Papers), Virtual Event, August 1-6, 2021 , 2021, pp. 14–19. [213] R. Thirukovalluru, N. Monath, K. Shridhar, M. Zaheer, M. Sachan, and A. McCallum, “Scaling within document corefer- ence to long texts,” in Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021, Online Event, August 1-6, 2021 , ser. Findings of ACL, vol. ACL/IJCNLP 2021, 2021, pp. 3921–3931. [214] I. Beltagy, M. E. Peters, and A. Cohan, “Longformer: The long- document transformer,” CoRR , vol. abs/2004.05150, 2020. [215] C. Alt, M. H ¨ubner, and L. Hennig, “Improving relation extraction by pre-trained language representations,” in 1st Conference on Automated Knowledge Base Construction, AKBC 2019, Amherst, MA, USA, May 20-22, 2019 , 2019. [216] L. B. Soares, N. FitzGerald, J. Ling, and T. Kwiatkowski, “Match- ing the blanks: Distributional similarity for relation',\n",
              "  'J. Ling, and T. Kwiatkowski, “Match- ing the blanks: Distributional similarity for relation learning,” in ACL , 2019, pp. 2895–2905. [217] S. Lyu and H. Chen, “Relation classification with entity type restriction,” in Findings of the Association for Computational Lin- guistics: ACL/IJCNLP 2021, Online Event, August 1-6, 2021 , ser. Findings of ACL, vol. ACL/IJCNLP 2021, 2021, pp. 390–395. [218] J. Zheng and Z. Chen, “Sentence-level relation extraction via contrastive learning with descriptive relation prompts,” CoRR , vol. abs/2304.04935, 2023. [219] H. Wang, C. Focke, R. Sylvester, N. Mishra, and W. Y. Wang, “Fine-tune bert for docred with two-step process,” CoRR , vol. abs/1909.11898, 2019. [220] H. Tang, Y. Cao, Z. Zhang, J. Cao, F. Fang, S. Wang, and P . Yin, “HIN: hierarchical inference network for document-level relationextraction,” in P AKDD , ser. Lecture Notes in Computer Science, vol. 12084, 2020, pp. 197–209. [221] D. Wang, W. Hu, E. Cao, and W. Sun, “Global-to-local neural networks for document-level relation extraction,” in Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020 , 2020, pp. 3711–3721. [222] S. Zeng, Y. Wu, and B. Chang, “SIRE: separate intra- and inter-sentential reasoning for document-level relation extrac- tion,” in Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021, Online Event, August 1-6, 2021 , ser. Findings of ACL, vol. ACL/IJCNLP 2021, 2021, pp. 524–534. [223] G. Nan, Z. Guo, I. Sekulic, and W. Lu, “Reasoning with latent structure refinement for document-level relation extraction,” in ACL , 2020, pp. 1546–1557. [224] S. Zeng, R. Xu, B. Chang, and L. Li, “Double graph based reasoning for document-level relation extraction,” in Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020 , 2020, pp. 1630–1640. [225] N. Zhang, X. Chen, X. Xie, S. Deng, C. Tan, M.',\n",
              "  'November 16-20, 2020 , 2020, pp. 1630–1640. [225] N. Zhang, X. Chen, X. Xie, S. Deng, C. Tan, M. Chen, F. Huang, L. Si, and H. Chen, “Document-level relation extraction as se- mantic segmentation,” in IJCAI , 2021, pp. 3999–4006. [226] O. Ronneberger, P . Fischer, and T. Brox, “U-net: Convolutional networks for biomedical image segmentation,” in Medical Image Computing and Computer-Assisted Intervention - MICCAI 2015 - 18th International Conference Munich, Germany, October 5 - 9, 2015, Proceedings, Part III , ser. Lecture Notes in Computer Science, vol. 9351, 2015, pp. 234–241. [227] W. Zhou, K. Huang, T. Ma, and J. Huang, “Document-level rela- tion extraction with adaptive thresholding and localized context pooling,” in AAAI , 2021, pp. 14 612–14 620. [228] C. Gardent, A. Shimorina, S. Narayan, and L. Perez-Beltrachini, “The WebNLG challenge: Generating text from RDF data,” in Proceedings of the 10th International Conference on Natural Language Generation , 2017, pp. 124–133. [229] J. Guan, Y. Wang, and M. Huang, “Story ending generation with incremental encoding and commonsense knowledge,” in AAAI , 2019, pp. 6473–6480. [230] H. Zhou, T. Young, M. Huang, H. Zhao, J. Xu, and X. Zhu, “Commonsense knowledge aware conversation generation with graph attention,” in IJCAI , 2018, pp. 4623–4629. [231] M. Kale and A. Rastogi, “Text-to-text pre-training for data-to-text tasks,” in Proceedings of the 13th International Conference on Natural Language Generation , 2020, pp. 97–102. [232] M. Mintz, S. Bills, R. Snow, and D. Jurafsky, “Distant supervision for relation extraction without labeled data,” in ACL , 2009, pp. 1003–1011. [233] A. Saxena, A. Tripathi, and P . Talukdar, “Improving multi-hop question answering over knowledge graphs using knowledge base embeddings,” in ACL , 2020, pp. 4498–4507. [234] Y. Feng, X. Chen, B. Y. Lin, P . Wang, J. Yan, and X. Ren, “Scalable multi-hop relational reasoning for knowledge-aware question answering,” in EMNLP , 2020, pp. 1295–1309.',\n",
              "  'relational reasoning for knowledge-aware question answering,” in EMNLP , 2020, pp. 1295–1309. [235] Y. Yan, R. Li, S. Wang, H. Zhang, Z. Daoguang, F. Zhang, W. Wu, and W. Xu, “Large-scale relation learning for question answering over knowledge bases with pre-trained language models,” in EMNLP , 2021, pp. 3653–3660. [236] J. Zhang, X. Zhang, J. Yu, J. Tang, J. Tang, C. Li, and H. Chen, “Subgraph retrieval enhanced model for multi-hop knowledge base question answering,” in ACL (Volume 1: Long Papers) , 2022, pp. 5773–5784. [237] J. Jiang, K. Zhou, Z. Dong, K. Ye, W. X. Zhao, and J.-R. Wen, “Structgpt: A general framework for large language model to reason over structured data,” arXiv preprint arXiv:2305.09645 , 2023. [238] H. Zhu, H. Peng, Z. Lyu, L. Hou, J. Li, and J. Xiao, “Pre-training language model incorporating domain-specific heterogeneous knowledge into a unified representation,” Expert Systems with Applications , vol. 215, p. 119369, 2023. [239] C. Feng, X. Zhang, and Z. Fei, “Knowledge solver: Teaching llms to search for domain knowledge from knowledge graphs,” arXiv preprint arXiv:2309.03118 , 2023. [240] J. Sun, C. Xu, L. Tang, S. Wang, C. Lin, Y. Gong, H.-Y. Shum, and J. Guo, “Think-on-graph: Deep and responsible reasoning of large language model with knowledge graph,” arXiv preprint arXiv:2307.07697 , 2023.',\n",
              "  'JOURNAL OF LATEX CLASS FILES, VOL. ??, NO. ??, MONTH 20YY 27 [241] B. He, D. Zhou, J. Xiao, X. Jiang, Q. Liu, N. J. Yuan, and T. Xu, “BERT-MK: Integrating graph contextualized knowledge into pre-trained language models,” in EMNLP , 2020, pp. 2281–2290. [242] Y. Su, X. Han, Z. Zhang, Y. Lin, P . Li, Z. Liu, J. Zhou, and M. Sun, “Cokebert: Contextual knowledge selection and embedding to- wards enhanced pre-trained language models,” AI Open , vol. 2, pp. 127–134, 2021. [243] D. Yu, C. Zhu, Y. Yang, and M. Zeng, “JAKET: joint pre-training of knowledge graph and language understanding,” in AAAI , 2022, pp. 11 630–11 638. [244] X. Wang, P . Kapanipathi, R. Musa, M. Yu, K. Talamadupula, I. Abdelaziz, M. Chang, A. Fokoue, B. Makni, N. Mattei, and M. Witbrock, “Improving natural language inference using exter- nal knowledge in the science questions domain,” in AAAI , 2019, pp. 7208–7215. [245] Y. Sun, Q. Shi, L. Qi, and Y. Zhang, “JointLK: Joint reasoning with language models and knowledge graphs for commonsense question answering,” in NAACL , 2022, pp. 5049–5060. [246] X. Liu, H. Yu, H. Zhang, Y. Xu, X. Lei, H. Lai, Y. Gu, H. Ding, K. Men, K. Yang et al. , “Agentbench: Evaluating llms as agents,” arXiv preprint arXiv:2308.03688 , 2023. [247] Y. Wang, N. Lipka, R. A. Rossi, A. Siu, R. Zhang, and T. Derr, “Knowledge graph prompting for multi-document question an- swering,” arXiv preprint arXiv:2308.11730 , 2023. [248] A. Zeng, M. Liu, R. Lu, B. Wang, X. Liu, Y. Dong, and J. Tang, “Agenttuning: Enabling generalized agent abilities for llms,” 2023. [249] W. Kry ´sci´nski, B. McCann, C. Xiong, and R. Socher, “Evaluating the factual consistency of abstractive text summarization,” arXiv preprint arXiv:1910.12840 , 2019. [250] Z. Ji, Z. Liu, N. Lee, T. Yu, B. Wilie, M. Zeng, and P . Fung, “Rho (\\\\ρ): Reducing hallucination in open-domain dialogues with knowledge grounding,” arXiv preprint arXiv:2212.01588 , 2022. [251] S. Feng, V . Balachandran, Y. Bai, and Y. Tsvetkov, “Factkb:',\n",
              "  'arXiv:2212.01588 , 2022. [251] S. Feng, V . Balachandran, Y. Bai, and Y. Tsvetkov, “Factkb: Gen- eralizable factuality evaluation using language models enhanced with factual knowledge,” arXiv preprint arXiv:2305.08281 , 2023. [252] Y. Yao, P . Wang, B. Tian, S. Cheng, Z. Li, S. Deng, H. Chen, and N. Zhang, “Editing large language models: Problems, methods, and opportunities,” arXiv preprint arXiv:2305.13172 , 2023. [253] Z. Li, N. Zhang, Y. Yao, M. Wang, X. Chen, and H. Chen, “Unveiling the pitfalls of knowledge editing for large language models,” arXiv preprint arXiv:2310.02129 , 2023. [254] R. Cohen, E. Biran, O. Yoran, A. Globerson, and M. Geva, “Evaluating the ripple effects of knowledge editing in language models,” arXiv preprint arXiv:2307.12976 , 2023. [255] S. Diao, Z. Huang, R. Xu, X. Li, Y. Lin, X. Zhou, and T. Zhang, “Black-box prompt learning for pre-trained language models,” arXiv preprint arXiv:2201.08531 , 2022. [256] T. Sun, Y. Shao, H. Qian, X. Huang, and X. Qiu, “Black-box tuning for language-model-as-a-service,” in International Conference on Machine Learning . PMLR, 2022, pp. 20 841–20 855. [257] X. Chen, A. Shrivastava, and A. Gupta, “NEIL: extracting visual knowledge from web data,” in IEEE International Conference on Computer Vision, ICCV 2013, Sydney, Australia, December 1-8, 2013 , 2013, pp. 1409–1416. [258] M. Warren and P . J. Hayes, “Bounding ambiguity: Experiences with an image annotation system,” in Proceedings of the 1st Work- shop on Subjectivity, Ambiguity and Disagreement in Crowdsourcing , ser. CEUR Workshop Proceedings, vol. 2276, 2018, pp. 41–54. [259] Z. Chen, Y. Huang, J. Chen, Y. Geng, Y. Fang, J. Z. Pan, N. Zhang, and W. Zhang, “Lako: Knowledge-driven visual estion answer- ing via late knowledge-to-text injection,” 2022. [260] R. Girdhar, A. El-Nouby, Z. Liu, M. Singh, K. V . Alwala, A. Joulin, and I. Misra, “Imagebind: One embedding space to bind them all,” in ICCV , 2023, pp. 15 180–15 190. [261] J. Zhang, Z. Yin, P .',\n",
              "  'embedding space to bind them all,” in ICCV , 2023, pp. 15 180–15 190. [261] J. Zhang, Z. Yin, P . Chen, and S. Nichele, “Emotion recognition using multi-modal data and machine learning techniques: A tutorial and review,” Information Fusion , vol. 59, pp. 103–126, 2020. [262] H. Zhang, B. Wu, X. Yuan, S. Pan, H. Tong, and J. Pei, “Trust- worthy graph neural networks: Aspects, methods and trends,” arXiv:2205.07424 , 2022. [263] T. Wu, M. Caccia, Z. Li, Y.-F. Li, G. Qi, and G. Haffari, “Pretrained language model in continual learning: A comparative study,” in ICLR , 2022. [264] X. L. Li, A. Kuncoro, J. Hoffmann, C. de Masson d’Autume, P . Blunsom, and A. Nematzadeh, “A systematic investigation of commonsense knowledge in large language models,” in Proceed- ings of the 2022 Conference on Empirical Methods in Natural Language Processing , 2022, pp. 11 838–11 855.[265] Y. Zheng, H. Y. Koh, J. Ju, A. T. Nguyen, L. T. May, G. I. Webb, and S. Pan, “Large language models for scientific synthesis, inference and explanation,” arXiv preprint arXiv:2310.07984 , 2023. [266] B. Min, H. Ross, E. Sulem, A. P . B. Veyseh, T. H. Nguyen, O. Sainz, E. Agirre, I. Heintz, and D. Roth, “Recent advances in natural language processing via large pre-trained language models: A survey,” ACM Computing Surveys , vol. 56, no. 2, pp. 1–40, 2023. [267] J. Wei, M. Bosma, V . Zhao, K. Guu, A. W. Yu, B. Lester, N. Du, A. M. Dai, and Q. V . Le, “Finetuned language models are zero- shot learners,” in International Conference on Learning Representa- tions , 2021. [268] Y. Zhang, Y. Li, L. Cui, D. Cai, L. Liu, T. Fu, X. Huang, E. Zhao, Y. Zhang, Y. Chen, L. Wang, A. T. Luu, W. Bi, F. Shi, and S. Shi, “Siren’s song in the ai ocean: A survey on hallucination in large language models,” arXiv preprint arXiv:2309.01219 , 2023. APPENDIX A PROS AND CONS FOR LLM S AND KGS In this section, we introduce the pros and cons of LLMs and KGs in detail. We summarize the pros and cons of LLMs and KGs in Fig. 1,',\n",
              "  'pros and cons of LLMs and KGs in detail. We summarize the pros and cons of LLMs and KGs in Fig. 1, respectively. LLM pros. •General Knowledge [11]: LLMs pre-trained on large- scale corpora, which contain a large amount of gen- eral knowledge, such as commonsense knowledge [264] and factual knowledge [14]. Such knowledge can be distilled from LLMs and used for downstream tasks [265]. •Language Processing [12]: LLMs have shown great per- formance in understanding natural language [266]. Therefore, LLMs can be used in many natural lan- guage processing tasks, such as question answering [4], machine translation [5], and text generation [6]. •Generalizability [13]: LLMs enable great generalizabil- ity, which can be applied to various downstream tasks [267]. By providing few-shot examples [59] or finetuning on multi-task data [3], LLMs achieve great performance on many tasks. LLM cons. •Implicit Knowledge [14]: LLMs represent knowledge implicitly in their parameters. It is difficult to inter- pret or validate the knowledge obtained by LLMs. •Hallucination [15]: LLMs often experience hallucina- tions by generating content that while seemingly plausible but are factually incorrect [268]. This prob- lem greatly reduces the trustworthiness of LLMs in real-world scenarios. •Indecisiveness [16]: LLMs perform reasoning by gen- erating from a probability model, which is an in- decisive process. The generated results are sampled from the probability distribution, which is difficult to control. •Black-box [17]: LLMs are criticized for their lack of interpretability. It is unclear to know the specific pat- terns and functions LLMs use to arrive at predictions or decisions. •Lacking Domain-specific/New Knowledge [18]: LLMs trained on general corpus might not be able to gen- eralize well to specific domains or new knowledge due to the lack of domain-specific knowledge or new training data.',\n",
              "  'JOURNAL OF LATEX CLASS FILES, VOL. ??, NO. ??, MONTH 20YY 28 KG pros. •Structural Knowledge [19]: KGs store facts in a struc- tural format (i.e., triples), which can be understand- able by both humans and machines. •Accuracy [20]: Facts in KGs are usually manually curated or validated by experts, which are more accurate and dependable than those in LLMs. •Decisiveness [21]: The factual knowledge in KGs is stored in a decisive manner. The reasoning algorithm in KGs is also deterministic, which can provide deci- sive results. •Interpretability [22]: KGs are renowned for their sym- bolic reasoning ability, which provides an inter- pretable reasoning process that can be understood by humans. •Domain-specific Knowledge [23]: Many domains can construct their KGs by experts to provide precise and dependable domain-specific knowledge.•Evolving Knowledge [24]: The facts in KGs are contin- uously evolving. The KGs can be updated with new facts by inserting new triples and deleting outdated ones. KG cons. •Incompleteness [25]: KGs are hard to construct and often incomplete, which limits the ability of KGs to provide comprehensive knowledge. •Lacking Language Understanding [33]: Most studies on KGs model the structure of knowledge, but ignore the textual information in KGs. The textual informa- tion in KGs is often ignored in KG-related tasks, such as KG completion [26] and KGQA [43]. •Unseen Facts [27]: KGs are dynamically changing, which makes it difficult to model unseen entities and represent new facts.',\n",
              "  'The Internet Movie Script Database (IMSDb) The web\\'s largest movie script resource! Search IMSDb Go! Alphabetical #ABCDEFGH IJKLMNOPQ RSTUVWXYZ Genre Action Adventure Animation Comedy Crime Drama Family Fantasy Film-Noir Horror Musical Mystery Romance Sci-Fi Short Thriller War Western Sponsor TV T ranscripts Futurama Seinfeld South Park Stargate SG-1 Lost The 4400 International French scripts Latest Comments ALL SCRIPTS LIAR LIAR Written by Tom Shadyac & Mike Binder Rough draft, April 1996 INT. FULGHAM KINDERGARTEN -- WEDNESDAY MORNING Two dozen KINDGERGARTNERS listen to their teacher, MS. BERRY. The word \"Work\" is on the blackboard. MS. BERRY \"Work.\" Today we\\'re going to share what our parents do for work. QUICK CUTS of a series of five-year olds standing beside their desks, addressing the class: JEFF My dad is a truck driver. MELINDA My mommy is a doctor. CAROLYN My dad is a librarian and my mom is a vegetarian. THEODORE (with difficulty) My father is a struck-sher-al- engine-ear. CRAIG - My mother is an actress. She works at Denny\\'s. KELLY My daddy works at a place where they make stuff, and my mommy is a mommy. ELLIOT (looking a little CRAZED) My father is a postal worker. The QUICK CUTS end with MAX:Marriott Bonvoy Explore Canada’s SightsOPEN',\n",
              "  'MAX My mom\\'s a teacher. As Max starts to sit: MS. BERRY And your dad? MAX (HESITANT) My dad? He\\'s . . . a liar. MS. BERRY (taken aback) A liar? I don\\'t think you mean \"a liar.\" MAX Well... he wears a suit and goes to court and talks to the judge and-- MS. BERRY (RELIEVED) Oh! I see-- you mean he\\'s a lawyer. Max shrugs. INT. COURTROOM .-- DAY FLETCHER REID, early 30\\'s, stands before the JUDGE. His manner is utterly genuine and convincing. FLETCHER A dark street. . . a stormy night... two desperate men struggle... one man is taken to the hospital, the other to ja il. The prosecutor wants you to believe this is an open-and-shut case of a poor man, brutally victimized. He nods at the victim - - a fragile OLD MAN in his 70\\'s. FLETCHER (CONT\\'D) Well, for once I agree with t he prosecutor. This is an open-and shut case -- but the true victim is my client. Fletcher\\'s CLIENT is a 250 pound brute in a suit. FLETCHER (CONT\\'D) Put yourself in his shoes for a moment--walking home from church, alone, in a frightening part of the suburbs. As he describes his client\\'s movements, Fletcher ACTS THEM OUT: FLETCHER (CONT\\'D) You\\'re nervous, timid, looking over your shoulder -- when suddenly, you encounter him-- (pointing at the old man)',\n",
              "  'pouncing from the shadows. You quiver in fear. The streetlight flashes on something shiny in his hand-- a knife? Suddenly Fletcher becomes the attacker, brandishing a weapon. The jurors RECOIL. FLETCHER (CONT\\'D) And in that terrifying instant you do what any respectable citizen would -- you defend y ou rse lf. Only after you shatter his arm and collarbone do you realize it\\'s all a mistake... the man was merely walking away from an ATM machine, the apparent flash of metal caused by his bank card. He reveals the weapon in his hand is only a credit card. FLETCHER (CONT\\'D) (CONCERNED) As you stand over his crumpled, though potentially still-dangerous form, your he art goes out to him. You want to help. First, you gather up the many bills he dropped, to stop them from bl owi ng away. Second, in an effort to get the name and number of someone to notify, you take his wallet. Finally, you leap into the man\\'s Lexus to head for assistance, when suddenly a police car speeds up. You breathe a sigh of relief: \"Someone to look -after the injured man! Oh joy!\" But do the police applaud your initiative? Do they hail your heroism? No-- they arrest you and throw you in the slammer! He walks along the jury box: FLETCHER (CONT\\'D) � And why? Why does the State turn its massive power against this individual? (takes an IMPRESSIVE moment, then answers his own QUESTION:) Discrimination, (to a black JUROR) But this time it\\'s not based on race. (to a female JUROR) Not based on gender. (to a man wearing a crucifix) Not based on religion, (to a heavy set JUROR)',\n",
              "  \"No--this time it's discrimination based on size! . I know what the prosecution wants you to think - - i t ' s always the big guy's fault. Is that what we've come to as a society -- persecuting people because they're large? Fletcher points accusingly at the opposition. FLETCHER (CONT'D) Shame on you, Mister Prosecutor! Shame on you! (turning back to JURY) The state is trying to barbeque my client on the spit of Justice. Only you can douse the flames. The decision is your s. And please...don't let your emotions run away with y ou . The fact that my client is a family man, raising his sons alone after the tragic death of their mother, has absolutely no bearing on this case. In the front row we see two sad-faced YOUNG CHILDREN. FLETCHER (CONT'D) Instead, let cold reason be your guide as you decide the fate of this church-going, orphan-rasing widower! Fletcher returns to his seat. Jurors, dab their eyes. EXT. COURTHOUSE -- AFTERNOON Fletcher bounds down the stairs, passing a fellow LAWYER, LAWYER How's it going, Fletcher? FLETCHER (he' s won) Another gratifying day serving Justice. Fletcher's huge client catches up to him. CLIENT Hey great job, Mr. Reid. I wish there was some way I could show my appreciation. FLETCHER Stay out of my neighborhood after dark. A PUBLICIST carrying, a clipboard approaches Fletcher. PUBLICIST Mr. Reid, do you have a � moment-? FLETCHER No, I'm late picking up my son. PUBLICIST -Because a couple of reporters want to interview you about your big win today. Fletcher instantly shifts directions. FLETCHER ..\",\n",
              "  'How\\'s my hair? And he\\'s off to woo a GANG OF REPORTERS. EXT. SUBURBAN PORCH - AFTERNOON A sad Max and his mother, AUDREY, wait silently on the steps. MAX What time is it? AUDREY (checks her WATCH) I \\' m sure he just got tied up in court again. Finally, Fletcher\\'s BMW pulls up. Max races to him, delighted. MAX Dad! FLETCHER Maximillian! (calls out a COMMAND) TRANSFORMERS!! . Fletcher instantly becomes a human version of the TRANSFORMER TOY making ROBOTIC MOVEMENTS and SOUNDS. Max knows the routine well, moves in perfect sine with dad. . ... Until -- . FLETCHER Malfunction in vector 3 ! ! Malfunction in vector 3!! (pretends to lose control of a \\'robotic\\' arm) Look out! It\\'s on tickle mode!! Fletcher\\'s \"mechanical arm\" becomes CLAW-LIKE, TICKLING MAX like crazy! Max loves it. Audrey watches these two kids, smiles. FLETCHER (re: Audrey) And who is this lovely lady? Max, could you introduce me? MAX That\\'s no lady, that\\'s mom! AUDREY. � Thanks, Max. � FLETCHER Mom? ! (under his BREATH) Himnm. . . I don\\'t remember her looking that good, (becomes the robot again) Malfunction in Vector 4! Malfunction in Vector 4! Fletcher\\'s other robotic arm becomes a \"pincher\", comes after Audrey.',\n",
              "  \"AUDREY (PLAYFULLY) Keep Vector 4 away from me. Unless you want Vector 4 chopped off. FLETCHER You know, you were much easier when we were married... (re: her luggage) So where are you off too? AUDREY Stanford. I'm delivering a paper. FLETCHER O h really? Where I live, we use a boy on a bike. MAX Hey mom, dad's taking me to see wrestling! AUD REY (MILDLY PROTESTING) Oh, Fletcher! FLETCHER (PLAYFULLY mimicking her) Oh, Audrey! AUDREY Do you have to take him to tho se things? They're so violent. Fleccher IMITATES the familiar wise, old INDIAN CHIEF DAN GEORGE. FLETCHER/DAN GEORGE The boy must learn the way of the war rior . An d wh o be tter to teach him than Rowdy Rod- . Piper and Big John Stud? Audrey can't help but LAUGH. FLETCHER/DAN GEORGE He must be schooled in the way of the face-claw, the sleeper- hold, and the purple nuxple. For only then-- AUDREY (PLAYFULLY) Shut up!! FLETCHER/DAN GEORGE (to Max) The squaw will never understand us. A HORN HONKS. It's the good-natured, affable JERRY. Max runs up to him. JERRY\",\n",
              "  'Max, my man! Jerry gives Max \"five\", then kisses Audrey on the lips. JERRY Fletcher, good to see you? FLETCHER What? No kiss for me? JERRY (re: luggage) What do you say, Max? Give me a hand? Fletcher grits his teeth as Jerry gives Max a piggyback ride to get the luggage. FLETCHER (to Audrey) I didn\\'t know the boyfriend was going. AUDREY Jerry. His name is Jerry and yes, he\\'s going. Audrey heads inside. INT. HOUSE - DAY � . Audrey enters, shuts the blinds. FLETCHER T o Stanford? Overnight? Does this mean you two are... (cringes, can\\'t say the words) AUDREY I\\'ve been seeing him seven months, what do you think? FLETGHER I was hoping that after being married to me, you\\'d have no more strength left. AUDREY . Well you have to remember when we were married, I wasn\\'t having sex nearly as often as you were. FLETCHER MEDIC!! I\\'ve been hit. /- EXT. HOUSE - DAY \\' � \\' Audrey locks up. FLETCHER Well, great... I\\'m so happy for you two. I am just Mister H appy man. Happy, happy, happy. AUDREY Relax, Fletcher. It looks like Jerry\\'s taking that job offer in Boston. Fletcher turns sincere. FLETCHER Aud, I am so sorry...',\n",
              "  \"Behind her back, he FLAILS in celebration. She glances back... He stops, whistles innocently. JERRY (calling to AUDREY) Ready? Audrey and Jerry say goodbye to Max. They get in his Explorer. FLETCHER (to Audrey) Yo u gonna be okay? Because if not, we could leave Max with your sister and I could go out with you two, does that appeal to you at all? They drive off. FLETCHER Wave to the soon-to-be ex- boyfriend, Max. (flipping Max the KEYS) You drive. INT. BMW - AFTERNOON'- MOVING Fletcher is driving, Max beside him. MAX Dad, are we really going to go to wrestling? FLETCHER Absolutely, Maxattacker. We just have to stop by the office for one minute. Max SIGHS. He's heard this before. EXT. SKYSCRAPER - AFTERNOON Establishing the headquarters of ALLAN, STEWART & KONIGSBERG. As they head inside, Fletcher and Max pass a BEGGAR. BEGGAR 'Scuse me, sir. Do you have any change? FLETCHER (patting his POCKETS) 'Fraid not. Sorry. INT. SKYSCRAPER LOBBY - AFTERNOON Fletcher grabs The Daily Journal, paying for it with a HANDFUL OF CHANGE. His son takes this in. On their way to the elevators Fletcher and Max pass PHILIP, a dweebish bore. PHILIP Fletcher! FLETCHER Philip!\",\n",
              "  \"PHILIP And this must be Max! FLETCHER (trying to brush him off) . Yes. Yes it is. Well, it was good seeing you-- Fletcher starts off with Max, when Philip calls after him. PHILIP You know, Ethel and I had a blast at our last little get- together. FLETCHER Oh, me too. I can never get enough of charades. We'll have to do it again sometime. Fletcher heads into an open elevator... only to find the door's closing impededby Philip's foot. PHILIP When? FLETCHER Soon. The door again begins to close... when Philip stops it. PHILIP How 'bout tonight? FLETCHER Not that soon. I'm taking Max to see wrestling-- PHILIP We love wrestling. We could-- FLETCHER I don't think so. See, Max is really shy around strangers. Max looks up at Fletcher. He isn't. FLETCHER (CONT'D) Tell you what -- give me your card as a reminder. I'll call y o u . Soon. Promise. PHILIP Great! Philip hands him his card just as the door closes. INT. ELEVATOR - AFTERNOON Max watches as his father TEARS PHILIP'S CARD IN TWO. INT. RECEPTION AREA OF LAW OFFICES - AFTERNOON The receptionist, JANE, greets them. Jane has an ODD, UNATTRACTIVE HAIRDO. A large GIFT BASKET is on her desk. JANE Hi, Mr. Reid. (indicates, her HAIR) What do you think? FLETCHER Fabulous! I love it. (indicates the � BASKET) What's this?\",\n",
              "  \"JANE I don't know who sent it. But it's for Mr. Allan. It's his anniversary. FLETCHER Ah... The Partnership Committee meeting still scheduled for Friday? JANE (as she goes) Yep... Fletcher quickly removes a gift card from his pocket, scribbles on it, puts it in place of the one already there MAX What are you doing? FLETCHER Oh, I'm... fixing the card, (shows him the old card) Look, they spelled Mr. Allan's name wrong. Have an apple. INT. MIRANDA'S OFFICE - AFTERNOON ...Where a troubled FRED RAND is talking to MIRANDA, a beautiful, steely partner. FRED I can't do it. M IRANDA Fred, it's your duty to present the strongest case possible. FRED The strongest case possible, consistent with the truth. MIRANDA Let the Judge decide what's true. That's what he gets paid for. You get paid to win. FRED If you insist on my taking it to trial, I'll represent Mrs. Cole aggressively and ethically. But, Miranda -- I won't lie. Miranda looks out her window, calculating. MIRANDA Then we'll just have to find someone who'will. INT. HALLWAY OF LAW OFFICES -- AFTERNOON Fletcher strides through the hallway with Max, calling out GREETINGS to his colleagues. FLETCHER Hey, Pete! Great tie! � .\",\n",
              "  \"Max looks at PETE, -whose fashion-disaster tie startles him. FLETCHER Thomas--looks like you're losin' weight. THOMAS glances up from a file. Max notes that he's corpulent. THOMAS Gained three pounds. FLETCHER (wedging past HIM) On you, it works. � Fletcher arrives at his office. WE MEET his secretary, the fiftyish, .worldly-wise and world-weary GRETA. GRETA ' M ax ! What's new? .MAX Well. . . it's my birthday tomo rrow. We're having a party and everything. Flet cher's EYES WIDEN. He has clearly forgotten. GRE TA I'm sure your dad'11 give you something wonderful. Fletcher tries to wave her off, awkwardly stopping when Max turns to him. . MAX Yeah? FLETCHER Oh, yeah. You're going to love it. Uh, why don't you play in my office for a minute? Fax something, sue someone, have a good time. We'll be leaving in a second. Max heads into the office. Fletcher closes the door behind him turning it into a silly, two-handed wave. FLETCHER (CONT'D) Damn! I completely forgot. GRETA Oh, there's a surprise. Greta produces a wrapped GIFT. FLETCHER You're a saint. I should get you something. GRETA You did. She holds up another, smaller package. FLETCHER Ah. Well, I always do the classy thing. Any calls? � She hands him a stack of mail. GRETA . Let's see.. . � (CHECKING\",\n",
              "  'MESSAGES) Judge Patterson\\'s clerk. He needs your filing. FLETCHER Tell him it\\'s in the mail. GRETA (jotting down a NOTE) Right. You\\'ll do it next week. Mr. McKinley phoned, questioning that fourteen hours you billed on Christinas Eve. FLETCHER Write him a long, explanatory letter. Then bill him for the letter. GRETA (jotting down a . note). .. Done. Your mother called. FLETCHER I \\' m on vacation. GRETA This is your fifth week. . � . FLETCHER It\\'s a long vacation. GRETA (jotting down a NOTE) \"Break mother\\'s heart.\" Done. And that\\'s it, except Miranda\\'s looking for you. FLETCHER (checking watch) As if I don\\'t have anything better to do than bow and scrape at her royal perfumed p artner feet. Tell her I\\'m in court. GRETA Court\\'s closed. FLETCHER Tell her I broke my leg and had to be shot. GRETA (WHISPERS) Why don\\'t you tell her yourself? As Miranda approaches, Fletcher switches gears in an INSTANT: FLETCHER --And then send out a notice of judgement on my win today! GRETA',\n",
              "  '(DRY) I\\'ll get right on it. Fletcher turns -- and pretends to be surprised. FLETCHER Miranda! I didn\\'t see you. Hey, you look lovely, today. Here, I bought you a gift. He grabs Greta\\'s gift and hands it to Miranda. MIRANDA Thanks. I heard about your victory today. You\\'re making quite an impression on the partnership committee. FLETCHER (FEIGNING PUZZLEMENT; THEN) Oh, that\\'s right. You folks are meeting again soon. \"Allan, Stewart, Konigsberg, and Ried.\" There\\'s something about the rhythm of fours. It\\'s like a full measure. Well, anyway, I\\'ve got a client waiting in my office-- MIRANDA . Actually, something important h as come up. You\\'re not busy tonight, are you? Before Fletcher answers, we: CUT TO: INT. FLETCHER\\'S OFFICE - EVENING A sad Max sits on Fletcher\\'s big sofa. His heart sinks further when Fletcher enters. . . carrying two boxes of documents. MAX We\\'re not going, are we? FLETCHER Of course we are. A promise i s a promise. We are gonna see wrestling or my name isn\\'t Fletcher T. Reid. FLETCHER (to wrestler) Could you hand me that? (the wrestler DOES) Thank you. (without looking UP) We are having some fun, eh Maxer? PUSH IN on Max; he isn\\'t. � INT. HOTEL ROOM - NIGHT Jerry paces. Audrey is on the phone, waiting. She notices Jerry pacing.',\n",
              "  \"AUDREY Are you alright? JERRY Uh, yeah, just, uh... how long are you gonna be on the phone? AUDREY I just wanted to say good- night to Max, but he must still be out with Fletcher, (hangs up) JERRY (SUDDENLY) Will you marry me? She's SHOCKED. I AUDREY Uh...would I . . . ? What did you say? JERRY (NERVOUS) I proposed, I . . . Look, I know this Boston thing is a great opportunity, good job, mo ney .. . everything. But I started to think about being three thousand miles away from you and Max. And I didn't like it. I-- Look, I know it's a lot to ask, to move and everything, but I . . . . I love you. I love your son. Will you marry me? She stares at him, excited, but nervous. EXT. HIGH RISE APARTMENT BUILDING - MORNING Early morning outside Fletcher's building. INT. FLETCHER'S STUDY - MORNING Fletcher types on his computer. He's been up all night. He leans back, rubs his eyes. When he opens them he sees Max standing there in pajamas. Fletcher SMILES. FLETCHER Max Factor... Happy birthday. How old are you today? T h ir t y ? Forty? MAX Five. FLETCHER Well, you've held up well. I only wish there was some way to commemorate such an occasion, some small symbol to mark this day, like.... Fletcher produces -- FLETCHER . . . A present! .\",\n",
              "  \"Max eyes it with wonder. MAX What is it? FLETCHER (no idea) It's... it's. (it hits him) a surprise. Max knows his father doesn't have a clue but he rips the box open, revealing, a BASEBALL, GLOVE, DODGER'S CAP, and FULL MAJOR LEAGUE STYLE UNIFORM. MAX Baseball stuff! _J FLETCHER Baseball stuff. MAX (hugging his dad) Will you play catch with me? FLETCHER ' � : Absorootentootenlutely. Max beams. FLETCHER Tonight. After your party, you have my word on it. Max nods sadly as Fletcher turns back to his work. EXT. JERRY'S CAR - MORNING Jerry and Audrey are driving. Audrey's holding a couple of airline tickets. AUDR EY (re: tickets) Jerry, these are for tomorrow. JERRY The company wants me to get started right away. AUDREY I can't just pick up and move to Boston with two days notice. JERRY Ju st come check it out. You a nd Max, see the town. Let's . pick out a place together. Then, if you want to turn me down and scar me for life, fine. AUDREY It's just not that simple... What about my job? I've been\",\n",
              "  'at UCLA three years. JERRY It\\'s New England. They\\'re lousy with colleges. You can\\'t swing a bat back there without hitting a college. You\\'d get a job there in a second. AUDREY There are other factors involved. (POINTS) T here they are now. � . They pull up in front of Fletcher\\'s building where Fletcher and Max are waiting. Fletcher\\'s still reviewing a file. As Audrey gets out of Jerry\\'s car, Max runs over. AUDREY Did you have fun? How were the wrestling matches? FLETCHER Big fun. Manly fun. Am I right, Maxie? MAX (HALF-HEARTEDLY) It was fun.. � FLETCHER (re: Audrey) So how were the wrestling matches ? Did you have fun? JERRY Max , my man! My happy birthday man! Max and Jerry exchange \"fives\" and\\'a hug. Jerry gives Max a light punch on the arm. JERRY One-two-three-four-five... and one for good luck. FLETCHER Did you see that? He struck the child! MAX Look what dad got me! (shows the glove) JERRY Whoa! Great! I have my glove in the car. We\\'ll stop in the park on the way home and play c atch. Then tonight we\\'ll oil it, wrap a rubber band around it. . . It\\'ll be great. (to Fletcher) Great birthday present, dad! Fletcher hates him. Jerry and Max go to Jerry\\'s car. FLETCHER � . (makes a fist) When is it his birthday?',\n",
              "  \"AUDREY Some thing's come up. We need to talk. MAX Mom, let's go. I want to play. AUDREY (to Fletcher) We'll talk tonight. � FLETCHER Tonight? AUDREY Max's birthday? FLETCHER' Oh , yeah, right. Seven. I knew that. I did. I blocked it out weeks ago. The se ven tee nt h of May. Max's birthday. AUDREY It's the eighteenth. FLETCHER The seventeenth of May is the day I .remind myself that the eighteenth is Max's birthday. See you tonight. They drive away. INT. MIRANDA'S OFFICE - NIGHT Miranda, and Fletcher's new client, VIRGINIA COLE, an alluring woman in her early thirties/ review the document he spent the previous � night putting together. VIRGINIA This is good. This is really smart. FLETCHER Thank you. VIRGINIA Only i t ' s . . . Like not true. Every word of it is a lie. Fletcher and Miranda exchange glances. VIRGINIA (CONT'D) I mean... isn't that a problem? FLETCHER Mrs. Cole, the only problem here is that after you've provided years of faithful service and loving support, of raising his children -- They are his? VIRGINIA H m ? Oh yeah. One for sure. FLETCHER After all that, your husband wants to deny you a fair share of the marital assets based on one single act of\",\n",
              "  \"] INDISCRETION-- VIRGINIA Seven. FLETCHER Hm? VIRGINIA Seven single acts of indiscretion. FLETCHER --Seven acts of indiscretion, only one of which he has any evidence of, and all of which he himself is responsible, for. VIRGINIA He is? FLETCHER Mrs. Cole, I stayed up all night last night studying your c ase. Not just your case... b ut you. And, by now, I feel I know you. You are the victim here. The wife of a cold, distant businessman. Starved for affection, driven into the arms of another man-- VIRGINIA Seven. FLETCHER (not missing a BEAT) --Seven .other men. You're not trying to deny him what is r ig htf ull y his. All you're insisting on is what is r ig htf ull y yours. And maybe an idgy-smidgy bit more. I think you're bending over backwards. VIRGINIA Well, I did agree to give him j oint custody of the kids... (to Miranda) He's always been a good father. FLETCHER And you've always been a good wife. VIRGINIA (getting worked UP) Yeah. . . FLETCHER There's such a thing as being t oo nice. That's why you need aggr essive representation. To show the court that there is\",\n",
              "  'more than one side to this st ory. All I\\'m asking is the opportunity to see that justice is done on your behalf. (takes her hand) Will you give me that opportunity? He stares into her eyes. A moment, then... VIRGINIA Yes! I \\' m tired of getting kicked around. FLETCHER Good for you! VIRGINIA Thank you, Mr. Reid. I \\' m so grateful I have an attorney I can trust. S he gives him a HUG and momentarily grabs his ass. With a farewell nod to Miranda, she leaves. Miranda turns, smiles at Fletcher, then shuts the door. She moves in on him. MIRANDA You\\'re good. You\\'re really good. FLETCHER Oh, pshaw. (pronounces it with the \"p\") She picks a piece of lint off.his jacket. MIRANDA N o, I mean it. The Cole case is worth a truckload of money to this firm, not to mention the press it\\'s going to g en er at e . You win this case and I guarantee you\\'ll make partner. (STRAIGHTENING his tie) Actually, how would you like to make a partner right now? FLETCHER Excuse me? She grabs his lapels and pulls him in for a deep KISS. INT, AUDREY AND MAX\\'S LIVING ROOM - NIGHT A PARTY in progress, KINDERGARTNERS being entertained by a MAN in a clown suit and clown make-up. CLOWN (SINGING) Captain Fuzzy is my name, Making children happy is my game, With a shake and a juggle, And a big belt buckle, You\\'ll all be glad I came. \\' He flops down on his back causing something in his pants to HONK. Audrey and Jerry watch. AUDREY',\n",
              "  \"(indicating the CLOWN) What do you think? JERRY Well, if you don't hire your brother, who will? . She heads into... INT. KITCHEN - CONTINUOUS � JERRY They called me again from B oston. They really want me there tomorrow. AUDREY . . . I can't go to Boston. JERRY How come? AUDREY Max. JERRY He'll love it there. AUDREY It's Fletcher. JERRY Fletcher? AUDREY I can't move Max three thousand miles away from his father. JERRY Audrey, I have never said a bad word about your ex -- AUDREY I know. JERRY But how much responsibility does Fletcher take for Max, now? He'd never come over if you didn't remind him. AUDREY I know. But if they're three thousand miles apart they'll never see each other. Fletcher will never come to Boston and how can I send Max cross-country to him? .JERRY So because your ex-husband is unreliable, we can't-'- AUDREY\",\n",
              "  \"I know, it's not logical, it's emotional. I'm sorry. Pause. JERRY I still want to marry you. AUDREY Are you sure? Jerry picks up the PHONE, pulls out a piece of paper, dials. JERRY (into phone) Mr. Crisitelli, Jerry She!ton... I hope I'm not calling too late... Mr. Crisitelli, I'm afraid I have to turn down your offer..'. So am I . . . Well, I've fallen in love with this beautiful woman in L.A. and she doesn't want to leave and I won't leave without her... Well, thank you very much... Yes, good-bye. (hangs up) He wasn't there, but that's 1 the speech I would've made. She smiles and KISSES him. The PHONE RINGS. Audrey answers. AUDREY Hello... INTERCUT WITH: INT. MIRANDA'S OFFICE - NIGHT A UDREY Fletcher, where are you? We're getting ready to cut the cake. FLETCHER Urn, actually, something has come up. A problem on a new caaa-- . Miranda bites one of Fletcher's nipples. F LET CHE R A-h-h-h-! AUDREY What happened? FLETCHER Nothing. I just nailed my � knee into the desk... Listen, I'm really sorry I can't 'make it. AUDREY Max is going to be so disappointed. FLETCHER I'll make it up to him, I promise. I'll pick him up from school tomorrow, okay?\",\n",
              "  'AUDREY Do you want me to put him on the phone? Miranda starts \"reeling in\" the phone cord. FLETCHER Ah, no. I have to go. AUDREY Right. ANGRILY, she hangs up. Fletcher stares UNHAPPILY at the phone, before Miranda THROWS HIM BACK ONTO THE COUCH. INT. AUDREY AND MAX\\'S DINING ROOM - NIGHT WE PAN DOWN from banners reading HAPPY BIRTHDAY, MAX!. . . to a room full of guests... to a desultory five-year-old. Audrey finishes lighting the candles on the homemade cake. AUDREY All right, birthday boy, make a wish. Max doesn\\'t respond. AUDREY (CONT\\'D) C\\'mon, honey. It can be anything-- whatever you want most in the world. When he .doesn\\'t respond, she leans down to him.. AUDREY (CONT\\'D) Max, your dad is sorry. He had to work. MAX He said he was coming. He promised. AUDREY Yes, well, he... promises he\\'ll see you tomorrow. Max doesn\\'t believe it. < He turns his full attention to the candles on the cake. In VOICE OVER we hear what she does not. MAX (V.O.) I wish, for just one day, Dad couldn\\'t tell a lie. He takes a breath --and blows out all the candles. A strange WIND blows the drapes and the wisp of smoke up, up, up... to the clock on the wall. It\\'s 9:15. CUT TO: A clock on a wall. It\\'s 9:15\\'. We are-- INT. MIRANDA\\'S OFFICE - NIGHT PAN around Miranda\\'s office, where the displaced sofa is adorned with Fletcher\\'s shoes... To the credenza, where Fletcher\\'s pants hang... To the lamp, where Fletcher\\'s shorts swing... To the desk, where a ravished Miranda lies next to Fletcher. Superbly confident of the answer, she asks-- MIRANDA S o . . . was it good for you? Without thinking, Fletcher responds in the most astonishing way possible-- he TELLS THE TRUTH. FLETCHER I\\'ve had better.',\n",
              "  'Miranda turns to him in disbelief -- but it\\'s nothing compared to the LOOK OF SHOCK on Fletcher\\'s face. INT. HALLWAY OUTSIDE MIRANDA\\'S OFFICE - NIGHT The door opens -- and the naked Fletcher is forcefully kicked out. He goes TUMBLING over a desk as a RAIN OF CLOTHES follow. The door SLAMS SHUT again, leaving him without his pants. A CLEANING LADY stares at him in shock, then takes her broom, aims for his crotch, SWINGS.and. . . INT. BEDROOM - FRIDAY MORNING An alarm CLOCK RINGS. Fletcher BOLTS UP in bed. With regret and wonder he remembers: FLETCHER \"I\\'ve had better?\" INT. FLETCHER\\'S BATHROOM - MORNING Fletcher brushes his teeth, looks up at his reflection in the mirror, mouth foaming. FLETCHER \"I\\'ve had better?!\" INT. HALLWAY OF FLETCHER\\'S APARTMENT BUILDING - MORNING Dressed for work, Fletcher waits for the elevator. FLETCHER (laughing it off) \"I\\'ve had better?\" I t arrives. He steps in. INT. APARTMENT ELEVATOR - MORNING The elevator is empty, except for Fletcher... and a beautiful young WOMAN. FLETCHER New in the building? MODEL I just moved in Monday. FLETCHER Ah. Well, you must allow me to give you the grand tour. MODEL (SHE\\'S INTERESTED) O h? Do you do that for all the new tenants? FLETCHER No. Just the ones I want to bang like a drum. Fletcher\\'s face REGISTERS extreme SHOCK and... INT. LOBBY OF APARTMENT BUILDING - MORNING We HEAR a SMACK off camera and a PING as the elevator door opens. The model storms off and A STUNNED Fletcher steps',\n",
              "  'out, rubbing his freshly slapped face. EXT. COURTROOM -, MORNING A SHAKY Fletcher strides toward the courthouse... when he is accosted by a BEGGAR. BEGGAR Any change, Mister? .FLETCHER Absolutely. But he continues walking. BEGGAR Could you spare some? FLETCHER Unquestionably. Fletcher walks faster, PUZZLED that he has answered truthfully. The beggar is even more puzzled. BEGGAR Will you? FLETCHER No. BEGGAR How come? FLETCHER Because I resent your p re sen ce. You fill me with an unpleasant mixture of disgust and guilt. Further, I don\\'t believe you\\'ll use the money for food, but I believe you\\'ll use it for, at worst, drugs, or, at best, whiskey, or . cigarettes. Also, I\\'m cheap. As Fletcher heads up the stairs... BEGGAR Jerkoff. INT. COURTROOM - MORNING A winded Fletcher joins Virginia at the respondent\\'s table, VIRGINIA You look like you\\'re having a rough morning. FLETCHER I\\'ve had better. He WINCES as he recognizes the words. Then, an extremely wealthy, respectable industrialist, RICHARD COLE enters with his attorney, DANA APPLETON, young, brisk, confident. DANA Good morning, Fletcher. FLETCHER Dana. \" RICHARD All right, Virginia, how much will it take to put an end to this?',\n",
              "  \"FLETCHER Fifty per cent of your estate. Richard is SHOCKED. DANA Fifty per cent? With a pre- nup and proof of adultery? What's your case? FLETCHER Our case is simply this. . . Fletcher opens his mouth to enlighten her -- but he CAN'T GET THE WORDS OUT. He tries to FORCE OUT SOUNDS, but succeeds only in looking like a fish gasping on dry land. DANA . Interesting, though based on your track record, I expected a little more. Nearing panic, Fletcher whirls to his BRIEFCASE and grabs the brief. FLETCHER .. Wa it! Wait! I've got it in writing! But when Dana tries to take the document, the astonished Fletcher finds himself PHYSICALLY UNABLE TO RELEASE IT. DANA Let go! FLETCHER I'm trying! He INVOLUNTARILY snatches the document away and IT PULLS HIM to a nearby TRASH CAN where he throws it out. At this moment the BAILIFF calls. BAILIFF All rise for the Honorable Judge William Stevens. DANA Very funny, Fletcher. You want to play hardball, I'm game. JUDGE STEVENS takes the bench. JUDGE STEVENS Calling case BA 09395, Richard Cole versus Virginia Cole. How're we doing this morning, counsel? DANA Fine, thank you. JUDGE STEVENS And you, Mr. Reid? FLETCHER Well, I'm a little upset about a bad sexual episode I had last night-- Fletcher screeches to a standstill, suddenly aware of what he just said. After an awkward silence-- JUDGE STEVENS (DRYLY) Well, you're still young. It'll happen more and more.\",\n",
              "  'In the meantime, what do you say we get. down to business? First, Mr. Reid, I see that your client was previously represented by Mr. Rand of your office. FLETCHER Yes, Your Honor. JUDGE STEVENS I take it you\\'re seeking to substitute in as counsel? FLETCHER Yes, Your Honor. JUDGE STEVENS Fine , fine. And for the record, the reason is? FLETCHER Mr. Rand had severe ethical objections to my client\\'s case. Fletcher is incredulous. Somehow his greatest asset in the world, his mouth, has become his worst enemy. JUDGE STEVENS I take it you don\\'t share the same ethical objections, Mr. Reid? FLETCHER I have lower standards, Your Honor. JUDGE STEVENS I see. Well, if Mrs. Cole wants the substitution of counsel, I\\'ll allow it. Is that what you want, Mrs. Cole? Virginia looks to the judge, then to Fletcher, whose unorthodox syle seemed so brilliant earlier. VIRGINIA (UNSURE) Yes? JUDGE STEVENS Fine. VIRGINIA (aside, to FLETCHER) What are you doing? FLETCHER (WORRIED) . I don\\'t know. (to judge, with SOME DESPERATION) Your Honor, I \\' d like a continuance! JUDGE STEVENS \" This case has already been',\n",
              "  \"delayed several times, Mr. Reid. FLETCHER I realize that, Your Honor, but I ' d really, really, really like a continuance. JUDGE STEVENS I'll have to hear good cause, counselor. What's the problem? FLETCHER'S P.O.V. The ROOM begins to SPIN slowly -- then faster -- then faster -- until we wind up squarely on -- FLETCHER'S FACE FLETCHER I can't lie! JUDGE STEVENS (IMPATIENT) Commendable, Mr. Reid, but I'm still waiting for the good caus e. Now, do you have it or . not? ' FLETCHER (TRUTHFUL) Not. JUDGE STEVENS Motion for a continuance denied. Is there any chance of a settlement in this case? DANA I don't think so, Your Honor. Mr. Reid made it abundantly clear that the last thing in the world he wanted was to -- FLETCHER (DESPERATE) SETTLE! SETTLE! SETTLE! Dana and Mr. Cole look at Fletcher with surprise. JUDGE STEVENS There appears to have been a change in strategy. Let's go to my chambers and negotiate. He BANGS the gavel. INT. JUDGE STEVENS'S CHAMBERS - MORNING Dana and an apprehensive Fletcher sit before the judge DANA Your Honor, under the terms of the prenuptual agreement, if Mrs. Cole commits adultery, she is entitled to nothing. We have in our possession an audiotape made by a licensed private investigator of an\",\n",
              "  \"explicit act of sexual congress with a man who is not her husband. JUDGE STEVENS Sounds pretty damning, Mr. Reid. FLETCHER It certainly does. DANA However, my client has no desire to see his ex-wife destitute. Against my advice, he's willing to offer her a cash settlement of.two point four million dollars. JUDGE STEVENS Two four seems like a pretty fair offer, Mr. Reid. FLETCHER Fantastically fair. Phenominally fair. In fact, I'd say beyond fair, bordering on stupid. Dana fumes. The judge finds Fletcher's boldness refreshing. JUDGE STEVENS What are you suggesting, Mr. Rei d? That Ms. Appleton's willingness to proffer such an offer betrays a lack of faith in her position? FLETCHER (utterly sincere) No, not at all. She's got my client dead to rights. When attorneys go to sleep at night, they dream of having a case as strong as hers. DANA Can the sarcasm, Reid. All right, I admit it -- I've seen you make even the lamest case fly. But this time I have you. Even Clarence Darrow couldn't explain this away. She brandishes the audiotape. JUDGE SAMIOAN Wel l, Mr. Reid? without a dynamite explanation, I'd say you're dead in the water. How's you client's story? FLETCHER The best that money can buy, Y o u r Honor. � JUDGE STEVENS Strong corroborating evidence? FLETCHER We have evidence that you are not going to-believe. Despite herself, Dana is beginning to look worried. JUDGE STEVENS You're pretty confident how\",\n",
              "  'this trial is going to come out, eh, Mr. Reid? FLETCHER (hopeless) . \"Confident\" is too weak a word, Your Honor. I am certain what will happen if I take this puppy to trial. The verdict will be a stunning, humiliating defeat that will cut a spectacularly promising legal career off at the knees. Fletcher is referring to himself, of course, but Dana thinks he\\'s speaking about her. She buckles. DANA All right! Double the offer! Four point eight! And not a penny more. (venomous, to FLETCHER) Bastard! She storms out, leaving an astounded Fletcher behind. JUDGE STEVENS You are some negotiator, Mr. Reid. If your client has half a brain, she\\'ll jump at the offer. CUT TO: VIRGINIA No! We are -- INT. COURTROOM - MORNING Fletcher has joined Virginia at the respondent\\'s table FLETCHER No? ! Mrs. Cole, this offer was a miracle. I\\'m talking about a walking-on-water, Lazarus-rising-from-the-dead, FIND-NO-LINE-AT-THE-FRIGGIN\\'- DMV miracle! You\\'ve gone from two point four to four point eight million in... (checks his WATCH) four minutes. Think of it this way -- now you\\'re getting paid seven hundred thou per schtupp! � � � VIRGINIA Mr. Reid, you convinced me yesterday -- I\\'m the victim here, starved for affection, driven into the arms of another man-- FLETCHER Seven! �',\n",
              "  \"VIRGINIA -- Seven other men. With the story you came up with, I don't think I can lose. I want to proceed. FLETCHER Mrs. Cole, you don't � . understand, I-- But before Fletcher can finish, the judge enters. JUDGE STEVENS Well , Mr. Reid. Do we have a settlement? Fletcher looks pleadingly at his client, but she is firm. He shakes his head unhappily. The judge is irritated. JUDGE STEVENS (CONT'D) Th ere' s no settlement. Trial to start at one-thirty sharp. He BANGS the gavel. Fletcher emits an involuntary whimper. INT. HALLWWAY OF LAW OFFICES - MORNING DA ZED, Fletcher makes his way down the hall. Jane comes toward him wearing a hairstyle that resembles a nest. He tries to avoid her, but... JANE What do you think? FLETCHER . I think you need help. HORRIFI ED, Fletcher hurries on. The heavyset Thomas ambulates in his way. � - � THOMAS What's shakin', Fletcher? FLETCHER Your cellulite, Tubster. The now panicked Fletcher breaks into a run, passing Fred. FRED Hiy a, Fletcher. How's the Cole case going? FLETCHER (not stopping) 'Straight into the crapper, you wuss, with my career right behind it. P Fletcher is RUNNING NOW, COVERING HIS EARS and SINGING LOUDLY so as not to hear OTHER EMPLOYEE 'GREETINGS... FLETC HER LA-LA-LA-LA-LA-LA!! Fletcher speeds past-- � GRETA Hi, boss. What's happening WITH-- FLETCHER DON'T ASK! FOR GOD'S SAKE,\",\n",
              "  'PLEASE DON\\'T ASK! -- And races into his office. I NT . FLETCHER\\'S OFFICE - MORNING He leans against the door, trying to catch his breath. � FLETCHER, (PACING) Do n\\'t panic. You can beat this -- it\\'s all a matter of willpower. He dives for his desk and rifles through it. FL ET CH ER A test. . . Something small... Aha! He holds up a BLUE PEN. FLETCHER (CONT\\'D) Red. Red. All right. Focus, (with great DELIBERATION) The color of this pen is � r--. R--. R--! The color of this pen is--blue! AAAAHH! (burying his HEAD) Ahhhh! One\\' tiny lie and I can\\'t say it!! (suddenly sitting UP) \\' I\\'ll write.it! He takes a sheet of PAPER, his pen and writes \"This pen i s . . . \" He tries to write an \"R\" but can\\'t. He STRAINS. S TR AIN S HARDER. He\\'s out of his chair, on the desk. His feet KICK OVER OBJECTS on the shelves\\'behind him. He finally forces pen to paper. He looks down where he wrote INADVERTENTLY: \"This pen is blue.\" F LE TC HE R NO, NO, NO, NO, NO! ! ! ! Greta enters to find-- FLETCHER running around the office, shaking the blue pen in the air. GRETA Boss, what\\'s wrong? FLETCHER . The pen is blue!! The pen is . b l u e ! ! The GODDAMN PEN IS BLUE !!! Almost weeping, he collapses into a chair. A moment -- then Greta tentatively offers him a red pen. GRETA Red?',\n",
              "  'FLETCHER (BITTER) Oh, that\\'s easy for you to say?! GRETA Are you all right? FLETCHER (getting up) I have to go home. GRETA Home? Was the case settled? FLETCHER No. I have to be in court at one-thirty. GRETA Well, then how can you.go home? FLETCHER I don\\'t know, I don\\'t know!!! GRETA Okay. (walking on EGGSHELLS) Before I forget -- Rubin and Dun n called. They want to know where the Darvis settlement offer stands. FLETCHER I only proposed a settlement t o dick with them. I never had any intention of going th ro ug h with it. . Not certain why her boss would .shoot himself in the foot, Greta nonetheless jots down his remarks. GRETA \\' � ...dick with them.\" Okay. Your accountant, Philip, called to remind you about getting together. . FLETCHER I\\'d rather shave my ass and sit in vinegar.. GRETA (jotting down a NOTE) Got it. And your mother called again. Are you still on vacation? FLETCHER (EMPHATICALLY nodding \"yes\") No. GRETA So then you\\'re here? FLETCHER (EMPHATICALLY',\n",
              "  'shaking his head \"NO\") Yes. GRETA I \\' m having a little trouble following you. what do I say to your mom? FLETCHER (RESIGNED) Tell her I\\'m a thoughtless son who\\'d rather spend ten hours clogging the wheels of justice than five minutes talking to her-- but only if she asks. You.might also add that she deserves better, though I hope to God you don\\'t. GRETA Thanks for clearing that up. And that\\'s it, except your ex called and asked when you were cowing over to see your son. FLETCHER (REMEMBERS) OHH! I\\'M SUCH A SHIT!! He reacts, particularly stunned by this truth. INT. VOLVO - MOVING / FLETCHER\\'S OFFICE - MORNING Audrey is driving Max, who wears his new baseball uniform when her cellular PHONE RINGS. She picks it up. We INTERCUT between car and office. FLETCHER AUDREY-- AUDREY\\' Hey, Fletcher. T was wondering if you were going to still pick up Max after school today. FLETCHER I don\\'t think I can. I had a case I was certain would settle and it didn\\'t. I have to go to trial this afternoon, God help me. AUDREY (not believing HIM) Right. FLETCHER It\\'s true... I really do want to see Max, today. Fletcher considers what he just said, realizes it is true. FLETCHER (CONT\\'D) How about that. I really do. AUDREY',\n",
              "  '(CYNICALLY) But things keep coming up at the last minute. FLETCHER Yes, but-this time it\\'s different. AUDREY I see. And how is that? . � FLETCHER ( he walked into IT) This time I\\'m telling the truth. AUDREY But last night you weren\\'t? FLETCHER No. AUDREY What were you doing? FLETCHER Having sex. AUDREY (barely holding her temper), It must have been with someone very \"special.\" FLETCHER No. It was with someone I don\\'t even like. But I thought it would help my career and at the moment that seemed more important than attending my son\\'s birthday! A U D R EY M y God!! She SLAMS DOWN the phone. INT. FLETCHER\\'S OFFICE Fletcher BANGS THE PHONE against his head in frustration! \\'. FLETCHER A H H H H H H ! ! I WHAT IS WRONG WITH ME I ! I E X T . FULGHAM KINDERGARTEN - MORNING The Volvo parks. Audrey gets out. She leans over to say good-bye to her son. , . MAX Is dad picking me up? AUDREY No, I\\'m sorry, Max. He can\\'t make it. I will. I\\'ll work it out.',\n",
              "  \"Max is disappointed. MAX I guess my wish didn't come � true. AUDREY What wish? MAX I wished that, for just one day, Dad couldn't tell a lie. Max heads toward his teacher. Audrey is deeply moved. INT. FLETCHER'S OFFICE He's dialing the phone. FLETCHER Answer, answer, answer... INTERCUT WITH AUDREY'S CAR AUDREY Hello. FLETCHER Audrey, let me explain. Something has happened to me-- AUDREY Fletcher, something else is about to happen to you. FLETCHER. What do you mean? AUDREY Max and I are moving to Boston. FLETCHER What?! AUDREY Jerry asked me to marry him. He wants Max and I to fly with him this weekend to pick out a house. And I'm going to go. God knows I don't have any reason to stay here. FLETCHER (panicking) . Wait, you can't move! If you take Max away... I'll practically never see him. AUDREY Well then you'll have pretty much the same relationship you have with him now. FLETCHER Audrey, please.... Is this because of what I just said on the phone? AUDREY That was the straw and this is the camel's back saying goodbye.\",\n",
              "  'FLETCHER Where are you? AUDREY Heading home. FLETCHER When you gee there, stay there. I\\'ll be right over. We have to talk. AUDREY FLETCHER-- FLETCHER I\\'ll be right- there! He hangs up and heads for the door. It opens and Miranda enters. FLETCHER (CONT\\'D) Aaaah! MIRANDA Flet cher. Fletcher, Fletcher, Fletcher. I must confess-- after last night\\'s incident, I was. . . hurt. So hurt. I was tempted to do whatever little things lie in my power to scuttle your chances of making partner. Fletcher is FRIGHTENED. MIRANDA (CONT\\'D) But then I thought, \"No, that \\'s not fair. Fletcher didn\\'t mean to insult me.\" (STRAIGHTENING his tie) \"It was just some massive, boneheaded misunderstanding, and Fletcher is very, very sorry.\" Fletcher smiles. It looks like he\\'s off the hook, until-- MIRANDA (CONT\\'D) Isn\\'t that right, Fletcher? FLETCHER (in agony) Uh -- not really, no. MIRANDA (stunned, angry) N o ? No?! What are you sayi ng? Have you no respect for me?! FLETCHER None, whatsoever. I mean, I \\' d like to respect you, and if it weren\\'t for your mistreatment of the associates, your rudeness to the staff, and the fact that your work sucks, I would. M IRANDA But -- what about last night?',\n",
              "  'FLETCHER I was afraid you wouldn\\'t support my partnership if I turned you down. Plus, I have an immature need for sexual conquests. INT. HALLWAY OF LAW OFFICES - MORNING W e HEAR A SMACK! The door flies opens -- and a furious Miranda stalks off. � INT. FLETCHER\\'S OFFICE - MORNING Fletcher rubs his freshly SLAPPED FACi.. INT. BMW - MOVING / EXT, STREET - MORNING Fletcher speeds away. He pulls the blue pen from his pocket. FLETCHER Gotta focus. . . gotta focus. He\\'s so preoccupied that he speeds through a crosswalk and almost hits an OLD MAN. FLETCHER The color of the pen is -- re d ! . But he hasn\\'t regained the ability to lie -- he\\'s referring to the RED LIGHT he just ran, nearly colliding with a truck. The DRIVER screams: DRIVER What\\'s your problem, schmuck?! FLETCHER (the truth) I \\' m an inconsiderate prick! Fletcher once again focuses on the blue pen. FLETCHER (CON \"ID) C\\'mon, you can do this! The color of the pen is -- RED! This time he\\'s referring to the flashing red light of a POLICE CAR in his rearview mirror. FLETC HER S hi t !! Fletcher pulls over. A POLICE OFFICER strolls up. POLICE OFFICER Do you know why I stopped you? FLETCHER Depends on how long you were following me. POLICE OFFICER Why don\\'t we take it from the top. FLETCHER � (in agony) Here goes -- I didn\\'t fasten my seatbelt, I didn\\'t glance in my rearview mirror, I didn\\'t signal when I pulled away from the curb, I sped, I followed too closely, I ran a',\n",
              "  \"stop sign, I almost hit :a Chevy Camaro, I almost hit a ge ezer, I sped some more, I failed to yield at a crosswalk, I changed lanes in the intersection, I changed lanes without signalling, and I changed lanes in the INTERSECTION-WITHOUT signalling while running a red light and speeding. A long moment. POLICE OFFICER May I see your driver's license? FLETCHER No. POLICE OFFICER And why is that? FLETCHER It's in my other pants. POLICE OFFICER I see. And where are your � other pants? FLETCHER Hanging from my boss's credenza. POLICE OFFICER Do you expect me to believe that? FLETCHER No. POLICE OFFICER Do you think I'm an idiot? FLETCHER Yes -- but that's beside the point! My license actually is in my other pants, and they actually were hanging from a credenzaJ I wouldn't lie to you! I mean, I would if I could, but I can't! POLICE OFFICER I see. So you ..have no reason to try and hide your license from me? FLETCHER I didn't say that. I have other reasons. Seventeen reasons, to be precise. (begrudgingly, off the officer's look) Unpaid parking tickets. (BESEECHINGLY) Be gentle. EXT. AUDREY'S HOUSE - MORNING\",\n",
              "  \"A cab speeds up to the house. Fletcher runs out. Audrey is headed to her car. FLETCHER Audrey, wait! AUDREY Wait? You know, I just had an insight into myself. I ' m crazy. You call me up and . tell me to wait here because you'll be right over and -- here's the crazy part -- I actually wait. FLETCHER I can explain-- AUDREY I missed a department meeting. I. . . Did you come in a cab? FLETCHER Yes. AUDREY Where's your car? EXT, POLICE IMPOUND YARD - MORNING Audrey finishes paying the impound-yard CASHIER and joins Fletcher, who is waiting alongside hundreds of towed cars. FLETCHER Thank you. . I can't tell you how much this means to me. AUDREY I can. One thousand, six hundred, and fifty-four dollars and eleven cents. FLETCHER Ow. At this moment WE HEAR a hideous scraping noise -- and a TOW-YARD EMPLOYEE whips Fletcher's BMW into view and. parks... revealing a prominent new scrape on the door. FLETCHER (CONT'D) You scratched my car! TOW-YARD EMPLOYEE Where? . FLETCHER Right there! TOW-YARD EMPLOYEE Oh that? That was already there. FLETCHER (OUTRAGED) Why, you -- you liar! Do you know what I ' m going to do about this?\",\n",
              "  'TOW-YARD EMPLOYEE What? FLETCHER (angrier and ANGRIER) ...Nothing! Because if I take you to small-claims court, it will just drain eight hours out of my life, and you probably won\\'t show up, and if I finally got the judgment you\\'d just stiff me anyway, so what I\\'m gonna do is piss and moan like an impotent jerk and . then bend over and take it up the tail pipe! TOW-YARD EMPLOYEE You\\'ve been here before, haven\\'t you? He flips Fletcher the keys and goes. AUDREY Well I can\\'t remember when I\\'ve had more fun, now if you\\'ll excuse me, I have a class. She starts out. FLETCHER Audrey, wait. I want to talk to you about this Boston situation. . AUDREY What do you want to say? FLETCHER You can\\'t go. It\\'s not fair. Taking Max three thousand miles away is not fair. AUDREY Let\\' s define \"fair.\" Last � night a five-year old boy was crushed because his father lied to him about coming to his birthday party. Fair? FLETCHER Last night-- AUDREY -- Was none of my business. When it happened two years ago it was my business, but now I don\\'t have to care anymore. See, that\\'s the magic of div orce . But it does matter to Max. Everything you do matters to him... and everything you don\\'t do. FLETCHER All right-- now let me tell you something...you\\'re absolutely right. I\\'m guilty of all charges. I\\'m throwing myself on the mercy of your -court. Audrey doesn\\'t know what to say. Fletcher seems very sincere, but she can\\'t trust him.',\n",
              "  \"FLETCHER {CONT'D) I have an idea. I'll come over tonight, right after court lets out and play with M ax. Have him invite some fri ends over. We'll have a game and everything. Then, you and I can sit down and talk. AUDREY We're suppose to be on a plane TONIGHT-- FLETCHER No, Audrey. Just talk to me abou t this first. Please. Au drey, I've lost you. Don't make me lose Max, too. AUDREY You're really coming? FLETCHER This is iron-clad. This is the mother of all promises. What time? AUDREY ...Six? FLETCHER Ten-to-six. AU DR EY (UNSURE) All right... only if I tell Max you're coming and you don't show up and I have to see that look on Max's face -- that heartbreaking look-- it's Boston, Fletcher. FLETCHER. I will be there. As Audrey gets in her car -- . AUDREY I hope so. Do you know what your son was doing at nine- fif teen las t ni ght? He was making a wish on his birthday c ake. He was wishing that, for just one day, his dad couldn't tell a lie. She drives away. Fletcher starts for his car, pensive, when a new thought strikes him. FLETCHER Oh my God! That-'s it! An innocent kid - - a heartfelt plea-- a birthday wish! Sure, it' s impossible --but it 'makes sense!..! If he can wish it, he can unwish it! INT. ELEMENTARY SCHOOL HALLWAY Fletcher marches quickly down the hall, cake box under his\",\n",
              "  \"arm. INT. MAX'S KINDERGARTEN CLASS Ms. Berry's reading a story when Fletcher enters. Max brightens. MAX Dad! MS . BERRY Are you Max's dad? I ' m Ms. Berry, Max's teacher! FLETCHER Hi. Listen, I need to talk to MAX-- MS. BERRY Mr. Reid, we were just talking ab ou t careers. You're a lawyer, aren't you? FLETCHER (WARY) Yes. MAX Mr. Reid it'would be wonderful for the children to hear something positive about lawyers! FLETCHER Well, actually-- � MS. BERRY Children! .Mr. Reid is going to tell us what it's like to be a lawyer. She leads the kids in APPLAUSE. Fletcher takes center s tage. The children stare, rapt with attenion. FLETCHER Uh, hi. Uh, I'm a lawyer and I work at a big law firm with a lot of other lawyers and I do stuff in a law court. Thank you. He starts out. MS. BERRY One moment, Mr. Reid. Maybe some of the children have QUESTIONS (hands shoot up) Jeffrey? JEFF What kind of lawyer are you? FLETCHER Mostly, I ' m a divorce lawyer. BILLY What's that? FLETCHER\",\n",
              "  'It means if you\\'re daddy left your mommy, he\\'d call me. CRAIG So what do you do? FLETCHER (growing more and more impatient) I help people fight over their money and their children. THEODORE Can\\'t they fight without you? FLETCHER \\' \" They could but then J wouldn\\'t make a living. JILL Why would my daddy leave my mommy? FLETCHER To marry a younger woman. To escape a loveless marriage and have cheap meaningless sex. To cling to an illusion of youth as his body gives way to sore backs, flat feet, spare tires, gum disease, hair loss, liver spots, kidney stones, clogged arteries, diabetes, goiter and eventual death. The kids EYES GO WIDE. A moment, then: MS. BERRY (BRIGHTLY) Well, I think it\\'s time for fingerpainting. EXT. PLAYGROUND - DAY They\\'re in the playground just outside the classroom. FLETCHER Monster-Max. MAX Dadzilla. You came to play catch? FLETCHER No. I \\' d like to, but I can\\'t right now. Max is disappointed again. FLETCHER (CONT\\'D) I \\' m sorry I missed your party la st night. How was your Uncle Glen? MAX Stupid. His big nose and stupid orange hair... FLETCHER That\\'s why he should have worn make-up. Fletcher elbows Max, playfully, trying to induce a laugh. Max doesn\\'t laugh.',\n",
              "  \"' MAX . I want to play kickball with my friends.- FLETCHER Yeah, okay, urn... Your mother told me about... the wish you made last night. It came true. Max is amazed. MAX Really? You mean you have to tell the truth? FLETCHER Yes. MAX No matter what? FLETCHER No matter what. Max grins -- then suddenly asks, in rapid succession. MAX Is wrestling real? FLETCHER In the Olympics, yes. On . Channel 23, no. MAX Will sitting close to the TV set make me go blind? FLETCHER Not in a million years. MAX If I keep making this face-- (makes a horrible FACE) will it get stuck that way? FLETCHER Uh-uh. MAX .If I go in the water right after lunch, will I drown? FLETCHER Only if you can't 'swim. MAX Why do I have to eat squash? FLETCHER Because your mom buys it. MAX How come you're always too busy to play with me? The sudden shift in tone startles Fletcher. He feels awful. FLETCHER I . . . I don't know. I ' m . . . Hey, you know I'm coming over to nig ht. We're gonna play together.\",\n",
              "  'MAX Baseball? FLETCHER . Yes! This is absolutely an A-number one promise. You and I -- tonight -- baseball. Fletcher and Max do their ritual \"five\" slap. FLETCHER Now, listen, Max, I need a favor from you. I \\' m in a little trouble today. I need you to take that wish back. MAX So you can lie? FLETCHER Not to you. MAX To who? FLETCHER Max, sometimes grownups... need to lie. It\\'s hard to explain, but i f . . . Look, here\\'s an example. When Mommy was pregnant with you, she gained a little weight. Se venty pounds. I thought she was gonna give birth to a car. But she\\'d say to me \"How do I lo ok?\" So I\\'d say, \"Oh, honey, you\\'re beautiful, you\\'re glowing.11 Otherwise, I would\\'ve hurt Mommy\\'s fee ling s. Understand? Max nods. MAX You didn\\'t think she was beautiful. FLETCHER Right. No... Max, I don\\'t know how to get along in the grown-up world if I have to stick to the truth. I could lose my case, I could lose my promotion, I could even lose, my job... Do you understand? Max shakes his head \"no.\" FLETCHER (CONT\\'D) Will you help me anyway? A moment -- then Max reluctantly nods. FLETCHER (CONT\\'D) That\\'s my boy! Fletcher opens the box, revealing a cake and candles.... He takes out two birthday hats. He puts one on Max and one on himself. FLETCHER (CONT\\'D) Now, do whatever you did last night... only this time, make an un-wish. Not really happy, Max turns to the candles on the cake. He takes a breath-- and blows them out.',\n",
              "  \"MAX I did it. ^ FLETCHER Great! Great! Now to test -- � Fletcher spots an attractive FEMALE teacher. Fletcher hurries to her and says something. A moment. Then she SLAPS HIM. Fletcher returns to his son. MAX Did it work? FLETCHER . . (rubbing his sore CHEEK) Not like I ' d hoped. Did you really unwish it? Max nods. MAX Only... FLETCHER Only what? MAX Yesterday, when I wished it, I r ea lly meant it. This time when I unwished it I only did it 'cause you told me to. FLETCHER (losing patience) Well, then do it again. Only this time, mean it. MAX I can't. FLETCHER Why not?! MAX Because I don't want you to lie. FLETCHER I explained this to you! I have to lie. Everybody lies! Mommy lies, even the wonderful Jerry lies-- ' MAX But you're the only one who makes me feel bad. . ' . � � Fletcher is stunned by how much this hurts. MS. BERRY (CALLING) Max, recess 'is over, come on in. MAX\",\n",
              "  \"I have to go. FLETCHER I am coming over, tonight, Max. You believe me, don't you? Max hesitates, then nods. FLETCHER (CONT'D) I'll see you tonight, buddy... That's a promise. Max heads back to class. Fletcher picks up the cake, looks at it, then dumps it in a trash barrel. EXT. SKYSCRAPER - DAY A worried and preoccupied Fletcher is heading toward his office building when a MACHO ATTORNEY passes by. MACHO ATTORNEY Yo, Fletcher! How's it hanging? FLETCHER Short and shrivelled. Fletcher hurries up the steps when he spots Philip. He shields his face with his briefcase. Philip recognizes him anyway. PHILIP Fletcher! I'm still waiting for your call. I guess you must've lost my card -- FLETCHER No -- PHILIP Or my phone was busy -- FLETCHER No -- � PHILIP Or you just forgot -- FLETCHER No -- PHILIP (cannot be DISCOURAGED) Or something. So anyway,' why don't you swing by my place around seven-thirty! Philip starts off, when Fletcher calls after him resolutely. FLETCHER Philip... I don't want to come over to your house! A long moment, then -- PHILIP Fi n e ! We'll go out! There's this new karaoke bar I've been dying to try. I'll pick you up at your office! Seven-\",\n",
              "  \"thirty! ! And he runs off. Frustrated, Fletcher hurries on. INT. OFFICE'S - DAY Fletcher drags himself past Greta's desk. Miranda gives him the stink-eye. Fletcher doesn't see her. GRETA Do you want your messages? FLETCHER No. He goes into his office. Greta is concerned. She follows him in, leaving his door open. INT. FLETCHER'S OFFICE - CONTINUOUS Fletcher collapses onto his couch. Greta enters. GRETA Are you okay? FLETCHER My son hates me. GRETA No! He loves you. I've seen y ou together. You're his hero. FLETCHER O h yeah? Last night at his birthday party, he made a � wish. That I wouldn't be able to tell a lie for one whole day. GRETA Kids... FLETCHER It came true. GRETA What? FLETCHER It's true. Didn't it seem odd to you that I kept telling the truth all morning? GRETA Well, yeah, but... (INCREDULOUS) You're telling me that you can't lie. FLETCHER That's right! I am incapable of lying. INT. OUTER OFFICE Miranda is 'eavesdropping. A wicked gleam in her eye, INT. FLETCHER'S OFFICE\",\n",
              "  'GRETA Just today? FLETCHER Apparently until 9:15 tonight. It\\'s a twenty-four hour curse. GRETA Yes, those are going around. FLETCHER You don\\'t believe me. GRETA Of course not. FLETCHER Go ahead. Ask me something I \\' d normally lie about. She thinks. GRETA A ll right. Remember a few months ago, I wanted a raise-- FLETCHER (QUICKLY) Forget it. Let\\'s not do this. GRETA -- and the firm wouldn\\'t.give me one. And I asked you if you would give it to me out of your own pocket and you said the company wouldn\\'t permit it because it creates jealousy among the other secretaries? Was that true or did you just not want to pony up the dough? INT. OUTER OFFICE Greta is emptying all her personal effects into boxes. She\\'s leaving. Fletcher is on the phone and looks very harras,sed. FLETCHER Greta, please... (into phone) Yes Judge Stevens, hi!.. Fletcher Reid. I \\' m scheduled to be in your court in half- an-hour... Judge Stevens, I badly, badly need a continuance. . . so I can go home and stay there the rest of the day...111? Am I ill? He wants to say \"yes\", but he can\\'t. FLETCHER In a way. (covers the MOUTHPIECE) Please, lie to him for me. Greta holds up a framed photograph. GRETA I remember when you. bought me t hi s silver frame. From Tiffany\\'s.',\n",
              "  \"(QUESTIONING) . . . Tiffany's? FLETCHER Jumbo's House of Junk. She thrpws it in the trash and keeps packing. FLETCHER (CONT'D) I'll give you the raise! GRETA (gives him the FINGER) Here's your raise. FLETCHER (into phone) Hi, Judge Stevens?... Yes, I know I haven't given you a reason. The PHONE RINGS. FLETCHER (CONT'D) (into phone) But if you could just do this for me, I-- The phone won't stop ringing. . � ' FLETCHER (CONT'D) Hold on, please, (pushes two BUTTONS) Hello... Mom!! The phone flies into the air. He catches it. FLETCHER {CONT'D) Mom... Well, I wasn't actually on vacation... Because I didn't want to talk to you... Because you insist on talking to me about Dad's bowel movements -- size, color, frequency... I'll call you later... No, not really. He pushes -two more buttons. Then SCREAMS. FLETCHER (CONT'D) Oh dammit! I cut him off! I cut off the Judge! Greta... He falls to his knees. FLETCHER (CONT'D) I'm on my knees in a nine hundred dollar suit. Don't leave. Greta stops. She seems to consider. GRETA A few years ago a friend of mine had a burglar up on her roof. FLETCHER Yes? GRETA .. A burglar. -He fell through the kitchen skylight and landed on a cutting board on a butcher's knife, cutting his l e g. He sued my friend. The\",\n",
              "  \"burglar sued my friend. Thanks to guys like you-- he won. My friend had to pay him six thousand dollars. Is that justice? FLETCHER No. . . but what' s your poijit! GRETA My point is, it's hard to get justice. But this is justice, (pinches his CHEEK) Have a nice day in court, bubbie. She leaves. Fletcher starts to give chase... FLETCHER GRETA-- He runs directly into Miranda. FLETCHER Aaaah! Miranda smiles like a cat that's trapped a mouse. MIRANDA Ah, Fletcher, so nice to bump i nto you. Are you busy? FLETCHER Extremely. MIRANDA Good. Would you follow me, please? Highly nervous, Fletcher follows Miranda down the hall. MIRANDA (CONT'D) Fletcher, did you know that the partnership committee is being headed up by Mr. Allan himself? (off his wary NOD) Say, you used to work directly for Mr. Allan, didn't you? (off his waried NOD) Tell me, what do you think of him? FLETCHER (HELPLESS) He's a pedantic, pontificating, pretentious bastard, a belligerent old fart, a worthless, steaming pile of cow dung. MIRANDA (GRINNING) How delightful! She swings open a door, ushering Fletcher into -- INT. CONFERENCE ROOM - DAY The room is filled with ATTORNEYS, including MR. ALLAN,\",\n",
              "  \"the founder himself. Fletcher freezes. MIRANDA Pardon me for interrupting your, meeting. Mr. Allan, you remember Fletcher Reid. � MR. ALLAN � It's good to see you again, Fletcher. '. An involuntary WHIMPER from Fletcher. MIRANDA Oh, that's right. You used to work together. .Tell me, what do you think of Mr. Allah? Fl etc he r gulps. This is it. His career is history. He's trying to hold it back, but-- MIRANDA I said... What do you think of Mr. Allan? FLETCHER . He's a pedantic, pontificating, pretentious bastard, a belligerent old fart, a worthless, steaming pile of cow dung. DE AN SILENCE. Then --Mr. Allan bursts into raucous LAUGHTER. He is joined by everyone except Miranda, who looks on, STUNNED. Everyone pounds the table in hysterics. MR. ALLAN Marvelous! Marvelous! That's what I love most about this firm-- the collegial atmosphere, the hearty good- fellowship! Miranda is incensed. MR. ALLAN (CONT'D) And thanks for those flowers for my anniversary. My wife loved them. FLETCHER Well, I'm due in court... bye- bye. INT. HALLWAY - CONTINUOUS Fletcher exits the conference, closes the door, breathes a sigh of relief, then FAINTS. . INT. COURTROOM CLOSE UP of Fletcher, seated alone at a table. His hands are on his-face. He looks totally dazed. At the other table, sit Dana Appleton and Mr. Cole. BAILIFF All rise. They do. Judge Stevens enters. He sits. Everyone sits. JUDGE STEVENS Counselors, are we ready to begin?\",\n",
              "  \"FLETCHER (eagerly and a little too LOUDLY) N o sir! We are not ready to b eg in. My client has not arrived. The doors OPEN and Virginia Cole enters with her CHILDREN and a NANNY. FLETCHER � . -- until now. He collapses into his chair. (to Falk, with DETERMINATION) Did you and Mrs. Cole ever make lo-- forni-- roll in the h-- make the beast with two ba -- Did you two ever fu-- fu-- Fu! He begins to hyperventilate. Virginia turns to Falk. VIRGINIA Wate r! Get him water! Falk hurries into the building as Fletcher hacks on. FLETCHER Fu-- fu-- / VIRGINIA S it down! Get some air! (slaps him on the BACK) Try to relax! Breathe deeply! Falk hurries out with a cup, hands it to Fletcher, who downs it in one gulp -- then spews it out again, SCREAMING in PAIN. - VIRGINIA (CONT'D) W hat?! What?! FALK I couldn't find any water, so I got him coffee! Fletcher runs up and down the steps, frantically fanning h is scalded mouth. The bailiff appears. BAILIFF Judge is taking the bench. Fletcher's expression turns to terror. INT. COURTROOM - DAY The judge settles in. Mr. Allan and a smug Miranda look on from the gallery. JUDGE STEVENS You may proceed, Mr. Reid. . Everyone turns to Fletcher in anticipation. In a voice quaking with fear... VIRGINIA Sorry. One of the kids threw up in the car.\",\n",
              "  'Virginia takes her seat, leaving her two young children sitting dejectedly in the gallery with their nanny. FLETCHER (INCREDULOUS WHISPER) You brought your kids. . . to your divorce? VIRGINIA (by way of EXPLANATION) Sympathy. FLETCHER Well, it\\'s working. I feel sorry for them already. The judge BANGS the gavel. JUDGE STEVENS Ms. Appleton, you may begin. CUT TO: Dana Appleton questions BRYSON, a private investigator. Fletcher watches with mounting anxiety, NERVOUSLY DRINKS from a GLASS OF WATER at his table. BRYSON (referring to his NOTES) -- From March six through June twelve, I surveilled Mrs. Cole at the behest of Mr. Cole. During that period, I noted that Mr. Cole left each day between seven-forty and seven- fifty. Thereafter, Mrs. Cole would frequently have a male visitor arrive and stay for one to four hours. I was able to take several photographs of the male visitor. He shows a photo - - o f a strapping hunk. Fletcher TAKES A HUGE DRINK. . DANA I see. And do you .know what Mrs. Cole and her male visitor did during their frequent... visits? BRYSON Well, they were pretty good about keeping the shades drawn -- but I sure was able to hear. I made an audiotape of one such., \"session.\" H e hands her the tape. Fletcher refills his glass. DANA With the Court\\'s permission, I would like to play the tape. FLETCHER Your Honor, I object! JUDGE STEVENS And why is that, Mr. Reid? FLETCHER',\n",
              "  \"(can't help HIMSELF) Because it's devastating to my � case. The judge is startled by his candor. JUDGE STEVENS Overruled.. As Dana pops the tape into a player, Fletcher anxiously DOWNS THE GLASS. Periodically CUTTING to Virginia, Mr. Cole, Dan and the thirsty Fletcher, we hear Virginia and her visitor engaged in intense physical activity. MALE VISITOR (O.S.) So , what did you say? You ready? VIRGINIA (O.S.) Oh boy am I ready. MALE VISITOR (O.S.) Good. Let me help you off with that. Come on, lie down. VIRGINIA (O.S.) Wai t a minute. Do you have protection? MALE VISITOR ( O . S . ) Right here. Okay, now I ' m gonna show you something new. VIRGINIA ( O . S . ) Oh, I've never done it like this before. . � . MALE VISITOR (O.S.) Don't worry, you can take it. Oh yeah. That's it. There you go. Yes! Yes! WE HEAR labored rhythmic breathing. MALE VISITOR (O.S) (CONT'D) Yes, yes, yes -- As Dana fast-forwards again, then resumes... with still more labored breathing, building intensity and -- MALE VISITOR (O.S.) (CONT'D) Oh yeah, bring it on .home -- yes! Yes! Yes! VIRGINIA (O.S.) Yes! YES! YES! The groans reach their incredible climax. There's a still moment. . . As the shy COURT REPORTER, the macho BAILIFFS and the no- nonsense judge all mop their brows, Dana shuts off the tape. She turns to Fletcher with a satisfied smile. DANA Your witness. FLETCHER No questions. JUDGE STEVENS No questions? . VIRGINIA No questions? FLETCHER\",\n",
              "  \"(afraid to ask ANY) No questions. DANA (TRIUMPHANT) Petitioner rests. JUDGE STEVENS All right, Mr. Reid. You may proceed. FLETCHER (to himself) How?! Gathering his courage, he stands, downs the last of his water, and moves to the lecturn. He's about to speak... when a WONDERFUL FEELING sweeps through him. After a momement, he grins. FLETCHER (CONT'D) Would the Court be willing to grant me a short bathroom break? JUDGE STEVENS It can't wait? FLETCHER Not unless you want to mop up. JUDGE STEVENS (FRUSTRATED) All right, but get back in here immediately so .we can finish this. Fletcher beams. Then necessity compels him to race out. INT. REST ROOM - DAY Fletcher stands before the urinal, taking the longest leak i n legal history. Relief. Then, he looks at his watch. It's only 4:15. ' FLETCHER What did I think? That I could piss for forty-five minutes?! He HITS HIS FOREHEAD in frustration... and gets an idea. He HITS HIMSELF AGAIN and AGAIN, SMASHES HIS HEAD INTO THE WALL, POKES HIMSELF IN THE EYES, YANKS ON HIS EARS, finally KNOCKS HIMSELF IN THE STALL, where he continues his attack. A MAN enters, hears a commotion from behind the stall door. MAN What's going on in-there? FLETCHER (O . S . ) I ' m abusing myself! Do you mind?! The man looks disgusted. He carefully leaves the room. INT. COURTROOM - DAY The judge is PISSED. Suddenly the bailiff helps in the severly beaten Fletcher. The entire courtroom is SHOCKED. BAILIFF I found him like this in the bathroom. Somebody beat the\",\n",
              "  \"hell out of him. JUDGE STEVENS Who did this? FLETCHER (TRUTHFULLY) A madman, Your Honor.. A desperate fool at the end of his pitiful.rope. JUDGE STEVENS -What did he look like? FLETCHER (DESCRIBING HIMSELF) About five eleven, hundred eighty-five pounds, crazed look in his eye. JUDGE STEVENS Bailiff, have the deputies search the building. A HUBBUB rises. He bangs the gavel. JUDGE STEVENS (CONT'D) Under the circumstances, I have no choice but to recess this case until tomorrow morning at nine. Fletcher smiles serenly -- until -- JUDGE STEVENS (CONT'D) -- Unless, of course, you think you can still proceed? Fletcher covers his mouth in a desperate attempt to avoid answering, but he can't repress the truth. JUDGE STEVENS (CONT'D) Can you? FLETCHER Yes, I can. JUDGE STEVENS Splendid. I admire your courage, Mr. Reid. I'll give you a few minutes to compose yourself, and then we'll get started. Fletcher looks as if he has just been sentenced to death. EXT. COURTHOUSE STEPS - DAY Fletcher sits on the courthouse steps, miserable. PHONE RINGS. FLETCHER Hello. INTERCUT WITH MAX AT HOME. AUDREY IS THERE. MAX Dad... FLETCHER (summoning up ENTHUSIASM) Maxi-pad. How's it going?\",\n",
              "  \"MAX Great. You know Paul and Emanuel from across the street? FLETCHER The twins. MAX . (EXCITEDLY) Well, they never want to play baseball with me, but I told them I was gonna play tonight with my Dad, so now they want to play with us. Is it okay? FLETCHER Sure. MAX Oh boy. We're setting up a whole field in the yard. Where we buried Petey the hamster is second base. (Fletcher sighs) You're still coming right?' FLETCHER (sees Virginia APPROACHING) I'11 be there. I gotta go now, Max. I'll see you in two hours. Max hangs up. MAX (to Audrey) He's really coming. She smiles, but she's worried. . COURTHOUSE STEPS � Virginia approaches with her handsome lover, LAURENCE FALK. VIRGINIA Mr. Reid, you remember Laurence Falk, the man from the tape. FALK How are you? FLETCHER I've slipped into the seventh circle of Hell, thank you, and you? Virginia exchanges an anxious look with Falk. VIRGINIA Shouldn't we go over our testimony? FLETCHER Well, basically the plan is I walk you through the tape step by step, I ask you questions-- VIRGINIA And we give the explanation\",\n",
              "  'you came up with. FLETCHER Exactly. FALK So all we have to do is lie. Sounds simple enough. FLETCHER Do esn\\'t it? And I\\'ll finish up with a dramatic series of questions, something like... \"Mr. Falk, isn\\'t it true that you and Mrs. Cole have never made lo--\" But Fletcher GAGS. He CAN\\'T GET THE QUESTION OUT. The others look concerned, but he waves them off. FLETHCER (CONT\\'D) Sorry. I\\'m fine. \"Mr. Falk, isn\\'t it true that you and Mrs. Cole have never made lo-- IO-.H To his horror, he GAGS AGAIN, .unable to form the word. FLETCHER (to himself) Oh my God! I can\\'t do it! I can\\'t finish the question if I know the answer is a lie! At this moment Miranda and Mr. Allan come up the steps. MR. ALLAN Don\\'t let me interrupt, Fletcher. I just want you to know I\\'ll be observing this a fternoon. Miranda insisted I see you in action. Fletcher shoots a hateful look at Miranda. She smiles. MR. ALLAN (CONT\\'D) I\\'m looking forward to it. Go get \\'em! Mr. Allan and Miranda head into the building, leaving Fletcher more desperate than before. FLETCHER C\\'mon! Gotta rephrase the question! FLETCHER Respondent calls... Lawrence Falk. Fletcher\\'s clears his throat. Here goes... FLETCHER Mr. Falk, do you know my client, Virginia Cole? FALK Yes. FLETCHER Isn\\'t it true that your relationship with my client is',\n",
              "  'entirely platonic, not? The \"not\" was INVOLUNTARY. It takes everyone by surprise. FALK Excuse me? FLETCHER If I might rephrase your Honor. (trying again) Is your relationship with my client entirely patonic, not? X\\\\ Is your relationship with my client not entirely platonic? Is not your relationship with my client entirely platonic? (thinks he\\'s got it, beams with CONFIDENCE) Mr. Falk, is not your relationship with my client entirely platonic? FALK (CONFUSED) No. I mean, yes. I think. FLETCHER Yes, is your relationship with my client not entirely platonic, or yes, is not your relationship with my client entirely platonic? FALK What? FLETCHER How \\'bout just answering the question you think I\\'m asking? \\' DANA Your Honor, he\\'s badgering the wintness! JUDGE STEVENS It\\'s hig witness! FLETCHER Did you ever not make lo-- Did you not ever make lo-- (losing it) YOU HAD SEX WITH HER EVERYTIME YOU MET, DIDN\\'T YOU? DIDN\\'T YOU?!! Falk looks shaken as Fletcher barrels on, unable to stop FLETCHER (screaming at HIM) ADMIT IT! YOU .SLAMMED HER!! YOU STOKED THE FUR FIRE! YOU -DID THE YAM DANCE! !',\n",
              "  \"FALK (breaking down) YES, YES,-- IT'S TRUE! I HUMPED HER. BRAINS OUT! ! A GASP from the audience. All eyes are on Fletcher. FLETCHER (WEAKLY) No further questions. DANA Uh...no questions. JUDGE STEVENS (to Fletcher) Call your next witness. FLETCHER I have no further witnesses, your Honor. A MURMUR erupts from the crowd. JUDGE STEVENS You have no further witnesses?! Fletcher meekly shakes his head, no. VIRGINIA (whispers, to FLETCHER) What are you doing? Call me. FLETCHER (to Virginia) I can't. JUDGE STEVENS Mr. Reid? VIRGINIA Call me, damn it! FLETCHER You don't understand. I can't lie . Until nine-sixteen tonight, I can't even.ask a question that calls for a lie! Virginia GRABS HIM BY THE TIE, pulls him CLOSE to her face. VIRG INIA L is ten , you bastard. I want m y money. I am not gonna wind up a 31 year old divorce on welfare because my scum bag attorney had a sudden attack of conscience! Fletcher suddenly stops -- focused on something Virginia said. FLETCHER (to himself) Thirty-one? JUDGE STEVENS Mr. Reid, we're not getting any younger... Fletcher quickly looks at the blowup of Virginia's prenup and her passport. JUDGE STEVENS (he's had it)\",\n",
              "  \"Mr. Reid you have presented virtually nothing in the way of evidence and as such I have no choice but to rule in favor of -- FLETCHER WAIT! Silence. FLETCHER � . (DRAMATICALLY) Your Honor, I call Virginia Cole to the stand. Stunned, Virginia nervously makes her way up, MR. ALLAN (in the gallery) What the hell is he doing? MIRANDA Kissing his career goodbye. The Baliff stands before the witness. BALIFP Do you swear to tell the truth, the whole truth and nothing but the truth, so help -you God? VIRGINIA I do. Fletcher approaches,. CONFIDENT NOW, COCKY. FLETCHER Mrs. Cole -- may I call you Virginia? VIRGINIA Yes. FLETCHER But that would be a lie, wouldn't it? VIRGINIA What do you mean? FLETCHER Isn't your true name... (BRANDISHING PASSPORT) Carlotta?! VIRGINIA Well, yes. But it wasn't me so I started using Virginia. Is there anything wrong with that? FLETCHER � . Not really. It's just the first and smallest in the tissue of lies that is the Kleenex of your life. Let's take one simple document as a sample of your veracity, shall\",\n",
              "  'we, Carlotta? He grabs her purse from the desk, rifles through it, F LETCHER Your driver\\'s license. What color are your eyes? VIRGINIA Blue. FLETCHER Tru e blue? What if I asked you to remove your contact l enses? What color would they be then? VIRGINIA (RELUCTANTLY) Brown. FLETCHER And here it says you\\'re a b l on de . Are you? (off her silence) C\\'mon, Carlotta, there\\'s a very easy way for us to check. If you don\\'t remember, perhaps Mr. Falk will. VIRGINIA Brunette. FLETCH ER More like a dirty brown, isn\\'t it? (she nods) Let\\'s see - . - \" Weight: one-o- five\"? Please... VIRGINIA . One-eighteen. (off his look) One-twenty-six. I swear! FLETCHER So on this single document, you basically lied at every opportunity. I\\'m sure a woman as vain as you would also lie about her age. It says you were born in 1964. What\\'s the truth? 1962? \\'60? How young did you try to make yourself? VIRGINIA (JOYFULLY) Wrong! I didn\\'t lie to make myself younger. I made myself older. I was born in 1965!\" FLETCHER (FEIGNING SURPRISE) What? You\\'re trying to tell us you lied to make yourself older? VIRGINIA Yes! \" lied so I could get married! So .there Mister \\'I',\n",
              "  \"GOT-ALL-THE-ANSWERS-BECAUSE-I- went-to-law-school'! JUDGE STEVENS Mr. Reid, does this have a point? FLETCHER Oh, you bet it does, your Honor! (on a roll) My client lied about her age because she was only 17 when s he got married. Which makes h er a minor. And in the great state of California, NO MINOR CAN ENTER INTO A LEGAL CONTRACT WITHOUT PARENTAL CONSENT INCLUDING-- DANA (defeated, to HERSELF) Prenuptual agreements. FLETCHER (knows he has THEM) PRENUPTUAL AGREEMENTS! THANK YOU VERY LITTLE! This contract is void!!! The fact that my client gets nailed � more often than a two-by-four is irrelevant. Standard community property applies and this woman is entitled to half of the marital assets or thirty-seven point three-nine- five million dollars!! (to Dana) Yo . . . . a e . . . . T A T T T ! ! u r OSTT! (DRAMATICALLY) Nothing further, your Honor! A MURMUR OVERTAKES THE ROOM! / JUDGE STEVENS (banging his GAVEL) Q uie t! Let me see-the license and birth certificate. All is quiet while the Judge reviews the documents. Then: JUDGE STEVENS In light of this new evidence, the court must rule in favor o f the defense. Mrs. Cole is hereby awarded half of the marital assets -or thirtyrseven million three hundred and ninety-five thousand dollars. The courtroom ERUPTS. FLETCHER'S WON! Dana, Mr. Cole are devastated. '\",\n",
              "  \"MR. ALLAN That son of bitch pulled it off! Mr. Allan gives Fletcher a thumbs-up; simultaneously, Miranda gives him the finger. JUDGE STEVENS O r d e r ! Order!! Now i understand both parties have agreed to joint custody. Is that correct? FLETCHER AND DANA YES-- VIRGINIA No! I'm contesting custody. Fletcher freezes. FLETCHER What? VIRGINIA (re: her husband) Payback. For him trying to prevent me from collecting my thirty-seven million. FLETCHER He was entitled to prevent you. You committed adultery.' You only won because you're a liar, remember? . VIRGINIA No. You pointed out that my husband took advantage of a poor underage girl. I was the vic tim here. And now I'm going to hit him where it hurts. FLETCHER But -- but -- you said he was a good father. JUDGE STEVENS M r. Reid? Do we have an agreement on custody or not? Fletcher takes a distressed look at the children. FLETCHER No. . JUDGE STEVENS In that case, there will be a custody hearing tomorrow mor ning at nine. Court is adj ourned! He BANGS THE GAVEL. Everyone gets up, but Fletcher's attention is drawn to a commotion between Virginia and her kids. VIRGINIA Stop that! We're leaving now! CHILD I want to go with Daddy. V,'\",\n",
              "  \",- .- Fletcher watches, horrified, as she drags the kids away from their tearful father. MR. COLE Don't worry. I'll see you no matter what. I promise. Mr. Allan has made his way up to Fletcher. MR. ALLAN (re: the COMMOTION) I love kids. They give you so much leverage in a case like this. (pats Fletcher on BACK) Congratulations, partner. how does it feel? And with that question asked, as he watches poor Mr. Cole and his kids, the truth dawns on Fletcher like a sledgehammer! FLETCHER ' Excuse me. Just a second. (to the Judge) Y our Honor? Your Honor? Wait! JUDGE STEVENS We',re adjourned, Mr. Reid. FLETCHER Screw that!! She lies and she wins ?! What are we, nuts? Everyone stops, watches Fletcher. FLETCHER (CONT'D) T his woman --my client -- goes down with the frequency of a nuclear submarine and we just gave her thirty seven million dollars because she's a liar! And now as an extra added little bonus, we're going to let her steal, the kids, too? JUDGE STEVENS Mr. Reid, you are out of order! FLETCHER (SCREAMING) � SO'S THE HAND DRYER IN THE MEN'S ROOM!! Do you ever stop to ask yourself, why do people hate us? Could it be because what we did here today sucks?! We don't care about the truth! We don't want to find the t r u th ! We want to win! We want to win at all costs...and you know what the worst thing about wanting to win so badly i s ? WINNING! Winning and finding out you're left with nothing!\",\n",
              "  \"JUDGE STEVENS That's enough, Mr. Reid -- FLETCHER -Let' s see what I' ve done today. I've helped a gold digging slut get richer. I'm taking this guy's kids away. (to Mr. Allan) I don't like you in the least, now I'm one of your partners! YOU WANNA KNOW WHAT IT FEELS LIKE MR. ALLAN? IT FEELS LIKE SHIT! BUT TO TELL YOU IT FEELS LIKE SHIT, FEELS FUCKING GREAT 1 I Fletcher does feel strangely fantastic. Free, JUDGE STEVENS That's it, Mr..Reid. I find you in contempt! FLETCHER GOOD! I'M CONTEMPTIBLE! MY WHOLE GODDAMN LIFE IS JUST ONE BIG FAT FIB! YOU LIKE MY HAIR? -- (mussing hair) MOUSSED! SHOULDERS -- (ripping out PADS) PADDED! SHOES -- (kicking them OFF) LIFTED! TEETH -- (pulling out CAPS) CAP PED! FIV E-NIN ETY A CHICKLET!! COMMOTION in the court. The judge BANGS HIS GAVEL!!! JUDGE STEVENS Bailiff! Remove Mr. Reid from the courtroom! FLETCHER Yo u wanna know the truth? Oh yeah, let's let it rain... The truth is is that I've traded my life...a beautiful wife, an incredible son for THIS PISS POT OF BIG DOUBLE O'S!\",\n",
              "  \"The bailiff grabs Fletcher, forces him out... FLETCHER GO AHEAD, YOUR HONOR, BANG YOUR GAVEL .-- KEEP TELLING YOURSELF YOU'RE A BIG SHOT! DO I SENSE A CASE OF GAVEL ENVY!! WHAT'S THAT UNDER YOUR ROBE -- INSUFFICIENT EVIDENCE?!! (the judge is turning beet RED) . I TOUCHED A NERVE DIDN'T I? WE'RE ALL A BUNCH OF BULLSHIT ARTISTS!! IS THAT THE TRUTH IN YOUR PANTS OR ARE YOU JUST HAPPY TO SEE ME?? Fletcher is pushed passed Mr. Allan. MR. ALLAN You just killed your career. I hope you're happy. FLETCHER I'M BEYOND HAPPY MY BUTT FACED FRIEND--- I'M EUPHORIC! EXT. AUDREY'S PORCH -'DAY A sad Max is seated on the steps. TWO other BOYS are there with baseball equipment. PAUL We're going home. EMMANUEL Yeah, thanks for the great game, Max. Emanuel knocks Max's hat off. Audrey's been watching from the door. She goes and sits by her son. AUDREY Max, honey. Your dad had a very big case today. It probably just-- MAX I don't want to talk about it. AUDREY Okay. MAX (SUDDENLY) I hate dad! I hate him! AUDREY Honey, don^t say that.\",\n",
              "  'Max is really upset. It\\'s \"that look\" and then some. The look Audrey never wanted to see again. She makes a decision. AUDREY Max, there\\'s something I-want to talk to you about. . . INT. JAIL AREA Fletcher\\'s handcuffed and is led to jail by TWO OFFICERS-. There\\'s a happy/crazedness to him now. The truth is pouring forth, but he looks way, way off the deep end. (desperately, p as si ng a phone) Pho ne call!! Phone call!! I g et to make a phone call!! INT. AUDREY\\'S KITCHEN -- DAY Max and Audrey at the table. The airline tickets Jerry gave her are in front of them. MAX When would we move? AUDREY Soon. My semester\\'s almost over. You only have a week left of school... You like Jerry don\\'t you? (he nods) So what do you say, should we check it out? Jerry wants us to come with him tonight. He has to pick out a place to live and he really wants our help? MAX Could I get a sled for when it snows? AUDREY Of course you can. Max thinks, then: MAX Okay. INT. JAIL F l e t c h e r \\' s holding a phone. He\\'s frantic, now. F L E TC H E R ( re: ringing PHONE) Answer! Answer!! Answer!! ! % The phone RINGS, Audrey answers it. AUDREY Hello. . INTERCUT FLETCHER/AUDREY FLETCHER',\n",
              "  'Audrey! It\\'s Fletcher-- AUDREY (PISSED) I can\\'t talk now, Fletcher. We have to pack. FLETCHER Wait, the most amazing thing\\'s happened to me! I am feeling so good... (REALIZING) Pack?! Did you say pack?! AUDREY Max was sitting on the porch again, waiting for his dad. I won\\'t let you do this to him anymore. I won\\'t let you do this to me. FLETCHER A ud rey , wait. Please, I need to talk to you. I .swear, I\\'m a changed man. Just come to the courthouse with a thousand dollars and bail me out... Hello? (to a cop) O ne more call!! I need another call!! INT. JAIL CELL - DAY Fletcher is pacing back and forth. A GROUP OF TOUGH PRISONERS are on the far side of the cell, trying to stay as far away from Fletcher as they can. FLETCHER And what about our water su pply ? You don\\'t think \"the man\\'s\" dumped enough toxins to render every dick in this cell as lifeless as a beached minn ow? You\\'re damn rightJ \"The man\" does anything he w an ts. We\\'re nothing but pu ppets... Little game pieces they move back and forth. A DEPUTY appears. \\' DEPUTY Mr. Reid. ^ FLETCHER That\\'s me. Fletcher T. Reid. Pawn no. 332-154-9867. DEPUTY You made bail. Some woman. INT. OUTER AREA Fletcher rushes in. FLETCHER Audrey? (he spots) Greta?!',\n",
              "  'GRETA Am I too late? Have you been se xu al ly molested yet? I could circle the block. FLETCHER Greta! Greta!! . . . . Look at you, you well preserved, underpaid, overworked, underappreciated thing you. G iv e me a hug! You came and got me out!! Hug me!! GRETA (totally wierded OUT) Yes, well, I heard you went all noble in front of Mr. Allan so-- FLETCHER Y ou know what?! I love you. I loveyouloveyouloveyou. I wa nt to hug you. Come here.., GRETA Mr. Reid, what has gotten intc to you?! FLETCHER Just the truth, Greta. Fifteen years of being stuck in a lie is nowhere near as powerful as one day of being stuck in the truth. (checks his WATCH) Oh, my God!! I have to go! Thanks again, Greta! (as he runs off he calls back to HER) By the way, the truth is that I need you and I couldn\\'t file a paperclip without you! Greta smiles, then catches herself, and quickly regains her \"composure\". CUT TO: EXT.STREETS/INT. BMW Fletcher\\'s driving like a madman... FLETCHER (on his phone) Answeransweransweranswer... We HEAR a RECORDED VOICE: � VOICE The subscriber you called is either unavailable or outside the calling area. FLE TCHER Shit!!',\n",
              "  \"INT. LAX UNITED TERMINAL - DAY A udrey and Max meet Jerry by the ticket counter. Max is wearing the Dodger cap his dad gave him. Jerry surprises him with a Boston Red Sox hat. JERRY A little going away present. I was gonna get you a bowl of clam chowder but they only had Manhattan. A UDREY Say thank you, Max. MAX Thanks. � . Max takes off the hat his dad gave him and replaces it with the Boston hat. INT. BMW - DAY Fletcher's on the phone. He sails passed a parked POLICE CAR. FLETCHER (into phone) Shelton, Jerry Shelton. What time's that flight leave? 7:50. Thank you. (checks his WATCH) Oh, shit! Shit!! Shit! Fletcher spots the FLASHING LIGHTS. FLETCHER Shiiiiit!!! He pulls over -- so quick he jumps the curb. POLICE OFFICER Would you step out of the car, please? Fletcher obeys. FLETCHER Listen; I know I'm driving a little crazy but i have an emergency to attend to... The cop's just getting off his walkie talkie. POLICE OFFICER I'm impounding this vehicle. FLETCHER W h y ? What for? For changing lanes? POLICE OFFICER I just ran your tags through the computer. You've got. seventeen unpaid parking tickets. FLETCHER No ! I paid them! This morning! That's the truth! I swear!! POLICE OFFICER\",\n",
              "  'Not according to the computer. FLETCHER The computer is wrong! It \\' hasn\\'t been updated. The computer\\'s a liar! POLICE OFFICER You can straighten it out at the impound yard. FLETCHER (checks his watch, firmly) NO! POLICE OFFICER No? FLETCHER That\\'s right, no! I\\'m not gonna lose my son because some stupid clerk was too lazy to update the computer. (getting cockier. as he goes) Now if you want to follow me, . you can follow me and take the car after I get where I\\'m going. I\\'m a lawyer and I k now my rights! Understand?! CUT TO: A TOW TRUCK drives away with Fletcher\\'s car, leaving Fletcher stranded. EXT. STREETS - DAY \\' Fletcher frantically tries to hail a... FLE TCHER T axi! Taxi!! No luck. He spots A PAYPHONE digs through the Yellow Pages. Finds \"Ten Minute Taxi\". Yes ! He fishes for change. Shit! He doesn\\'t have any!! FLETCHER (looking . HEAVENWARD) Noooo!!! He spots a man walking by. FLETCHER \\'Scuse me, sir. Do you have any - - The man turns. It\\'s the same BEGGAR Fletcher was rude to outside the courthouse. BEGGAR Change? Absolutely. He continues walking. FLETCHER Could you spare some? BEGGAR Unquestionably.',\n",
              "  \"The beggar continues on. ' FLETCHER Alright, I get your point. But this is a crisis! Look, I'll give you ten bucks. The beggar pulls out a quarter and holds it up. BEGGAR (ADMIRING QUARTER) It's so shiny and new. FLETCHER Tw ent y. . BEGGAR Minted in Denver. Imagine that. FLETCHER Thirty-four. That's all I have. A moment as the beggar thinks, then: BEGGAR It's worth twice that to screw you. He walks off, grinning. FLETCHER JERKOFF! BEGGAR LAWYER! Fletcher turns, spots a familiar building in the distance. FLE TCH ER M y office!! INT. LOBBY FLETCHER'S OFFICE BUILDING - DAY He starts in the front door, when a SECURITY GUARD stops him. SECURITY GUARD Whoa, where do you think you're going? FLETCHER I just need to use the phone to call a cab. I work here. MR. ALLAN (O.S.) Used to work here. Mr. Allan has just exited the elevator. ... � MR. ALLAN (to security GUARD) Son, that man is tresspassing. The guard starts toward Fletcher threateningly. FLETCHER Hold it! (to Mr. Allan)\",\n",
              "  \"I've got ten years worth of dirt on you and this firm, and I'm in the kind of mood today to get a lot off my chest. You let me use the phone or I start talking!! CUT TO: Fletcher's is THROWN ON HIS ASS in the street. Mr. Allan has watched from atop the stairs of the building. MR. ALLAN Still euphoric, Reid? He goes back inside. � Fletcher starts to get up when a CAR SCREECHES to a HALT, inches away. MAN'S VOICE ( O . S . ) Fletcher! � It's PHILIP. PHILIP Seven-thirty... It's Karaoke time! Fletcher runs up and HUGS the astonished man, FLETCHER PHILIP!! LOOK AT YOU!!! MY PHILIP!! Fletcher KISSES HIM ON THE LIPS. INT. PHILIP'S CAR - DAY Philip's driving Fletcher. FLETCHER You're saving my life, Philip. PHILIP You know, it's funny, but for some reason I was beginning to think you didn't like me. Isn't that silly? FLETCHER . No. It's not silly. I don't like you. PHILIP What? FLETCHER I don't like you. I'm sorry. I find you boring. I hate ch ara des . And you wouldn't know a good time if it sat on your face. (feels bad) I'm sorry. It was easier than telling you how I really felt. Are you upset? A moment, then: PHILIP No. To be honest, I don't like you either. You treat people like obstacles and you cheat at charades. . FLETCHER\",\n",
              "  'Then why are you always trying to socialize with me? PHILIP You\\'re a client. I figured if I didn\\'t try to be your friend, you\\'d get a new accountant. FLETCHER Philip, I don\\'t like you as a person, but I\\'m crazy about you as my accountant. I \\' d never hire a new accountant. Never! PHILIP So we don\\'t have to like each other anymore? FLETCHER - Not at all. . PHILIP All right. Sooner I get you � to the airport, sooner I can dump your sorry ass off. EXT. AIRPORT -- DAY Philip\\'s car skids to a stop. Fletcher jumps out. INT. LAX TERMINAL - DAY . Fletcher races in. FLETCHER Bedelayed. Bedelayed. Fog, rain, something, anything... He sees the DEPARTURE BOARD \" F li g ht 69. Departs 7:50. On Time. Gate 17.\" Fletcher looks at the clock -- It\\'s 7:46!! Holy Shit!! INT. LAX ESCALATOR Fletcher pushes his way HE a crowded escalator. Past people standing on the left despite the SIGN that says STAND ON RIGHT. FLETCHER Excuse me. . . excuse me. . . Come on folks, let\\'s let the frantic man pass... Sorry... Thank... you... Standing on the right, passing on the left. They can\\'t make this- deal any easier than it is... Come on... coming through... At the top,- a WOMAN in a NURSES UNIFORM asks for money... WOMAN Help the poor?... FLETCHER (speeding past) I don\\'t trust you. I don\\'t know what the hell that uniform is. Sorry. (a Hare Krishna tries to stop HIM)',\n",
              "  'NOT NOW, TOGA BOY! INT. LAX - SECURITY AREA Fortunately, there\\'s no line at the metal detector. Fletcher races right by but SETS OFF THE ALARM. INSPECTOR Please step through again. FLETCHER Ahhh!! ! Damn..; Fletcher frantically tosses his keys, cufflinks, his Rolex into a tray. He tries again. It BUZZES again! FLETCHER What? I\\'tii practically naked! A guy in a TURBAN passes over him with a DETECTOR WAND. FLETCHER It\\'s called a ZIPPER, Hodgy... The wand BEEPS over Fletchers front pocket. He reaches in and pulls out the now familiar BLUE PEN... INT. LAX - DEPARTURE CONCOURSE Fletcher races by Gate 15, 16, gets to 1 7 . . . but sees the PLANE Slowly TAXIING AWAY. FL E TC H ER N ooo!!! Fletcher spots a door marked \"NOT AN EXIT\". Goes for it when a FLIGHT ATTENDANT interrupts. FLIGHT ATTENDANT Can I help you? FLETCHER Look out!! ! -- (TRUTHFUL) -NOTHING\\'S COMING!! The woman raises her eyebrows and looks anyway. And Fletcher BOLTS THROUGH THE EXIT! EXT. TARMAC - DAY He scurries down a flight of stairs calling after the plane which is moving away. No way he\\'ll catch it. Then, he sees a MECHANIC working on a MOBILE STAIRS UNIT (These are the steps they pull up to planes) Fletcher gets an insane idea. . . The worker hears an ENGINE START, looks up to SEE FLETCHER in the truck, driving off, TOWING THE STAIRS. WORKER Hey!! Hey!!!! But Fletcher\\'s gone. EXT. AIRPLANE - DAY Fletcher\\'s DRIVING THE STAIRS trying to catch up with the plane. GROUND WORKERS react.',\n",
              "  'Soon, the \"stairs\" are racing alongside the plane. Fletcher looks for signs of Audrey and Max but he\\'s too low to see in the plane. He grabs the TOOL BOX\\'on the passenger\\'s seat,-puts it on the accelerator, pinning it to the floor. Then, he CLIMBS THE STEPS! The \"stairs\" sway back and forth as he reaches the top. INT. AIRPLANE - DAY - MOVING PASSENGERS calmly read while outside FLETCHER speeds along, WAVING HIS ARMS like a maniac. The ENGINE NOISE drowns out his call for... F LET CH ER MA X ? !! AUDREYY?!! A STEWARDESS stands in the aisle, giving the safety lecture. STEWARDESS In case of a water landing, please use your seat cushion AS-- � Her MOUTH DROPS as she notices Fletcher. EXT. TARMAC - DAY Fletcher is BANGING on the windows. People on the plane POINT, STARE in amazement. Fletcher looks ahead, SEES the stairs about to CRASH INTO THE WING! Fletcher desperately fiddles with some controls. At the last second, finds the one that LOWERS THE STAIRS. He surfs under the wing... ... and RAISES UP THE STAIRS at the other side. Fletcher\\'s at the front of the plane, where he finally spots. . . MAX, AUDREY AND JERRY SEATED IN THE BULKHEAD Max has the window seat, Audrey arid Jerry are next to him. Audrey has on her headset and Jerry is looking for his seatbelt. NEITHER SEES FLETCHER. Fletcher SCREAMS to get their attention. But it\\'s TOO NOISY. Then, Fletcher looks ahead and his EYES GO WIDE! FLETCHER\\'S POV The RUNWAY is ENDING!. Just then, Max looks up...SEES HIS DAD. Audrey is now trying to help Jerry find his seat belt. AUDREY (checks under his SEAT) It\\'s right here, honey. M AX Mom! Mom!! \\' AUDREY Just a second, Max.',\n",
              "  \"MAX Mom, it's dad! AU DREY W hat? What about dad? Audrey turns. Then she sees Fletcher WAVING weakly... AUDREY Fletcher?! AT THAT INSTANT -- THE PLANE MAKES A SHARP TURN! BUT THE STAIRS DON'T! They keeps going straight, heading � right for the END OF THE RUNWAY and a parked LOADED LUGGAGE CART. . - And BAM! FLETCHER, THE STAIRS, THE LUGGAGE ALL GO FLYING! Audrey strains to watch as FLETCHER lands hard ONTO A MOUNTAIN OF BAGGAGE! CLOSE ON FLETCHER With all the strength he has he lifts his head, sees he's in one piece, and then COLLAPSES IN DEFEAT. CUT TO: INT. JAIL CELL - NIGHT Fletcher's BANGED UP pretty good. His head is BANDAGED. He.puts a COLD COMPRESS to his BRUISED FOREHEAD and WINCES. FLETCHER (mumbles to HIMSELF) Oh boy, the truth hurts. Yes indeed. DEPUTY Mr. Reid. Someone made bail for you. EXT. POLICE STATION - NIGHT Fletcher comes out LIMPING, totally dishevelled, missing a shoe, and still holding the compress. FLETCHER (WEAKLY) Greta? Is that you? He looks up and is surprised to see AUDREY and JERRY waiting for him just outside the door. Max is sitting at the bottom of the stairs, still ANGRY. He sees his dad, then quickly turns away. FLETCHER (to Audrey and Jerry, trying to seem chipper) Sorry I made you miss your flight, not really. (no response) You're obviously a little . upset, not that I blame you... although I'll bet you'll still\",\n",
              "  \"get the bonus miles. . . AUDREY Fletcher, are you crazy? What were you doing? FLETC HER That's two questions. A; Yes, but I think the legal term is temporarily insane. And B; I was trying to finally have that talk with you about Boston. Audrey's patience are growing thin... FLETCHER Okay, okay... The whole truth and nothing but the truth, (with difficulty, SINCERELY) I tried to stop the plane because it was taking off with my life... you and Max. This comes as a surprise to Audrey. Not just what Fletcher said, but the way he said it. FLETCHER I know you've met somebody... somebody pretty great... and the truth is I wish you didn't but you did and... All I ' m asking i s . . . Please don't move to Boston. Please don't take Max away. She's definitely moved by Fletcher, but not convinced. AUDREY You can come visit anytime. It's only a four hour flight. FLETCHER I don't want to visit him. That's what I've been doing-- visiting him, dropping by, stopping in. I want to be in his life. I don't want to be some jerk that sees him at Easter. I want to be his father. Fletcher turns to Jerry. FLETCHER I know I have no right to ask, but can I talk you out of taki ng that job? I can get you a better job here in L.A. I've got all kinds of connect ions... what do you do again? JERRY I design security systems. FLETCHER How symbolic. Okay great. You know Pac-Tec? JERRY The biggest. FLETCHER One of their systems shorted out and burned down a supermarket. I got them off.\",\n",
              "  \"Another proud day for justice. If I ask them they'll beat your Boston offer in two s ec on ds . .. AUDREY Don't put Jerry in the middle. JERRY It's okay. (to Fletcher) Boston means this (snaps his FINGERS) to me. All I want is for this lady and Max to be happy. Preferably, with me. Whatever they want, I'll go along with. They both look to Audrey. AUDREY All I want is for Max to be happy. Audrey looks over to Max seated at the bottom of the stairs. He's still upset. AUDREY You better know your jury. You're hot exactly Max's hero today. FLETCHER Just let me present my case. Fletcher walks over,, tries to be playful, starts WALKING, TALKING LIKE THE TERMINATOR. FLETCHER/TERMINATOR I have been sent from the future to destroy you. . . Argghhh! (no response, a BEAT) You mad at me? Max nods. Fletcher's at a loss for how to begin. Then: FLETCHER You wanted me to stop lying. But lying isn't the problem. .. Why we lie ~ that's the problem. Sometimes we lie to make someone else feel better. But sometimes we lie because the truth gets in our way... (touches him) But being an adult means you sacrifice some things for more important things. Much more important things. I was so stupid, Max. (pointing to his own head) Malfunction in vector one. All this time you've been here and I could see you anytime I felt like it. And I... didn't. Please don't go to Boston. Max, I love you more than anything else in the world and you know it's true. I couldn't say it if it\",\n",
              "  'weren\\'t true. Not today. A moment as Max studies his father, then: MAX (to Audrey) He\\'s telling the truth, Mom. He\\'s not allowed to lie. I made a wish and anything Dad says has to be the truth. (to Fletcher) .. Right? But Fletcher\\'s looking at his watch... FLETCHER Max. .. it\\'s 9:22. AUDREY What? FLETCHER Max, you made the wish at 9:15. I\\'ve been able to lie for the last seven minutes. Max steps away from Fletcher. MAX So then, you were... FLETCHER No! It wasn\\'t a lie. I just wanted to be honest with you and tell you -- there was no wish to guarantee it anymore. You just have to believe me. Max looks at Audrey, who is letting Max decide for himself Max looks at Fletcher and tries to decide. MAX (to Audrey) Mommy... do we have to go to Boston? Audrey looks at Jerry, then back at Max. AUDR EY No. We don\\'t have to. Fletcher hugs his son -- the kind of hug that says \"I\\'ll never let you go.\" MAX (to Fletcher) Can we play catch tomorrow? Fletcher smiles. . . EXT. PARK - DAY A beautiful park with a basball diamond. Fletcher is seated on a bench, waiting. He\\'s dressed in sweats, with a baseball glove. Soon, Jerry, Audrey, and Max pull up... M AX Dad! ! FLETCH ER Ma x i mu m ! ! Fletcher picks Max up. MAX Transformer!!! . Fletcher and Max do the TRANSFORMER ROUTINE again... FLETCHER Malfunction in vector seven. I have lost control of my affection reflex... Fletcher starts KISSING MAX on the head over and over. He',\n",
              "  \"sees Audrey. FLETCHER Procreate! Procreate! AUDREY (PLAYFULLY) Fletcher... You're gonna lose a limb-- MAX Come on, dad, let's play catch!! FLETCHER Sr. . . UE (starts to toss MAX) Here you go, mom. (Max screams) Oh, you mean with a ball... He puts Max down. Max runs into position. Fletcher stops for a second and turns to Jerry, man to man. FLETCHER I take back every dirty, dishonest thing I ever said about you, wrote about you, faxed about you, E-mailed about you. JERRY Appreciated. Fletcher tosses the baseball up and down. FLETCHER So, you up for a little friendly competition? JERRY No, you go play with your son. FLETCHER I wasn't talking about basesball. A slow smile from Jerry. Fletcher winks and tosses the ball to Max. FLETCHER (to Max) Alright, it's time to show you the old Fletcher Reid change up. Fletcher winds up in an EXAGERATED SUPER FAST MOTION, then instantly shifts to SUPER SLOW MOTION. Max CRACKS UP. Audrey LAUGHS. Jerry can't help but smile, too. There may be better things in life... but at this moment, it's hard to think of a single one. Honestly. THE END\",\n",
              "  'Liar Liar Writers : Tom Shadyac Mike Binder Genres : Comedy Fantasy User Comments Back to IMSDb Index | Submi t | Link to IMSDb | Disclaimer | Privacy pol icy | Contact',\n",
              "  'Provided proper attribution is provided, Google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works. Attention Is All You Need Ashish Vaswani∗ Google Brain avaswani@google.comNoam Shazeer∗ Google Brain noam@google.comNiki Parmar∗ Google Research nikip@google.comJakob Uszkoreit∗ Google Research usz@google.com Llion Jones∗ Google Research llion@google.comAidan N. Gomez∗ † University of Toronto aidan@cs.toronto.eduŁukasz Kaiser∗ Google Brain lukaszkaiser@google.com Illia Polosukhin∗ ‡ illia.polosukhin@gmail.com Abstract The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English- to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data. ∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect',\n",
              "  'and implemented the first Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research. †Work performed while at Google Brain. ‡Work performed while at Google Research. 31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7 [cs.CL] 2 Aug 2023',\n",
              "  '1 Introduction Recurrent neural networks, long short-term memory [ 13] and gated recurrent [ 7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [ 35,2,5]. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [38, 24, 15]. Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states ht, as a function of the previous hidden state ht−1and the input for position t. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. Recent work has achieved significant improvements in computational efficiency through factorization tricks [ 21] and conditional computation [ 32], while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains. Attention mechanisms have become an integral part of compelling sequence modeling and transduc- tion models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [ 2,19]. In all but a few cases [ 27], however, such attention mechanisms are used in conjunction with a recurrent network. In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs. 2 Background The goal of reducing sequential computation also forms the',\n",
              "  'hours on eight P100 GPUs. 2 Background The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [16], ByteNet [ 18] and ConvS2S [ 9], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2. Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 27, 28, 22]. End-to-end memory networks are based on a recurrent attention mechanism instead of sequence- aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [34]. To the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence- aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [17, 18] and [9]. 3 Model Architecture Most competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,35]. Here, the encoder maps',\n",
              "  'sequence transduction models have an encoder-decoder structure [ 5,2,35]. Here, the encoder maps an input sequence of symbol representations (x1, ..., x n)to a sequence of continuous representations z= (z1, ..., z n). Given z, the decoder then generates an output sequence (y1, ..., y m)of symbols one element at a time. At each step the model is auto-regressive [10], consuming the previously generated symbols as additional input when generating the next. 2',\n",
              "  'Figure 1: The Transformer - model architecture. The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively. 3.1 Encoder and Decoder Stacks Encoder: The encoder is composed of a stack of N= 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position- wise fully connected feed-forward network. We employ a residual connection [ 11] around each of the two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is LayerNorm( x+ Sublayer( x)), where Sublayer( x)is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel = 512 . Decoder: The decoder is also composed of a stack of N= 6identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position ican depend only on the known outputs at positions less than i. 3.2 Attention An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum 3',\n",
              "  'Scaled Dot-Product Attention Multi-Head Attention Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel. of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key. 3.2.1 Scaled Dot-Product Attention We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the query with all keys, divide each by√dk, and apply a softmax function to obtain the weights on the values. In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q. The keys and values are also packed together into matrices KandV. We compute the matrix of outputs as: Attention( Q, K, V ) = softmax(QKT √dk)V (1) The two most commonly used attention functions are additive attention [ 2], and dot-product (multi- plicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor of1√dk. Additive attention computes the compatibility function using a feed-forward network with a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code. While for small values of dkthe two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of dk[3]. We suspect that for large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients4. To counteract this effect, we scale the dot products by1√dk. 3.2.2 Multi-Head Attention Instead of performing a single attention function with dmodel-dimensional keys, values and queries, we found it beneficial to',\n",
              "  'attention function with dmodel-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values htimes with different, learned linear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional 4To illustrate why the dot products get large, assume that the components of qandkare independent random variables with mean 0and variance 1. Then their dot product, q·k=Pdk i=1qiki, has mean 0and variance dk. 4',\n",
              "  'output values. These are concatenated and once again projected, resulting in the final values, as depicted in Figure 2. Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this. MultiHead( Q, K, V ) = Concat(head 1, ...,head h)WO where head i= Attention( QWQ i, KWK i, V WV i) Where the projections are parameter matrices WQ i∈Rdmodel×dk,WK i∈Rdmodel×dk,WV i∈Rdmodel×dv andWO∈Rhdv×dmodel. In this work we employ h= 8 parallel attention layers, or heads. For each of these we use dk=dv=dmodel/h= 64 . Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality. 3.2.3 Applications of Attention in our Model The Transformer uses multi-head attention in three different ways: •In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [38, 2, 9]. •The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder. •Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to −∞) all values in the input of the softmax which correspond to illegal connections. See Figure 2. 3.3',\n",
              "  'all values in the input of the softmax which correspond to illegal connections. See Figure 2. 3.3 Position-wise Feed-Forward Networks In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between. FFN( x) = max(0 , xW 1+b1)W2+b2 (2) While the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1. The dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality dff= 2048 . 3.4 Embeddings and Softmax Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor- mation and softmax function to convert the decoder output to predicted next-token probabilities. In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [ 30]. In the embedding layers, we multiply those weights by√dmodel. 5',\n",
              "  'Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types. nis the sequence length, dis the representation dimension, kis the kernel size of convolutions and rthe size of the neighborhood in restricted self-attention. Layer Type Complexity per Layer Sequential Maximum Path Length Operations Self-Attention O(n2·d) O(1) O(1) Recurrent O(n·d2) O(n) O(n) Convolutional O(k·n·d2) O(1) O(logk(n)) Self-Attention (restricted) O(r·n·d) O(1) O(n/r) 3.5 Positional Encoding Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel as the embeddings, so that the two can be summed. There are many choices of positional encodings, learned and fixed [9]. In this work, we use sine and cosine functions of different frequencies: P E(pos,2i)=sin(pos/100002i/d model) P E(pos,2i+1)=cos(pos/100002i/d model) where posis the position and iis the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from 2πto10000 ·2π. We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset k,P Epos+kcan be represented as a linear function of P Epos. We also experimented with using learned positional embeddings [ 9] instead, and found that the two versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training. 4 Why Self-Attention In this section we compare various aspects of self-attention',\n",
              "  'during training. 4 Why Self-Attention In this section we compare various aspects of self-attention layers to the recurrent and convolu- tional layers commonly used for mapping one variable-length sequence of symbol representations (x1, ..., x n)to another sequence of equal length (z1, ..., z n), with xi, zi∈Rd, such as a hidden layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we consider three desiderata. One is the total computational complexity per layer. Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required. The third is the path length between long-range dependencies in the network. Learning long-range dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network. The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies [ 12]. Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types. As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O(n)sequential operations. In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence 6',\n",
              "  'length nis smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece [38] and byte-pair [ 31] representations. To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size rin the input sequence centered around the respective output position. This would increase the maximum path length to O(n/r). We plan to investigate this approach further in future work. A single convolutional layer with kernel width k < n does not connect all pairs of input and output positions. Doing so requires a stack of O(n/k)convolutional layers in the case of contiguous kernels, orO(logk(n))in the case of dilated convolutions [ 18], increasing the length of the longest paths between any two positions in the network. Convolutional layers are generally more expensive than recurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity considerably, to O(k·n·d+n·d2). Even with k=n, however, the complexity of a separable convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer, the approach we take in our model. As side benefit, self-attention could yield more interpretable models. We inspect attention distributions from our models and present and discuss examples in the appendix. Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences. 5 Training This section describes the training regime for our models. 5.1 Training Data and Batching We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source- target vocabulary of about 37000 tokens. For English-French,',\n",
              "  '[ 3], which has a shared source- target vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary [ 38]. Sentence pairs were batched together by approximate sequence length. Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens. 5.2 Hardware and Schedule We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps (3.5 days). 5.3 Optimizer We used the Adam optimizer [ 20] with β1= 0.9,β2= 0.98andϵ= 10−9. We varied the learning rate over the course of training, according to the formula: lrate =d−0.5 model·min(step_num−0.5, step _num·warmup _steps−1.5) (3) This corresponds to increasing the learning rate linearly for the first warmup _steps training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used warmup _steps = 4000 . 5.4 Regularization We employ three types of regularization during training: 7',\n",
              "  'Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German and English-to-French newstest2014 tests at a fraction of the training cost. ModelBLEU Training Cost (FLOPs) EN-DE EN-FR EN-DE EN-FR ByteNet [18] 23.75 Deep-Att + PosUnk [39] 39.2 1.0·1020 GNMT + RL [38] 24.6 39.92 2.3·10191.4·1020 ConvS2S [9] 25.16 40.46 9.6·10181.5·1020 MoE [32] 26.03 40.56 2.0·10191.2·1020 Deep-Att + PosUnk Ensemble [39] 40.4 8.0·1020 GNMT + RL Ensemble [38] 26.30 41.16 1.8·10201.1·1021 ConvS2S Ensemble [9] 26.36 41.29 7.7·10191.2·1021 Transformer (base model) 27.3 38.1 3.3·1018 Transformer (big) 28.4 41.8 2.3·1019 Residual Dropout We apply dropout [ 33] to the output of each sub-layer, before it is added to the sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of Pdrop= 0.1. Label Smoothing During training, we employed label smoothing of value ϵls= 0.1[36]. This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score. 6 Results 6.1 Machine Translation On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0 BLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is listed in the bottom line of Table 3. Training took 3.5days on 8P100 GPUs. Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models. On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0, outperforming all of the previously published single models, at less than 1/4the training cost of the previous state-of-the-art model. The Transformer (big) model trained for English-to-French used dropout',\n",
              "  'state-of-the-art model. The Transformer (big) model trained for English-to-French used dropout rate Pdrop= 0.1, instead of 0.3. For the base models, we used a single model obtained by averaging the last 5 checkpoints, which were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We used beam search with a beam size of 4and length penalty α= 0.6[38]. These hyperparameters were chosen after experimentation on the development set. We set the maximum output length during inference to input length + 50, but terminate early when possible [38]. Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature. We estimate the number of floating point operations used to train a model by multiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision floating-point capacity of each GPU5. 6.2 Model Variations To evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the 5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively. 8',\n",
              "  'Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base model. All metrics are on the English-to-German translation development set, newstest2013. Listed perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to per-word perplexities. N d model dff h d k dvPdrop ϵlstrain PPL BLEU params steps (dev) (dev) ×106 base 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65 (A)1 512 512 5.29 24.9 4 128 128 5.00 25.5 16 32 32 4.91 25.8 32 16 16 5.01 25.4 (B)16 5.16 25.1 58 32 5.01 25.4 60 (C)2 6.11 23.7 36 4 5.19 25.3 50 8 4.88 25.5 80 256 32 32 5.75 24.5 28 1024 128 128 4.66 26.0 168 1024 5.12 25.4 53 4096 4.75 26.2 90 (D)0.0 5.77 24.6 0.2 4.95 25.5 0.0 4.67 25.3 0.2 5.47 25.7 (E) positional embedding instead of sinusoids 4.92 25.7 big 6 1024 4096 16 0.3 300K 4.33 26.4 213 development set, newstest2013. We used beam search as described in the previous section, but no checkpoint averaging. We present these results in Table 3. In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section 3.2.2. While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads. In Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our sinusoidal positional encoding with learned positional embeddings [ 9], and observe nearly identical results to the base model. 6.3 English Constituency Parsing To evaluate if the Transformer can generalize to other tasks we performed experiments on English constituency parsing. This task',\n",
              "  'can generalize to other tasks we performed experiments on English constituency parsing. This task presents specific challenges: the output is subject to strong structural constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes [37]. We trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the Penn Treebank [ 25], about 40K training sentences. We also trained it in a semi-supervised setting, using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences [37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens for the semi-supervised setting. We performed only a small number of experiments to select the dropout, both attention and residual (section 5.4), learning rates and beam size on the Section 22 development set, all other parameters remained unchanged from the English-to-German base translation model. During inference, we 9',\n",
              "  'Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23 of WSJ) Parser Training WSJ 23 F1 Vinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3 Petrov et al. (2006) [29] WSJ only, discriminative 90.4 Zhu et al. (2013) [40] WSJ only, discriminative 90.4 Dyer et al. (2016) [8] WSJ only, discriminative 91.7 Transformer (4 layers) WSJ only, discriminative 91.3 Zhu et al. (2013) [40] semi-supervised 91.3 Huang & Harper (2009) [14] semi-supervised 91.3 McClosky et al. (2006) [26] semi-supervised 92.1 Vinyals & Kaiser el al. (2014) [37] semi-supervised 92.1 Transformer (4 layers) semi-supervised 92.7 Luong et al. (2015) [23] multi-task 93.0 Dyer et al. (2016) [8] generative 93.3 increased the maximum output length to input length + 300. We used a beam size of 21andα= 0.3 for both WSJ only and the semi-supervised setting. Our results in Table 4 show that despite the lack of task-specific tuning our model performs sur- prisingly well, yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar [8]. In contrast to RNN sequence-to-sequence models [ 37], the Transformer outperforms the Berkeley- Parser [29] even when training only on the WSJ training set of 40K sentences. 7 Conclusion In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention. For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles. We are excited about the future of attention-based models and plan to apply them to other tasks. We plan to extend the',\n",
              "  'the future of attention-based models and plan to apply them to other tasks. We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio and video. Making generation less sequential is another research goals of ours. The code we used to train and evaluate our models is available at https://github.com/ tensorflow/tensor2tensor . Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments, corrections and inspiration. References [1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450 , 2016. [2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. CoRR , abs/1409.0473, 2014. [3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural machine translation architectures. CoRR , abs/1703.03906, 2017. [4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine reading. arXiv preprint arXiv:1601.06733 , 2016. 10',\n",
              "  '[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. CoRR , abs/1406.1078, 2014. [6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv preprint arXiv:1610.02357 , 2016. [7]Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014. [8]Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural network grammars. In Proc. of NAACL , 2016. [9]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu- tional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017. [10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850 , 2013. [11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im- age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 770–778, 2016. [12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in recurrent nets: the difficulty of learning long-term dependencies, 2001. [13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation , 9(8):1735–1780, 1997. [14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations across languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing , pages 832–841. ACL, August 2009. [15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016. [16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural Information Processing Systems, (NIPS) , 2016. [17] Łukasz Kaiser',\n",
              "  'attention? In Advances in Neural Information Processing Systems, (NIPS) , 2016. [17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference on Learning Representations (ICLR) , 2016. [18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko- ray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 , 2017. [19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks. InInternational Conference on Learning Representations , 2017. [20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015. [21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint arXiv:1703.10722 , 2017. [22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint arXiv:1703.03130 , 2017. [23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task sequence to sequence learning. arXiv preprint arXiv:1511.06114 , 2015. [24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention- based neural machine translation. arXiv preprint arXiv:1508.04025 , 2015. 11',\n",
              "  '[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated corpus of english: The penn treebank. Computational linguistics , 19(2):313–330, 1993. [26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference , pages 152–159. ACL, June 2006. [27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention model. In Empirical Methods in Natural Language Processing , 2016. [28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive summarization. arXiv preprint arXiv:1705.04304 , 2017. [29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact, and interpretable tree annotation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL , pages 433–440. ACL, July 2006. [30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv preprint arXiv:1608.05859 , 2016. [31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. arXiv preprint arXiv:1508.07909 , 2015. [32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538 , 2017. [33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi- nov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine Learning Research , 15(1):1929–1958, 2014. [34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28 , pages 2440–2448. Curran',\n",
              "  'R. Garnett, editors, Advances in Neural Information Processing Systems 28 , pages 2440–2448. Curran Associates, Inc., 2015. [35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems , pages 3104–3112, 2014. [36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015. [37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In Advances in Neural Information Processing Systems , 2015. [38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144 , 2016. [39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with fast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016. [40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate shift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume 1: Long Papers) , pages 434–443. ACL, August 2013. 12',\n",
              "  'Attention Visualizations Input-Input Layer5 It is in this spirit that a majority of American governments have passed new laws since 2009 making the registration or voting process more difficult . <EOS> <pad> <pad> <pad> <pad> <pad> <pad> It is in this spirit that a majority of American governments have passed new laws since 2009 making the registration or voting process more difficult . <EOS> <pad> <pad> <pad> <pad> <pad> <pad> Figure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for the word ‘making’. Different colors represent different heads. Best viewed in color. 13',\n",
              "  'Input-Input Layer5 The Law will never be perfect , but its application should be just - this is what we are missing , in my opinion . <EOS> <pad> The Law will never be perfect , but its application should be just - this is what we are missing , in my opinion . <EOS> <pad> Input-Input Layer5 The Law will never be perfect , but its application should be just - this is what we are missing , in my opinion . <EOS> <pad> The Law will never be perfect , but its application should be just - this is what we are missing , in my opinion . <EOS> <pad>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top: Full attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5 and 6. Note that the attentions are very sharp for this word. 14',\n",
              "  'Input-Input Layer5 The Law will never be perfect , but its application should be just - this is what we are missing , in my opinion . <EOS> <pad> The Law will never be perfect , but its application should be just - this is what we are missing , in my opinion . <EOS> <pad> Input-Input Layer5 The Law will never be perfect , but its application should be just - this is what we are missing , in my opinion . <EOS> <pad> The Law will never be perfect , but its application should be just - this is what we are missing , in my opinion . <EOS> <pad>Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the sentence. We give two such examples above, from two different heads from the encoder self-attention at layer 5 of 6. The heads clearly learned to perform different tasks. 15'],\n",
              " 'uris': None,\n",
              " 'data': None,\n",
              " 'included': ['metadatas', 'documents']}"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Ver el contenido (por defecto devuelve ids, metadatas, documents)\n",
        "vectorstore.get()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wv01megykFTY"
      },
      "source": [
        "Incluso podemos ver los los indices, documentos y *embeddings*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g07eTsLfyZ5t",
        "outputId": "40506216-8c13-45e7-9248-0cd7e5b58d91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ID: f2bcc973-adca-4d8a-9de3-e91c00eb6a0e\n",
            "Documento: JOURNAL OF LATEX CLASS FILES, VOL. ??, NO. ??, MONTH 20YY 1 Unifying Large Language Models and Knowledge Graphs: A Roadmap Shirui Pan, Senior Member, IEEE , Linhao Luo, Yufei Wang, Chen Chen, Jiapu Wang, Xindong Wu, Fellow, IEEE Abstract —Large language models (LLMs), such as ChatGPT and GPT4, are making new waves in the field of natural language processing and artificial intelligence, due to their emergent ability and generalizability. However, LLMs are black-box models, which often fall short of capturing and accessing factual knowledge. In contrast, Knowledge Graphs (KGs), Wikipedia and Huapu for example, are structured knowledge models that explicitly store rich factual knowledge. KGs can enhance LLMs by providing external knowledge for inference and interpretability. Meanwhile, KGs are difficult to construct and evolve by nature, which challenges the existing methods in KGs to generate new facts and represent unseen knowledge. Therefore, it is complementary to unify LLMs and KGs together and simultaneously leverage their advantages. In this article, we present a forward-looking roadmap for the unification of LLMs and KGs. Our roadmap consists of three general frameworks, namely, 1) KG-enhanced LLMs, which incorporate KGs during the pre-training and inference phases of LLMs, or for the purpose of enhancing understanding of the knowledge learned by LLMs; 2) LLM-augmented KGs, that leverage LLMs for different KG tasks such as embedding, completion, construction, graph-to-text generation, and question answering; and 3) Synergized LLMs + KGs , in which LLMs and KGs play equal roles and work in a mutually beneficial way to enhance both LLMs and KGs for bidirectional reasoning driven by both data and knowledge. We review and summarize existing efforts within these three frameworks in our roadmap and pinpoint their future research directions. Index Terms —Natural Language Processing, Large Language Models, Generative Pre-Training, Knowledge Graphs, Roadmap,\n",
            "Embedding: [-0.51388657 -0.25669494 -0.20615539 -0.18620662 -0.01626176  0.03304666\n",
            " -0.27759844 -0.21492411 -0.01243054 -0.00398905 -0.25397259 -0.00098268\n",
            "  0.20700644 -0.10436599 -0.04958806 -0.00313735 -0.08442593 -0.19634463\n",
            " -0.31386113 -0.01961397  0.19986464  0.03193203  0.20724487 -0.06573146\n",
            "  0.10867881  0.06484466  0.16916515 -0.11669915  0.10995159 -0.10620932\n",
            " -0.09289739  0.65115428  0.07905628  0.10800038 -0.10267731  0.08533216\n",
            "  0.18935579 -0.20259513  0.08837736  0.24853341 -0.15350829  0.15347759\n",
            "  0.23998675  0.03295033  0.05804915 -0.23830937 -0.16414346  0.19353063\n",
            " -0.37253517  0.04561129 -0.12113982 -0.21322161 -0.12992156 -0.14332798\n",
            "  0.03573957  0.01016339 -0.06909362  0.11462446 -0.12902714  0.01857845\n",
            " -0.27501631 -0.0205891   0.03684405  0.4102191   0.45033538  0.28183204\n",
            " -0.24134737  0.09737825  0.29956895 -0.02173267 -0.27934727 -0.05212421\n",
            " -0.10541387  0.18766782  0.26510164 -0.15551661  0.01029573  0.27471709\n",
            "  0.24888802 -0.08371865  0.12514326 -0.03292685  0.03194848 -0.01968389\n",
            "  0.05481397  0.05399792  0.0534105  -0.33039045  0.21795334 -0.29053575\n",
            "  0.32452083 -0.20029262  0.05241776  0.37576127  0.22134502  0.14970554\n",
            "  0.26811051 -0.21524416  0.03434636  0.13059463 -0.03650753 -0.33796215\n",
            "  0.25374481  0.08576068 -0.02351193 -0.30294985  0.27257568  0.02135423\n",
            "  0.02501166 -0.06637798  0.07502531  0.02896358 -0.11196823 -0.15328182\n",
            " -0.08440423 -0.32195884  0.18525699  0.03647823 -0.31329367 -0.03783977\n",
            " -0.32192826 -0.17985505 -0.20697366 -0.13507909  0.05823003  0.23917514\n",
            " -0.35943824 -0.08828878 -0.22880317  0.23773834  0.0281108   0.20524964\n",
            "  0.06437414 -0.01958266  0.15561225 -0.20489685 -0.03722155 -0.2450812\n",
            " -0.20779593  0.31938127 -0.17786849  0.16024572  0.21304502 -0.04010943\n",
            " -0.01829775  0.15955272 -0.09813234  0.11204036  0.02171034  0.10425676\n",
            "  0.25797889 -0.31547451  0.04019062  0.22302839  0.26615259  0.0553176\n",
            "  0.22932984  0.11036948  0.06770307 -0.21112245 -0.09274002  0.03534521\n",
            " -0.02545357  0.14913602  0.14040516 -0.04182848 -0.03736611 -0.11214039\n",
            " -0.1936335   0.23203135  0.06788512 -0.01101233  0.01427472  0.09484542\n",
            "  0.33900774 -0.10243592  0.05996778 -0.25877905 -0.16585357 -0.07256598\n",
            "  0.04664561  0.12717496  0.50908363  0.08325368  0.0266319   0.2061923\n",
            "  0.27427766  0.05159613 -0.11476953  0.03755435 -0.10164897  0.15259805\n",
            "  0.5291602   0.09353407 -0.11498594 -0.0242663   0.37868598 -0.20549497\n",
            "  0.01106109 -0.02558587 -0.35170507  0.1404625  -0.08904399 -0.1161124\n",
            "  0.18424961 -0.05616685  0.13595015 -0.08648918 -0.15999611  0.25533834\n",
            " -0.22711834 -0.1253458  -0.19700886  0.0294058  -0.06936322  0.06297774\n",
            "  0.04485844  0.02899918 -0.35211971  0.12444536 -0.01904215  0.09150699\n",
            " -0.15588319 -0.19917029 -0.3607347   0.11445389 -0.19652498  0.17522043\n",
            " -0.14912158  0.04415351  0.12059768  0.16723828  0.64448738 -0.02482055\n",
            "  0.09312377 -0.10157361  0.16032809 -0.17705271 -0.23755552 -0.20772496\n",
            "  0.18498352  0.09668511 -0.17622328  0.11037292 -0.08220204 -0.10854817\n",
            " -0.46355137  0.0361632  -0.12686093  0.16993964 -0.24258678 -0.09471966\n",
            "  0.0284515   0.12023655  0.08016449 -0.09320311 -0.17669225 -0.16760585\n",
            " -0.05403542  0.21952857  0.20342414  0.40488061  0.0023016   0.33656004\n",
            "  0.2493183   0.18216877 -0.15892982  0.20496042 -0.00112285  0.07823913\n",
            " -0.35435021  0.05841158 -0.02079291 -0.08286344  0.29708073 -0.09148561\n",
            "  0.28502873 -0.45666462 -0.20238931 -0.06586368 -0.18683805  0.17816359\n",
            "  0.00404649  0.13434735  0.03296952 -0.01193796  0.01562013 -0.20847523\n",
            " -0.23773327 -0.25684071 -0.03748222  0.0966769   0.41348323 -0.09219661\n",
            "  0.18224844 -0.0024829  -0.10859785  0.00944835  0.17185006 -0.26575074\n",
            " -0.09534162  0.16157958 -0.06937669 -0.12498556 -0.01476904  0.22935693\n",
            "  0.0931349  -0.05478243  0.23799053 -0.14675993  0.29485956 -0.06021036\n",
            "  0.00164634  0.18416306 -0.04842572  0.29627889 -0.22004949  0.41780001\n",
            " -0.20100251  0.0158502  -0.09582612 -0.18470661 -0.04120702 -0.17659986\n",
            "  0.02848758 -0.02549396 -0.24620447 -0.05360469  0.06526999  0.14817701\n",
            "  0.04800251  0.08516421 -0.37087598  0.04911223  0.04239414  0.18265522\n",
            " -0.09499705  0.2736558  -0.08773477 -0.2008509   0.25289941 -0.19338633\n",
            "  0.05015191  0.05723638  0.28197643 -0.08021102 -0.28259197  0.29639351\n",
            " -0.1252332   0.02056647 -0.06417701  0.14269397 -0.29951742 -0.07622214\n",
            "  0.11823234  0.22285868 -0.31082711  0.06291133  0.03343932  0.2062373\n",
            " -0.00916205 -0.02290867 -0.13009709  0.09699382  0.05947208 -0.18051147\n",
            " -0.05496725 -0.24029589  0.19359401 -0.04101613 -0.18586104 -0.07505423\n",
            " -0.00689472  0.03938305 -0.01932525 -0.03754096  0.01571728  0.01432461\n",
            " -0.02848997  0.22056568 -0.13782118 -0.21725115 -0.15410805  0.00950423]\n",
            "\n",
            "ID: 272f7840-e978-4776-b45c-3d1704f9c26b\n",
            "Documento: Language Processing, Large Language Models, Generative Pre-Training, Knowledge Graphs, Roadmap, Bidirectional Reasoning. ✦ 1 I NTRODUCTION Large language models (LLMs)1(e.g., BERT [1], RoBERTA [2], and T5 [3]), pre-trained on the large-scale corpus, have shown great performance in various natural language processing (NLP) tasks, such as question answering [4], machine translation [5], and text generation [6]. Recently, the dramatically increasing model size further enables the LLMs with the emergent ability [7], paving the road for applying LLMs as Artificial General Intelligence (AGI). Advanced LLMs like ChatGPT2and PaLM23, with billions of parameters, exhibit great potential in many complex practical tasks, such as education [8], code generation [9] and recommendation [10]. •Shirui Pan is with the School of Information and Communication Tech- nology and Institute for Integrated and Intelligent Systems (IIIS), Griffith University, Queensland, Australia. Email: s.pan@griffith.edu.au; •Linhao Luo and Yufei Wang are with the Department of Data Sci- ence and AI, Monash University, Melbourne, Australia. E-mail: lin- hao.luo@monash.edu, garyyufei@gmail.com. •Chen Chen is with the Nanyang Technological University, Singapore. E- mail: s190009@ntu.edu.sg. •Jiapu Wang is with the Faculty of Information Technology, Beijing Uni- versity of Technology, Beijing, China. E-mail: jpwang@emails.bjut.edu.cn. •Xindong Wu is with the Key Laboratory of Knowledge Engineering with Big Data (the Ministry of Education of China), Hefei University of Tech- nology, Hefei, China, and also with the Research Center for Knowledge Engineering, Zhejiang Lab, Hangzhou, China. Email: xwu@hfut.edu.cn. •Shirui Pan and Linhao Luo contributed equally to this work. •Corresponding Author: Xindong Wu. 1. LLMs are also known as pre-trained language models (PLMs). 2. https://openai.com/blog/chatgpt 3. https://ai.google/discover/palm2 Fig. 1. Summarization of the pros and cons for LLMs and KGs. LLM pros:\n",
            "Embedding: [-0.24012934 -0.31098637 -0.09735315 -0.46396768 -0.22748636 -0.09953386\n",
            " -0.31989366 -0.14685243 -0.05741317  0.0737296  -0.26031801  0.22425172\n",
            "  0.23428005 -0.02823932 -0.12556688  0.06123231  0.13873385 -0.05890287\n",
            " -0.41342956 -0.27818704  0.11970261  0.02372462  0.14050226 -0.08174009\n",
            "  0.1323446   0.08149805  0.06478287 -0.16856864  0.33140272 -0.19733614\n",
            " -0.13175045  0.38495386  0.30095845 -0.02518066 -0.29918599  0.24709927\n",
            "  0.04561601 -0.08499427  0.05393837  0.17054144 -0.01178632  0.05809838\n",
            "  0.02532331  0.08179926 -0.06492223 -0.16692099 -0.211255    0.3150506\n",
            " -0.35685658  0.14124721 -0.27851009 -0.0861228  -0.26565185 -0.07422572\n",
            " -0.03819305 -0.16940902  0.02650999  0.16678768 -0.20678799 -0.02161345\n",
            " -0.40252733  0.00419084 -0.08224349  0.36415941  0.46685523  0.18713325\n",
            " -0.04341811  0.18153661  0.16755129  0.16711049 -0.13291709 -0.01884737\n",
            " -0.23251736  0.32570717  0.18509501 -0.1067758   0.04481625  0.10360527\n",
            "  0.40168479 -0.08969118  0.07453933 -0.00267456  0.03586552  0.09711704\n",
            "  0.12005669 -0.0451708  -0.00386402  0.08036621  0.28947994 -0.14869006\n",
            "  0.14341435 -0.26627716 -0.16204676  0.31612915 -0.00623558  0.17761862\n",
            "  0.20351623 -0.19004491  0.1235953   0.08990816 -0.11792856 -0.26667506\n",
            "  0.06912787  0.10217785  0.08875918 -0.40242431  0.09303546 -0.06743546\n",
            "  0.13793601 -0.0043043   0.02523548  0.0554716  -0.09340016 -0.10286684\n",
            " -0.18103954 -0.30265558  0.06280676 -0.01826445 -0.13322964 -0.10447978\n",
            " -0.15871421 -0.14793062 -0.03292012 -0.03790793  0.23212378  0.27445269\n",
            " -0.15275478  0.03966888 -0.09232759  0.01011162  0.21767585  0.17841291\n",
            " -0.01037846  0.0039025   0.16392486 -0.15964666  0.08296685 -0.44124287\n",
            " -0.24159898  0.14427014 -0.09578411  0.10752511  0.24227132 -0.10882083\n",
            "  0.20490371  0.28771114 -0.1262572   0.02447557  0.11054237 -0.05315685\n",
            "  0.29862282 -0.24180578 -0.00796312 -0.05014225  0.24741629 -0.01360415\n",
            "  0.15576522  0.06334721 -0.09071361 -0.27765438 -0.32016763  0.14473552\n",
            " -0.03685235 -0.19903044  0.42335248 -0.05001041  0.08679409 -0.07558006\n",
            " -0.15981847  0.37752151  0.04364411 -0.02063845 -0.17897467  0.03602026\n",
            "  0.33022636 -0.04394409 -0.17367941 -0.2208024  -0.1769214   0.04209218\n",
            "  0.29000211  0.01598871  0.29574859  0.24950947  0.02769431  0.13112058\n",
            "  0.25705764  0.24625649 -0.33943072  0.05713088  0.03739252  0.15891315\n",
            "  0.46855918 -0.18156147 -0.29782453 -0.0945749   0.52798057 -0.37196034\n",
            "  0.07498307 -0.11159859 -0.12337282  0.08513055 -0.23291098  0.01249189\n",
            "  0.14169897 -0.01779323 -0.21636915 -0.07291187  0.02627915  0.36107078\n",
            " -0.31476039 -0.01518872  0.14118616 -0.02342691  0.07447599 -0.28051415\n",
            "  0.1486019   0.20316926 -0.27815354  0.16104807 -0.05354383 -0.04868758\n",
            " -0.10980569 -0.39867958 -0.30338967 -0.00933707 -0.17888455  0.15021618\n",
            " -0.16730022 -0.04265718  0.11472792 -0.14007877  0.42227015 -0.02736454\n",
            "  0.06363279 -0.045812    0.13674292 -0.24245653 -0.43755993 -0.16890001\n",
            "  0.17272431  0.0828668   0.08706196  0.02619446 -0.11275429  0.14465231\n",
            " -0.63305235  0.09398747  0.05716364  0.19128025 -0.47532371 -0.02112414\n",
            " -0.18468225  0.22468138  0.06263758 -0.01515582 -0.03019624 -0.19329357\n",
            " -0.12081794  0.23316398  0.42462811  0.28335828  0.17673215  0.41354078\n",
            "  0.40744382 -0.0131316  -0.17904162  0.04042603  0.00192502  0.05194059\n",
            " -0.39631012  0.13166611 -0.01141159  0.03013444  0.3204816   0.06656142\n",
            "  0.0944131  -0.43210509 -0.14665708 -0.35853475 -0.04218576 -0.09163827\n",
            " -0.02491218  0.08809276 -0.11648767 -0.18580607  0.25022426 -0.26608691\n",
            " -0.14006443 -0.18632501  0.12959041  0.20867546  0.32337505 -0.24441351\n",
            " -0.09016138  0.02685307  0.16081592  0.13717078  0.18801415 -0.20892145\n",
            " -0.20727395 -0.03331986  0.03682073 -0.00245602 -0.08904092  0.10179339\n",
            "  0.14622693 -0.04344711  0.23683211 -0.01908623  0.25831497  0.19316722\n",
            " -0.07357104  0.28003904 -0.1972778   0.37264711 -0.07150847  0.55547607\n",
            " -0.16621181 -0.13019259 -0.11051658 -0.31543908 -0.0838397  -0.09797972\n",
            "  0.02540313 -0.01857271 -0.24822783  0.21482107 -0.02837642  0.02060343\n",
            "  0.10552596  0.23264919 -0.20175487  0.15180951  0.08799008  0.04566649\n",
            "  0.15752678  0.34119922 -0.25470069 -0.2409195   0.45136556  0.15017861\n",
            "  0.00369545 -0.12685542  0.24013047 -0.15340962 -0.18132222  0.48119435\n",
            " -0.08381423 -0.14835612 -0.11509294  0.22731379 -0.21485706 -0.20280474\n",
            "  0.15703407  0.13243113 -0.23132847  0.09590241  0.13201274  0.1159424\n",
            "  0.06829264  0.02750685 -0.30770037  0.24635947  0.18495362 -0.05420978\n",
            " -0.15544905  0.02854957  0.16944188  0.00503496 -0.18312132  0.13362037\n",
            "  0.09707459  0.00239662  0.25728691 -0.00841785 -0.02766727 -0.12946343\n",
            "  0.04607473  0.34039992 -0.1001692  -0.06091326  0.06673163 -0.15909569]\n",
            "\n",
            "ID: 903a3185-d3f0-45e5-ac08-eea88261c9e2\n",
            "Documento: Fig. 1. Summarization of the pros and cons for LLMs and KGs. LLM pros: General Knowledge [11], Language Processing [12], Generaliz- ability [13]; LLM cons: Implicit Knowledge [14], Hallucination [15], In- decisiveness [16], Black-box [17], Lacking Domain-specific/New Knowl- edge [18]. KG pros: Structural Knowledge [19], Accuracy [20], Decisive- ness [21], Interpretability [22], Domain-specific Knowledge [23], Evolv- ing Knowledge [24]; KG cons: Incompleteness [25], Lacking Language Understanding [26], Unseen Facts [27]. Pros. and Cons. are selected based on their representativeness. Detailed discussion can be found in Appendix A. Despite their success in many applications, LLMs have been criticized for their lack of factual knowledge. Specif- ically, LLMs memorize facts and knowledge contained in the training corpus [14]. However, further studies reveal that LLMs are not able to recall facts and often experience hallucinations by generating statements that are factually 0000–0000/00$00.00 © 2023 IEEEarXiv:2306.08302v3 [cs.CL] 25 Jan 2024\n",
            "Embedding: [-2.61469334e-01  2.43863270e-01 -5.09939194e-02 -4.08052206e-01\n",
            " -8.89106318e-02  2.15074532e-02  3.47701043e-01  4.15255204e-02\n",
            " -5.34782335e-02 -7.90349543e-02 -6.81111589e-02  9.03498568e-03\n",
            "  3.00300300e-01 -1.10646039e-01 -1.88415796e-01 -1.77104652e-01\n",
            "  2.15663046e-01  7.09732845e-02 -4.58224893e-01 -1.72532827e-01\n",
            "  9.54415649e-02  8.62530917e-02  6.82790130e-02  1.39406115e-01\n",
            "  8.03223029e-02  4.17410862e-03  3.34455520e-02 -2.30287928e-02\n",
            "  3.27716582e-03 -2.05248013e-01  3.36841494e-02  3.63630503e-01\n",
            "  2.74897039e-01  1.04447983e-01 -1.48902476e-01 -4.51196693e-02\n",
            "  7.56158829e-02  2.49181896e-01  1.45128161e-01  3.96873169e-02\n",
            "  1.20896265e-01  5.42676598e-02  9.33988392e-03  3.27021122e-01\n",
            "  2.21745484e-02  3.67050394e-02 -6.97586536e-02  2.05702186e-01\n",
            " -2.19408065e-01  1.17149942e-01 -8.40714723e-02  1.22384995e-01\n",
            " -1.07557610e-01 -3.15545589e-01 -5.33706807e-02  1.24930017e-01\n",
            "  3.02232858e-02 -1.96765773e-02 -2.27928653e-01 -1.45144656e-01\n",
            "  2.05303758e-01 -5.54445013e-02 -3.19176018e-01  2.18188465e-01\n",
            "  3.21633101e-01  2.97756165e-01  3.41101885e-02  7.56565034e-02\n",
            "  2.71452963e-01  2.50628263e-01  3.19943763e-03  2.43678987e-01\n",
            " -2.58823097e-01  1.04827330e-01  2.61331033e-02  3.83242816e-02\n",
            "  3.42312902e-02  2.57533751e-02  1.69683710e-01 -3.21073115e-01\n",
            " -2.89226845e-02  1.57552242e-01 -2.03349814e-03  6.81349933e-02\n",
            "  7.16348551e-03 -8.74772593e-02  9.78066474e-02  1.93058848e-01\n",
            "  2.26080894e-01 -2.41540253e-01  1.41721293e-01 -4.36026722e-01\n",
            " -1.58599913e-01 -4.77423286e-03  1.12849571e-01  1.53570995e-01\n",
            " -6.99801967e-02 -3.28298688e-01 -6.01994619e-02  1.03016250e-01\n",
            " -3.71249974e-01 -5.30362017e-02 -3.90604325e-02 -7.22603174e-03\n",
            " -7.70030357e-03 -5.49898148e-01  1.09069407e-01  3.16418558e-01\n",
            "  1.01348959e-01  7.79294074e-02 -4.54257578e-02  3.87063809e-02\n",
            "  1.41343996e-01  1.04738772e-01 -2.03351572e-01 -2.03870967e-01\n",
            "  1.72089934e-01  8.92659649e-02  3.11583802e-02 -2.53421634e-01\n",
            " -3.38408872e-02 -1.09698392e-01  1.89834043e-01  9.02225971e-02\n",
            " -4.45858650e-02  2.09921509e-01 -3.55044931e-01  2.76921894e-02\n",
            " -7.79962093e-02  5.28296828e-02  2.07750425e-01  2.93337315e-01\n",
            " -5.29480614e-02  9.00045931e-02  1.01377852e-01 -2.76146114e-01\n",
            "  1.07351139e-01 -9.52643976e-02 -7.00986683e-02  1.72457650e-01\n",
            " -1.40313029e-01  4.25320826e-02  2.42434412e-01  3.10009092e-01\n",
            "  1.20892696e-01  1.74688920e-03 -3.75266187e-02 -2.09062397e-01\n",
            " -8.32339097e-03  4.05276455e-02  3.70693207e-02 -2.94924378e-01\n",
            "  6.31989092e-02  1.96867630e-01  1.63808241e-01  9.43321139e-02\n",
            "  4.93445471e-02 -2.28044465e-02 -1.62277609e-01 -4.74024601e-02\n",
            " -1.32448539e-01  2.33787879e-01  1.21058218e-01  3.06775197e-02\n",
            "  2.82815974e-02  9.72326696e-02  8.43588859e-02 -2.71297574e-01\n",
            "  8.64157006e-02  2.74683774e-01 -1.29037499e-02 -1.25937507e-01\n",
            " -1.29860654e-01  3.91493082e-01  2.23936811e-01 -2.56716549e-01\n",
            "  4.31474149e-02 -1.92187428e-01 -1.02946825e-01  7.23251551e-02\n",
            "  6.09806813e-02 -1.62653491e-01  2.58366883e-01  1.84667990e-01\n",
            " -1.32840633e-01  2.25709781e-01  5.89410327e-02  2.24299356e-01\n",
            " -2.70085156e-01  1.78789958e-01  9.36245993e-02  2.68235147e-01\n",
            "  4.02648710e-02  1.61184430e-01 -4.13682818e-01 -2.96866357e-01\n",
            "  2.67138004e-01 -3.17296982e-01 -1.13781467e-01  1.63118035e-01\n",
            " -2.63108432e-01  8.87431875e-02 -1.30933702e-01 -3.34182918e-01\n",
            "  1.93080395e-01 -8.04528818e-02 -3.47489305e-02 -1.31009623e-01\n",
            "  2.12257400e-01  1.08984083e-01 -1.58989012e-01 -4.50934134e-02\n",
            " -1.87845260e-01 -7.37105384e-02 -2.10344307e-02 -1.48430660e-01\n",
            "  2.25480124e-02  1.80583179e-01 -3.62070173e-01  1.73808351e-01\n",
            " -2.04401240e-01 -4.49708328e-02 -1.82136640e-01 -3.64657998e-01\n",
            " -3.44928056e-01 -1.22409556e-02 -3.32569517e-03  1.57843053e-01\n",
            "  4.61648628e-02  1.95081145e-01  3.75177488e-02 -2.44861409e-01\n",
            "  4.69318062e-01 -5.15915453e-05 -1.27228126e-01  1.06454745e-01\n",
            " -1.19971130e-02 -3.16248775e-01 -3.23770165e-01 -2.30383128e-01\n",
            " -1.32629380e-01 -2.61897385e-01 -2.29666289e-02  3.81938964e-02\n",
            "  5.28130531e-02  2.00828835e-01 -4.86881614e-01 -4.80857901e-02\n",
            " -2.10064463e-02  2.48109043e-01 -4.35117722e-01 -6.43380806e-02\n",
            " -1.08445972e-01  7.56187364e-02  9.15455669e-02 -6.80676401e-02\n",
            "  3.71067263e-02 -9.68989208e-02 -7.35352412e-02 -2.90608644e-01\n",
            "  3.56807381e-01  2.87850648e-01 -1.48115188e-01  1.58498511e-01\n",
            "  1.05352715e-01 -1.06039062e-01  1.22360207e-01 -2.27082521e-01\n",
            "  4.71630283e-02 -5.14085479e-02 -3.73047851e-02 -4.74460155e-01\n",
            "  2.88011879e-01 -8.49674568e-02  3.63063157e-01 -2.16126382e-01\n",
            "  3.76406871e-03 -3.07903469e-01 -2.92024557e-02  2.00368166e-02\n",
            "  1.16104208e-01 -1.30104452e-01  8.92490223e-02 -9.57479700e-03\n",
            "  1.26504749e-01 -1.49058253e-01  7.24896416e-02  2.40105949e-02\n",
            " -2.34060958e-01 -1.22533053e-01  1.74741551e-01  3.13400626e-01\n",
            "  3.69197465e-02 -1.35643318e-01  1.93010509e-01 -9.65223461e-02\n",
            " -8.27878118e-02 -3.87092791e-02  2.59327322e-01  7.28479400e-02\n",
            " -7.28860795e-02  1.11493044e-01  5.93889179e-03 -2.23822325e-01\n",
            "  2.92565674e-03  4.45126649e-03 -5.51204383e-02  1.59343928e-01\n",
            "  1.02688238e-01  2.34212369e-01  2.44302392e-01  1.60168141e-01\n",
            "  4.36252169e-02  3.70893031e-01 -3.01571667e-01  2.06585273e-01\n",
            " -3.33694518e-01  4.30635393e-01 -1.19552396e-01 -6.75602257e-02\n",
            "  1.91902816e-02 -4.15879905e-01 -2.64778823e-01 -1.74585935e-02\n",
            " -1.32529572e-01  1.59182344e-02 -3.08233827e-01 -2.06419695e-02\n",
            "  2.35567484e-02  5.53393774e-02  1.29600158e-02  1.14220865e-01\n",
            " -3.61627251e-01 -4.13405709e-03  7.19083846e-02  3.08679305e-02\n",
            "  6.41026050e-02  1.80010721e-01 -2.25698009e-01 -1.63709059e-01\n",
            "  1.36927739e-01  2.43794441e-01  2.39130959e-01 -1.39956206e-01\n",
            "  1.08959593e-01  3.27668749e-02 -8.22165236e-02  3.41982186e-01\n",
            " -3.23181935e-02  3.94476891e-01  1.27744321e-02  1.22244582e-01\n",
            "  7.13822320e-02 -5.18068597e-02  2.43890613e-01 -2.16034725e-01\n",
            " -1.14949480e-01  1.92927629e-01  2.23840818e-01  1.63025141e-01\n",
            " -3.07599604e-02 -2.12139823e-02 -3.65003124e-02  5.47159672e-01\n",
            " -5.18011935e-02 -1.28939629e-01  1.50813041e-02  6.99305311e-02\n",
            "  3.01901363e-02  1.11807391e-01  4.00525890e-03  7.54060149e-02\n",
            " -1.65483758e-01  3.41545045e-02  1.41314685e-01  2.33878493e-01\n",
            "  1.44956306e-01 -1.89088821e-01 -7.77446926e-02  1.19418658e-01\n",
            " -6.87672645e-02 -4.24808741e-01 -1.95162460e-01  1.51619688e-01]\n",
            "\n",
            "ID: 3650e60e-20a1-4d69-bc5b-7cd73b7403de\n",
            "Documento: JOURNAL OF LATEX CLASS FILES, VOL. ??, NO. ??, MONTH 20YY 2 incorrect [15], [28]. For example, LLMs might say “Ein- stein discovered gravity in 1687” when asked, “When did Einstein discover gravity?”, which contradicts the fact that Isaac Newton formulated the gravitational theory. This issue severely impairs the trustworthiness of LLMs. As black-box models, LLMs are also criticized for their lack of interpretability. LLMs represent knowledge implic- itly in their parameters. It is difficult to interpret or validate the knowledge obtained by LLMs. Moreover, LLMs perform reasoning by a probability model, which is an indecisive process [16]. The specific patterns and functions LLMs used to arrive at predictions or decisions are not directly accessible or explainable to humans [17]. Even though some LLMs are equipped to explain their predictions by applying chain-of-thought [29], their reasoning explanations also suf- fer from the hallucination issue [30]. This severely impairs the application of LLMs in high-stakes scenarios, such as medical diagnosis and legal judgment. For instance, in a medical diagnosis scenario, LLMs may incorrectly diagnose a disease and provide explanations that contradict medical commonsense. This raises another issue that LLMs trained on general corpus might not be able to generalize well to specific domains or new knowledge due to the lack of domain-specific knowledge or new training data [18]. To address the above issues, a potential solution is to in- corporate knowledge graphs (KGs) into LLMs. Knowledge graphs (KGs), storing enormous facts in the way of triples, i.e.,(head entity, relation, tail entity ), are a structured and decisive manner of knowledge representation (e.g., Wiki- data [20], YAGO [31], and NELL [32]). KGs are crucial for various applications as they offer accurate explicit knowl- edge [19]. Besides, they are renowned for their symbolic reasoning ability [22], which generates interpretable results. KGs can also actively\n",
            "Embedding: [-4.80583936e-01 -9.38314050e-02 -7.79005587e-02 -7.81815127e-03\n",
            "  1.74446404e-01  2.70368420e-02 -1.06111601e-01 -1.34275287e-01\n",
            "  1.26259997e-01 -2.01061562e-01 -1.57684118e-01 -8.70860219e-02\n",
            "  9.36192423e-02  8.41923431e-03 -3.76835197e-01 -9.37031060e-02\n",
            "  1.63370259e-02 -2.74004132e-01 -2.50359118e-01  4.09292579e-02\n",
            "  2.13679001e-01  1.04017034e-01  1.54256582e-01  9.73338634e-02\n",
            " -5.93002364e-02 -6.93790019e-02 -6.42521083e-02 -2.41428465e-01\n",
            " -7.44424686e-02 -3.13619599e-02 -1.75495058e-01  2.88447320e-01\n",
            "  4.39801142e-02 -1.60766199e-01  2.76770629e-02  1.25447839e-01\n",
            "  4.15913463e-01 -1.26407132e-01  5.66718616e-02  5.79889864e-02\n",
            " -1.78506002e-01 -7.76360277e-03  3.14857066e-01  1.01194099e-01\n",
            " -1.13108046e-01  1.09137662e-01  2.17966586e-01 -9.57756676e-03\n",
            " -4.31514621e-01  4.35143895e-02  7.55221769e-03 -3.16287130e-02\n",
            " -9.65849385e-02 -5.31323314e-01 -2.11649269e-01  2.17216253e-01\n",
            " -8.85236729e-03  2.33383346e-02 -6.83706999e-02 -2.27232605e-01\n",
            " -1.04592606e-01  9.00593102e-02 -1.36874631e-01  3.13984454e-01\n",
            "  5.79419971e-01  2.12594464e-01 -1.86970800e-01 -6.11408278e-02\n",
            "  4.69507664e-01  1.09034404e-01  1.14466138e-01  1.78833455e-01\n",
            "  1.22295931e-01  2.85876125e-01  3.18246484e-01  5.62925041e-02\n",
            "  3.00432831e-01  3.65525573e-01  3.00140470e-01 -2.46461824e-01\n",
            " -2.71204039e-02 -3.20688635e-03 -9.92424339e-02  7.72131681e-02\n",
            " -8.89305472e-02 -2.15747133e-01  2.20349342e-01  9.61937159e-02\n",
            "  4.10750121e-01 -2.37272233e-01  3.79963577e-01 -4.19035435e-01\n",
            " -1.46983385e-01  5.27429692e-02  7.31281340e-02  2.86796212e-01\n",
            " -2.00649425e-01 -9.82808694e-02  7.20397457e-02 -1.04680896e-01\n",
            " -1.57569543e-01 -8.98767188e-02 -9.50411409e-02  1.83929890e-01\n",
            "  1.18444167e-01 -1.14155374e-01  3.51326447e-03 -1.40788779e-01\n",
            "  3.38403255e-01 -6.68749912e-03  1.57852113e-01 -6.10588789e-02\n",
            "  3.90696883e-01 -2.06192434e-01 -1.33694097e-01  6.57917932e-04\n",
            "  2.50701047e-02  2.15008765e-01 -1.74982846e-01  4.69066761e-02\n",
            " -2.56323427e-01 -3.25899601e-01  9.08271968e-03 -1.97190776e-01\n",
            " -1.49979424e-02  3.89943182e-01 -5.38798511e-01  4.61617075e-02\n",
            " -2.07798019e-01 -9.36032087e-02  1.18732736e-01 -1.35999978e-01\n",
            " -1.16744876e-01  1.42255276e-01  2.84060389e-01 -3.82331433e-03\n",
            " -2.06395052e-04  5.30514978e-02 -7.46597648e-02  1.86365634e-01\n",
            "  4.31887396e-02  6.94271624e-02  1.17521122e-01  1.40031040e-01\n",
            "  2.31746398e-02  1.85464025e-02  2.36616116e-02 -1.34902507e-01\n",
            " -2.44122952e-01  8.20625126e-02  2.28252947e-01 -2.74169087e-01\n",
            "  1.77218765e-01  8.82075652e-02  5.54271787e-02  1.69255331e-01\n",
            "  1.79441452e-01 -8.40417370e-02 -1.91115513e-01  2.03710515e-02\n",
            " -7.07368702e-02  2.41246462e-01  2.59875149e-01  3.26416135e-01\n",
            "  1.69353873e-01  2.99908310e-01 -1.25134587e-01 -3.40049863e-01\n",
            "  3.33166540e-01  9.02489796e-02  1.49177462e-01 -1.24153629e-01\n",
            "  1.48554176e-01  9.82243419e-02  3.94694239e-01 -3.17621142e-01\n",
            "  1.24392748e-01 -4.46316421e-01 -9.68275405e-03  3.01946551e-02\n",
            " -4.51727882e-02 -2.58874983e-01  5.18461287e-01  1.53234512e-01\n",
            " -2.37632304e-01  5.34211874e-01  9.95983109e-02  5.56884930e-02\n",
            " -5.36568612e-02  2.74124116e-01  3.46131206e-01  1.55347288e-01\n",
            "  2.41136640e-01  2.67778993e-01 -1.32731408e-01 -2.65675467e-02\n",
            "  2.30982006e-01 -7.46847838e-02 -1.82030529e-01  3.01459253e-01\n",
            " -3.95005643e-01 -9.07707438e-02 -1.54632032e-01 -9.51894522e-02\n",
            " -8.77289921e-02 -2.20814601e-01  4.22418058e-01 -3.45220566e-01\n",
            " -1.17085064e-02  9.29244682e-02  1.77903473e-02 -2.82173544e-01\n",
            " -3.37707132e-01 -2.90917791e-02 -1.56821921e-01  4.54158336e-03\n",
            " -1.18673332e-02 -1.56832591e-01 -2.03038856e-01 -4.52045240e-02\n",
            " -2.64310151e-01  2.02758059e-01 -3.56930465e-01 -2.38897890e-01\n",
            " -3.77645284e-01 -1.25785142e-01 -1.00099921e-01  3.13612849e-01\n",
            " -7.69244656e-02  2.72640049e-01 -3.21539402e-01 -1.00102268e-01\n",
            "  3.55329990e-01  1.38992622e-01  3.10497761e-01 -3.71786028e-01\n",
            " -2.49648884e-01 -3.62239659e-01 -1.57481670e-01 -2.16049179e-01\n",
            " -3.44746232e-01 -2.41370857e-01 -3.56545478e-01  5.81490770e-02\n",
            "  1.58345737e-02  1.92206949e-01 -2.52768964e-01 -2.13789701e-01\n",
            " -2.06122592e-01 -1.77243978e-01  1.56022117e-01 -1.90383077e-01\n",
            " -8.96204636e-02  1.52551448e-02  2.15476125e-01  1.45892099e-01\n",
            " -6.45496026e-02 -3.96616794e-02  1.03804298e-01  1.85402930e-02\n",
            "  2.85299957e-01  3.75441968e-01  1.41511962e-01  1.29866257e-01\n",
            "  1.53900430e-01  4.28885639e-01  2.75608927e-01  6.26585558e-02\n",
            " -2.17221841e-01 -7.92274252e-02 -6.93810359e-02 -1.47583336e-01\n",
            "  4.85334128e-01 -1.21834949e-01  9.29331332e-02 -4.02452618e-01\n",
            "  1.12482399e-01 -1.65723994e-01 -1.73384637e-01 -1.26828685e-01\n",
            " -1.34649768e-01  1.13179497e-01  3.12595218e-02 -3.01539246e-02\n",
            " -1.61811382e-01  8.80110264e-02 -1.41353130e-01 -2.21924454e-01\n",
            " -1.53266609e-01  1.21865846e-01 -1.84024990e-01  4.81841207e-01\n",
            "  7.73737133e-02 -1.50997471e-03  5.00339925e-01 -8.01815540e-02\n",
            "  5.37597127e-02 -1.67194664e-01  8.70452821e-02  3.99485007e-02\n",
            "  8.60318169e-02 -1.29235819e-01 -2.29700416e-01 -6.65558651e-02\n",
            "  2.62347609e-01  1.64615642e-02  3.52596581e-01  2.80496925e-01\n",
            " -2.56765224e-02 -1.35151863e-01  1.38512090e-01 -5.46056498e-03\n",
            "  1.29040303e-02  7.56740928e-01  1.37758777e-01  1.62120402e-01\n",
            " -2.48704344e-01  1.96710885e-01 -1.17370874e-01  2.10807681e-01\n",
            "  1.88426599e-02 -2.74654269e-01 -2.07931757e-01 -2.64757961e-01\n",
            " -2.95049325e-02  1.56797186e-01  5.89221865e-02 -2.65204087e-02\n",
            " -3.32471430e-01  2.97483593e-01 -3.34203660e-01 -1.36431172e-01\n",
            " -3.84956956e-01 -1.27796382e-01  5.03678992e-02  3.57841030e-02\n",
            " -1.81254923e-01  2.19636589e-01 -3.13678354e-01 -1.27795897e-02\n",
            "  4.96290140e-02 -5.75348176e-03  3.39378119e-01 -1.47868186e-01\n",
            "  3.02984595e-01 -9.15723667e-02 -1.40339896e-01  9.44106877e-02\n",
            " -2.76551425e-01  2.42552698e-01 -1.80631101e-01  3.27415019e-01\n",
            "  4.99334931e-02 -1.91077650e-01  1.25255033e-01 -1.36827856e-01\n",
            " -2.33317167e-02  9.18279365e-02  2.29730025e-01 -1.06080465e-01\n",
            " -7.94381276e-03  1.69714838e-01  2.11173110e-03  2.61746377e-01\n",
            " -4.65259403e-02 -5.61432913e-04 -2.39933610e-01 -9.41652730e-02\n",
            " -8.17953199e-02  3.01916778e-01  4.45611123e-03  1.61169380e-01\n",
            " -1.45899147e-01 -1.21151380e-01  3.26537728e-01  1.09456480e-01\n",
            " -1.58697292e-01  1.32059038e-01 -3.15307319e-01  1.47429854e-03\n",
            " -1.41539991e-01 -1.95122167e-01 -5.65826595e-02 -1.32115453e-01]\n",
            "\n",
            "ID: 7dd66b3a-4e0b-4551-a48c-eac35102f123\n",
            "Documento: their symbolic reasoning ability [22], which generates interpretable results. KGs can also actively evolve with new knowledge contin- uously added in [24]. Additionally, experts can construct domain-specific KGs to provide precise and dependable domain-specific knowledge [23]. Nevertheless, KGs are difficult to construct [25], and current approaches in KGs [27], [33], [34] are inadequate in handling the incomplete and dynamically changing na- ture of real-world KGs. These approaches fail to effectively model unseen entities and represent new facts. In addition, they often ignore the abundant textual information in KGs. Moreover, existing methods in KGs are often customized for specific KGs or tasks, which are not generalizable enough. Therefore, it is also necessary to utilize LLMs to address the challenges faced in KGs. We summarize the pros and cons of LLMs and KGs in Fig. 1, respectively. Recently, the possibility of unifying LLMs with KGs has attracted increasing attention from researchers and practi- tioners. LLMs and KGs are inherently interconnected and can mutually enhance each other. In KG-enhanced LLMs , KGs can not only be incorporated into the pre-training and inference stages of LLMs to provide external knowledge [35]–[37], but also used for analyzing LLMs and provid- ing interpretability [14], [38], [39]. In LLM-augmented KGs , LLMs have been used in various KG-related tasks, e.g., KG embedding [40], KG completion [26], KG construction [41], KG-to-text generation [42], and KGQA [43], to improve the performance and facilitate the application of KGs. In Syn- ergized LLM + KG , researchers marries the merits of LLMsand KGs to mutually enhance performance in knowledge representation [44] and reasoning [45], [46]. Although there are some surveys on knowledge-enhanced LLMs [47]–[49], which mainly focus on using KGs as an external knowledge to enhance LLMs, they ignore other possibilities of integrat- ing KGs for LLMs and the potential role of LLMs in KG\n",
            "Embedding: [-5.85912406e-01  7.44541958e-02 -1.68816954e-01 -1.13324091e-01\n",
            " -4.48537618e-01 -3.80846530e-01  8.06422979e-02  1.57486387e-02\n",
            "  1.46702200e-01 -7.58289248e-02  5.07967127e-03  3.08725536e-02\n",
            "  1.15418173e-01 -4.50884819e-01 -2.58474320e-01 -7.45372400e-02\n",
            "  4.93298098e-02  9.66923535e-02 -7.78546691e-01 -1.83507845e-01\n",
            " -2.10270584e-01  1.06039301e-01 -1.17661081e-01  8.10913891e-02\n",
            " -5.64668924e-02 -2.28287309e-01  1.27541751e-01  8.99461508e-02\n",
            "  3.47203523e-01 -2.92136192e-01  5.20307347e-02  2.82060236e-01\n",
            "  9.06253308e-02  1.19356975e-01 -2.03407586e-01  1.19371764e-01\n",
            " -2.69051909e-01  1.89341038e-01 -3.97489741e-02  9.04629659e-03\n",
            "  1.70204327e-01  7.74324685e-02  2.09892735e-01  4.94212896e-01\n",
            "  1.47327542e-01  9.94772464e-02 -1.15414716e-01  3.45071882e-01\n",
            " -4.57045138e-01  1.48013636e-01 -3.21376085e-01  1.96100131e-01\n",
            " -2.05142453e-01 -9.87045318e-02 -1.63654640e-01 -1.33809149e-01\n",
            " -7.84059055e-03  2.68401265e-01  4.41958532e-02 -3.14662427e-01\n",
            "  6.64552510e-01 -1.76476061e-01 -6.63726851e-02  1.32162981e-02\n",
            "  3.31006557e-01  1.14956014e-01  4.02560860e-01 -3.13497305e-01\n",
            "  1.15119971e-01  4.65893805e-01  3.43795210e-01  3.56197178e-01\n",
            " -2.64245033e-01  2.87996471e-01  7.62833208e-02  1.86408490e-01\n",
            " -2.07716271e-01  4.94031645e-02  2.05174878e-01 -1.10472925e-01\n",
            " -1.20906219e-01  1.39230475e-01  1.71288848e-01  1.80918902e-01\n",
            "  8.60623270e-03 -2.02517048e-01  2.69081146e-02  8.68760794e-03\n",
            " -2.63737161e-02  5.63461520e-02 -5.68381362e-02 -4.27318633e-01\n",
            "  1.91081181e-01  4.40435037e-02 -9.87372771e-02  4.50863570e-01\n",
            " -1.45917207e-01 -1.34564623e-01  8.32866132e-03 -4.42545768e-03\n",
            " -4.63317454e-01  2.18279123e-01 -8.92160684e-02  2.49053881e-01\n",
            " -1.48067132e-01 -7.85953283e-01  1.36574451e-02  3.20410520e-01\n",
            "  1.38482273e-01  5.73211089e-02  2.28625815e-03  1.00861683e-01\n",
            "  4.29406911e-02 -5.49049862e-03 -1.34280682e-01 -2.17476428e-01\n",
            "  1.88990116e-01  5.43383658e-02 -3.62338960e-01 -6.35425299e-02\n",
            " -8.89836550e-02 -7.98426867e-02  1.82020962e-02 -8.71966034e-02\n",
            "  3.47228274e-02  2.62648791e-01 -5.23108840e-01 -1.18283294e-02\n",
            " -2.74199784e-01  2.76800036e-01  4.75586593e-01  3.41269702e-01\n",
            " -1.05248496e-01 -2.41836131e-01  1.61167234e-04 -6.98448271e-02\n",
            " -1.26553476e-01 -6.50327429e-02 -3.51180770e-02  3.65005404e-01\n",
            "  1.45378694e-01 -1.51754364e-01  1.44182876e-01  2.83858597e-01\n",
            "  3.52789402e-01 -9.11743343e-02  2.14777038e-01 -1.95724159e-01\n",
            " -1.38181113e-02 -1.08838506e-01  1.82230830e-01 -4.77700055e-01\n",
            " -2.47847974e-01 -5.30485883e-02  2.64444441e-01 -5.26366569e-02\n",
            " -2.94388175e-01 -6.08127266e-02 -1.15130112e-01 -1.80350915e-01\n",
            " -3.17550868e-01  2.12386191e-01 -1.03562579e-01 -1.15186244e-01\n",
            "  6.21236973e-02 -6.09349795e-02  4.27321792e-01 -3.20286989e-01\n",
            "  1.44070983e-01  5.02524637e-02  1.42191127e-01 -1.54576197e-01\n",
            "  7.49367196e-03  2.96510249e-01  1.93046585e-01 -1.58711877e-02\n",
            " -1.03189759e-01 -2.15955347e-01 -1.40441488e-02  3.01510870e-01\n",
            "  4.80285376e-01 -2.53966063e-01  3.09291929e-01  9.55748856e-02\n",
            "  5.04943132e-02 -1.21650882e-02 -1.44014791e-01  3.01610827e-01\n",
            " -2.22248167e-01  3.49587262e-01 -6.28132932e-03  5.72450161e-01\n",
            " -1.87184513e-01  1.05249025e-02 -2.81240135e-01 -4.90645140e-01\n",
            "  1.29846528e-01 -3.29195380e-01  1.86766312e-01  1.60174131e-01\n",
            "  7.39029050e-03  1.93264022e-01 -8.98119807e-02 -3.73130113e-01\n",
            " -5.76966107e-02 -2.62273252e-01 -2.67615288e-01  5.74040487e-02\n",
            "  1.58604428e-01  1.15675807e-01 -3.78551126e-01 -7.90532380e-02\n",
            " -3.19725633e-01 -1.21317923e-01 -1.41587779e-01 -5.25435135e-02\n",
            "  5.20116746e-01  2.03084394e-01 -5.15398756e-02  2.22046047e-01\n",
            " -2.00797155e-01 -3.55743058e-02 -8.53642449e-02 -2.47016296e-01\n",
            " -2.08085477e-01 -1.71043631e-02 -2.29119346e-01  1.61521479e-01\n",
            " -1.08171560e-01 -1.44147590e-01 -1.65677458e-01 -2.65942395e-01\n",
            "  3.48421574e-01 -1.04470722e-01  6.02774993e-02  1.75745547e-01\n",
            " -1.36862949e-01 -2.96421677e-01 -1.11246124e-01 -1.52647749e-01\n",
            " -2.58272141e-01 -3.01241875e-01  8.25273991e-02 -2.44395077e-01\n",
            "  9.50776637e-02 -1.10995486e-01 -6.74873471e-01  2.17283845e-01\n",
            "  1.64438281e-02  2.59965986e-01 -3.71442437e-01  3.20844352e-01\n",
            " -1.61152333e-01  1.66508500e-02 -5.37154861e-02 -2.83407196e-02\n",
            "  1.04602300e-01  5.21413386e-02 -1.59728467e-01 -1.55692875e-01\n",
            "  1.94180369e-01 -5.86111657e-02 -1.11220457e-01  1.19673740e-02\n",
            "  7.09205121e-02 -2.67842531e-01  5.09159975e-02 -2.02023000e-01\n",
            "  2.57918537e-01  1.07687838e-01  5.46877384e-01 -4.10248607e-01\n",
            "  4.95245039e-01 -1.70610830e-01  5.68904459e-01  4.07651858e-03\n",
            "  1.24658853e-01  1.19595945e-01 -1.58841640e-01  1.51414454e-01\n",
            "  3.23931202e-02 -2.55755395e-01  1.06721133e-01 -2.03912240e-02\n",
            "  4.91198450e-02 -5.39039783e-02  2.03480646e-01  2.33559921e-01\n",
            " -3.84912550e-01 -2.99168736e-01  1.25156632e-02  2.27948859e-01\n",
            " -1.98439702e-01 -1.02861412e-01  7.52891004e-02 -1.74600080e-01\n",
            "  7.53809363e-02 -2.27944568e-01  1.96498916e-01  1.28115341e-01\n",
            "  2.03851268e-01  1.49697274e-01 -1.49316527e-03 -6.48281500e-02\n",
            "  1.69675667e-02 -5.83525300e-02  2.74878144e-01 -2.47302800e-02\n",
            "  3.67121369e-01  2.30965078e-01  7.67120346e-02 -9.06128436e-04\n",
            "  6.76138178e-02  3.86143118e-01 -2.36539096e-01  8.03888440e-02\n",
            " -3.33818883e-01  3.72090816e-01  7.38133863e-02  6.84494525e-02\n",
            "  9.75731909e-02 -3.72448921e-01 -9.97411534e-02  2.92461634e-01\n",
            " -1.44565195e-01 -7.00073177e-03 -2.84754653e-02  1.31180599e-01\n",
            " -2.31991589e-01  7.96936750e-02 -2.34111935e-01  2.88836416e-02\n",
            "  7.67678767e-02  3.33293021e-01  6.79648444e-02  2.88260937e-01\n",
            " -2.26165339e-01  5.93113825e-02 -2.85294831e-01  9.41225737e-02\n",
            "  7.18274191e-02  2.70091712e-01  5.08769929e-01 -3.63050066e-02\n",
            " -1.44107535e-01 -2.67241716e-01 -2.14244202e-02  6.56036958e-02\n",
            "  1.74033761e-01  6.99366093e-01 -2.21290529e-01  6.24699071e-02\n",
            "  2.71879792e-01 -2.10535154e-03  1.75457448e-02 -4.74902421e-01\n",
            " -2.17134088e-01  3.42116505e-01  2.90305614e-01  1.40502602e-01\n",
            " -2.70353109e-01  7.72306472e-02 -1.19580112e-01  4.84038413e-01\n",
            "  1.45888835e-01 -5.22580892e-02 -5.93916364e-02  2.31044739e-02\n",
            "  8.10137689e-02  4.25566807e-02  9.65826586e-02  1.92392558e-01\n",
            " -7.46321753e-02 -8.47948194e-02  1.31107241e-01  1.78883433e-01\n",
            " -4.97749299e-02 -1.45316854e-01 -8.20515379e-02  1.12315580e-01\n",
            "  2.38830037e-02 -2.93562353e-01 -4.06519063e-02 -2.09300876e-01]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Indicamos los elementos a obtener obtener\n",
        "db_data = vectorstore.get(include=['documents', 'embeddings'])\n",
        "# Número de elementos a mostrar\n",
        "N = 5\n",
        "# Extraer las listas de cada clave\n",
        "ids = db_data['ids']\n",
        "docs = db_data['documents']\n",
        "embs = db_data['embeddings']\n",
        "\n",
        "# Zipear las listas y mostrar los primeros N elementos\n",
        "for i, (id_, d, e) in enumerate(zip(ids, docs, embs)):\n",
        "    if i >= N:\n",
        "        break\n",
        "    print(f\"ID: {id_}\\nDocumento: {d}\\nEmbedding: {e}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEugq7VRkq_K"
      },
      "source": [
        "# 2. Recuperación de Información"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cuy5Im4ZkpkD"
      },
      "source": [
        "Crearemos un recuperador de información (`retrieval`).\n",
        "\n",
        "La función *vectorstore.as_retriever()* permite crear una interfaz sobre la base de datos vectorial que usaremos para realizar búsquedas semánticas y obtener los documentos más relevantes a una consulta.\n",
        "\n",
        "- Por defecto se aplica la búsqueda por similitud (*search_type=\"similarity\"*) y recupera los K=4 documentos más relevantes (*search_kwargs={\"k\": N}*).\n",
        "Estos parámetros se pueden modificar, usando \"mmr\" (Maximal Marginal Relevance, que permite obtener resultados más diversos)  así como establecer un N mayor a 0 (para recuperar más o menos documentos)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ez5J6AAnmWRW"
      },
      "outputs": [],
      "source": [
        "# Crear el retrieval\n",
        "retriever = vectorstore.as_retriever()\n",
        "  # search_type es como va a ser la búsqueda\n",
        "    # por defecto:\n",
        "    #   search_type=\"similarity\" -> búsqueda por similitud de vectores (coseno)\n",
        "    #   search_kwargs={\"k\": 4} TOP K para la búsqueda de similitud\n",
        "  # Otros valores:\n",
        "  #   search_type=\"mmr\" Maximal Marginal Relevance -> resultados sean más diversos\n",
        "  #   Variar el TOP K para la búsqueda de similitud"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yHE1y5mkN_j"
      },
      "source": [
        "# 3. Generación aumentada"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvOJWnpOkPyM"
      },
      "source": [
        "### Definción del Sistema QA\n",
        "Crearemos un Sistema de QA usando *Gemini*.\n",
        "\n",
        "Usaremos la versión *gemini-pro*, pero también puedes utilizar *gemini-1.5-pro* el cual es más poderoso. Notar que utilizamos nuestra API Key.\n",
        "\n",
        "Intentaremos que nuestro modelo sea mayormente determinístico. Dos parámetros que ayudan a lograr este comportamiento son:\n",
        "\n",
        "- temperatura (*temperature*): valor entre 0 y 1. Próximo a 1 es más creativo en sus respuestas, mientras que más cerca de 0 es más conservador.\n",
        "- *top_p*: probabilidad que controla la diversidad de las palabras generadas. Próximo a 1 habrá más posibles palabras a considerar, mientras que cerca de 0 habrá menos diversidad.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVZKSUnQy72P"
      },
      "outputs": [],
      "source": [
        "# Crear el LLM para responder las preguntas\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", google_api_key=GOOGLE_API_KEY, temperature=0.15, top_p=0.15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZmQ8DFWkcqw"
      },
      "source": [
        "Construimos el *Prompt*, que es una serie de instrucciones que le daremos al LLM para resolver la tarea.\n",
        "\n",
        "Indicaremos al LLM que debe responder consultas, considerando específicamente el *contexto* formado por los documentos más relaciodados con la consulta.\n",
        "\n",
        "Esta implementación de la técnica de RAG permite adaptar al modelo para generar respuestas fundamentadas y basadas en información relevante, mejorando así la calidad y precisión de las respuestas obtenidas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0THsZHqgzqFJ",
        "outputId": "bb88785d-0aae-4828-e4e1-b96127885442"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='Eres un asistente para tareas de question-answering. Usa el siguiente contexto para responder la solictud.Puedes explicar tu respuesta.Pregunta {question}\\nContexto: {context}\\nRespuesta:'), additional_kwargs={})])"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "template = (\"Eres un asistente para tareas de question-answering. Usa el siguiente contexto para responder la solictud.\"\n",
        "          \"Puedes explicar tu respuesta.\" # Además, debes devolver la información de contexto que usaste para devolver la respuesta.\"\n",
        "          \"Pregunta {question}\\n\"\n",
        "          \"Contexto: {context}\\n\"\n",
        "          \"Respuesta:\")\n",
        "\n",
        "# Definir tu prompt como un template de chat\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hlc-pVi4lbXa"
      },
      "source": [
        "Implementamos un Pipeline: secuencia ordenada de operaciones, donde el resultado de una se utiliza en la siguiente operación.\n",
        "\n",
        "1. Crear el contexto usando:\n",
        "-   `retriever` el cual pasa a: `format_docs` para unir los documentos más relevantes en una única pieza de texto.\n",
        "-   El contenido de `question` pasa sin modificaciones (`RunnablePassthrough`).\n",
        "\n",
        "2. Crear un diccionario (`context` y `question`) el cual pasa a: `prompt`.\n",
        "3. Construir el `prompt` el cual pasa a: `LLM` (Aquí sucede el RAG).\n",
        "4. `LLM` genera la respuesta y pasa a: `StrOutputParser` el cual parsea el resultado en formato estándar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17mJyr47zy4W"
      },
      "outputs": [],
      "source": [
        "def format_docs(docs):\n",
        "    # Funcion auxiliar para enviar el contexto al modelo como parte del prompt\n",
        "    return \"\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUwIroK9lj8t"
      },
      "source": [
        "# **Ahora sí, llegó el momento de hacer preguntas al LLM usando RAG**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "id": "4bltsgjx0BSL",
        "outputId": "d041fa6e-5efc-4a76-a681-5b9a3b4e2025"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Respuesta del modelo:\n"
          ]
        },
        {
          "data": {
            "text/markdown": "**Ventajas de los KGs:**\n\n* **Conocimiento estructural:** Los KGs almacenan hechos en un formato estructurado (es decir, triples), que pueden ser entendidos tanto por humanos como por máquinas.\n* **Precisión:** Los hechos en los KGs suelen ser curados o validados manualmente por expertos, lo que los hace más precisos y fiables que los de los LLM.\n* **Decisión:** El conocimiento fáctico en los KGs se almacena de forma decisiva. El algoritmo de razonamiento en los KGs también es determinista, lo que puede proporcionar resultados decisivos.\n* **Interpretabilidad:** Los KGs son conocidos por su capacidad de razonamiento simbólico, que proporciona un proceso de razonamiento interpretable que puede ser entendido por los humanos.\n* **Conocimiento específico del dominio:** Muchos dominios pueden construir sus KGs por expertos para proporcionar un conocimiento preciso y fiable específico del dominio.\n* **Conocimiento en evolución:** Los hechos en los KGs están en continua evolución. Los KGs pueden actualizarse con nuevos hechos insertando nuevos triples y eliminando los obsoletos.\n\n**Desventajas de los KGs:**\n\n* **Incompletez:** Los KGs son difíciles de construir y a menudo están incompletos, lo que limita la capacidad de los KGs para proporcionar un conocimiento completo.\n* **Falta de comprensión del lenguaje:** La mayoría de los estudios sobre KGs modelan la estructura del conocimiento, pero ignoran la información textual en los KGs. La información textual en los KGs suele ignorarse en las tareas relacionadas con los KGs, como la finalización de los KGs y el KGQA.\n* **Hechos no vistos:** Los KGs cambian dinámicamente, lo que dificulta el modelado de entidades no vistas y la representación de nuevos hechos.",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Preguntas a la Base de Conocimiento\n",
        "pregunta = \"Que ventajas y desventajas tienen los kgs?\" # @param {type:\"string\"}\n",
        "response = rag_chain.invoke(pregunta)\n",
        "\n",
        "print(\"Respuesta del modelo:\")\n",
        "Markdown(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mUodZQkl2ay"
      },
      "source": [
        "Verifiquemos los documentos más relevantes (chunks) que usó el modelo para generar la respuesta.\n",
        "\n",
        "- Puedes buscar estos fragmentos en el documento original"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RnEdcurW29mF",
        "outputId": "f837c840-088b-465b-a6f7-2f9aa7769cd1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chunk utilizados:\n",
            "Chunk 1:\n",
            "they can mutually validate each other, resulting in efficient and effective solutions powered by dual-driving wheels. There- fore, we can anticipate increasing attention to unlock the po- tential of integrating KGs and LLMs for diverse downstream applications with both generative and reasoning capabilities in the near future. 8 C ONCLUSION Unifying large language models (LLMs) and knowledge graphs (KGs) is an active research direction that has at- tracted increasing attention from both academia and in- dustry. In this article, we provide a thorough overview of the recent research in this field. We first introduce different manners that integrate KGs to enhance LLMs. Then, we introduce existing methods that apply LLMs for KGs and establish taxonomy based on varieties of KG tasks. Finally, we discuss the challenges and future directions in this field.We envision that there will be multiple stages (milestones) in the roadmap of unifying KGs and LLMs, as shown in Fig. 26. In particular, we will anticipate increasing research on three stages: Stage 1 : KG-enhanced LLMs, LLM-augmented KGs, Stage 2: Synergized LLMs + KGs, and Stage 3: Graph Structure Understanding, Multi-modality, Knowledge Up- dating. We hope that this article will provide a guideline to advance future research. ACKNOWLEDGMENTS This research was supported by the Australian Research Council (ARC) under grants FT210100097 and DP240101547 and the National Natural Science Foundation of China (NSFC) under grant 62120106008. REFERENCES [1] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre- training of deep bidirectional transformers for language under- standing,” arXiv preprint arXiv:1810.04805 , 2018. [2] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V . Stoyanov, “Roberta: A ro- bustly optimized bert pretraining approach,” arXiv preprint arXiv:1907.11692 , 2019. [3] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P .\n",
            "1999\n",
            "Chunk 2:\n",
            "they ignore other possibilities of integrat- ing KGs for LLMs and the potential role of LLMs in KG applications. In this article, we present a forward-looking roadmap for unifying both LLMs and KGs, to leverage their respective strengths and overcome the limitations of each approach, for various downstream tasks. We propose detailed cate- gorization, conduct comprehensive reviews, and pinpoint emerging directions in these fast-growing fields. Our main contributions are summarized as follows: 1)Roadmap. We present a forward-looking roadmap for integrating LLMs and KGs. Our roadmap, consisting of three general frameworks to unify LLMs and KGs, namely, KG-enhanced LLMs ,LLM- augmented KGs , and Synergized LLMs + KGs , pro- vides guidelines for the unification of these two distinct but complementary technologies. 2)Categorization and review. For each integration framework of our roadmap, we present a detailed categorization and novel taxonomies of research on unifying LLMs and KGs. In each category, we review the research from the perspectives of differ- ent integration strategies and tasks, which provides more insights into each framework. 3)Coverage of emerging advances. We cover the advanced techniques in both LLMs and KGs. We include the discussion of state-of-the-art LLMs like ChatGPT and GPT-4 as well as the novel KGs e.g., multi-modal knowledge graphs. 4)Summary of challenges and future directions. We highlight the challenges in existing research and present several promising future research direc- tions. The rest of this article is organized as follows. Section 2 first explains the background of LLMs and KGs. Section 3 introduces the roadmap and the overall categorization of this article. Section 4 presents the different KGs-enhanced LLM approaches. Section 5 describes the possible LLM- augmented KG methods. Section 6 shows the approaches of synergizing LLMs and KGs. Section 7 discusses the challenges and future research directions. Finally, Section 8 concludes\n",
            "1999\n",
            "Chunk 3:\n",
            "JOURNAL OF LATEX CLASS FILES, VOL. ??, NO. ??, MONTH 20YY 28 KG pros. •Structural Knowledge [19]: KGs store facts in a struc- tural format (i.e., triples), which can be understand- able by both humans and machines. •Accuracy [20]: Facts in KGs are usually manually curated or validated by experts, which are more accurate and dependable than those in LLMs. •Decisiveness [21]: The factual knowledge in KGs is stored in a decisive manner. The reasoning algorithm in KGs is also deterministic, which can provide deci- sive results. •Interpretability [22]: KGs are renowned for their sym- bolic reasoning ability, which provides an inter- pretable reasoning process that can be understood by humans. •Domain-specific Knowledge [23]: Many domains can construct their KGs by experts to provide precise and dependable domain-specific knowledge.•Evolving Knowledge [24]: The facts in KGs are contin- uously evolving. The KGs can be updated with new facts by inserting new triples and deleting outdated ones. KG cons. •Incompleteness [25]: KGs are hard to construct and often incomplete, which limits the ability of KGs to provide comprehensive knowledge. •Lacking Language Understanding [33]: Most studies on KGs model the structure of knowledge, but ignore the textual information in KGs. The textual informa- tion in KGs is often ignored in KG-related tasks, such as KG completion [26] and KGQA [43]. •Unseen Facts [27]: KGs are dynamically changing, which makes it difficult to model unseen entities and represent new facts.\n",
            "1520\n",
            "Chunk 4:\n",
            "their symbolic reasoning ability [22], which generates interpretable results. KGs can also actively evolve with new knowledge contin- uously added in [24]. Additionally, experts can construct domain-specific KGs to provide precise and dependable domain-specific knowledge [23]. Nevertheless, KGs are difficult to construct [25], and current approaches in KGs [27], [33], [34] are inadequate in handling the incomplete and dynamically changing na- ture of real-world KGs. These approaches fail to effectively model unseen entities and represent new facts. In addition, they often ignore the abundant textual information in KGs. Moreover, existing methods in KGs are often customized for specific KGs or tasks, which are not generalizable enough. Therefore, it is also necessary to utilize LLMs to address the challenges faced in KGs. We summarize the pros and cons of LLMs and KGs in Fig. 1, respectively. Recently, the possibility of unifying LLMs with KGs has attracted increasing attention from researchers and practi- tioners. LLMs and KGs are inherently interconnected and can mutually enhance each other. In KG-enhanced LLMs , KGs can not only be incorporated into the pre-training and inference stages of LLMs to provide external knowledge [35]–[37], but also used for analyzing LLMs and provid- ing interpretability [14], [38], [39]. In LLM-augmented KGs , LLMs have been used in various KG-related tasks, e.g., KG embedding [40], KG completion [26], KG construction [41], KG-to-text generation [42], and KGQA [43], to improve the performance and facilitate the application of KGs. In Syn- ergized LLM + KG , researchers marries the merits of LLMsand KGs to mutually enhance performance in knowledge representation [44] and reasoning [45], [46]. Although there are some surveys on knowledge-enhanced LLMs [47]–[49], which mainly focus on using KGs as an external knowledge to enhance LLMs, they ignore other possibilities of integrat- ing KGs for LLMs and the potential role of LLMs in KG\n",
            "1995\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-19-fcc82591d534>:2: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  results = retriever.get_relevant_documents(pregunta)\n"
          ]
        }
      ],
      "source": [
        "# Buscar los documentos más relevantes para una pregunta con el retriever\n",
        "results = retriever.get_relevant_documents(pregunta)\n",
        "# Almacenar los splits usados\n",
        "splits_usados = [doc.page_content for doc in results]\n",
        "# display(splits_usados)\n",
        "print(\"Chunk utilizados:\")\n",
        "for i, split in enumerate(splits_usados):\n",
        "    print(f\"Chunk {i+1}:\")\n",
        "    print(split)\n",
        "    print(len(split))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJlpN5rCmBho"
      },
      "source": [
        "# **Hacer preguntas al LLM sin usar RAG**\n",
        "Examinemos qué respondería el modelo sin un contexto, es decir, usando sólo el conocimiento aprendido durante su entrenamiento.\n",
        "\n",
        "Puedes evaluar si el modeo genera *alucinaciones* según tu criterio e interpretación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dddLoZ5BuZdT"
      },
      "outputs": [],
      "source": [
        "noRAG_template = (\n",
        "    \"Eres un asistente para tareas de question-answering.\"\n",
        "    \"ebes responder a la pregunta según tu conocimiento.\\n\"\n",
        "    \"Pregunta: {question}.\\n\"\n",
        "    \"Respuesta:\"\n",
        ")\n",
        "noRAG_prompt = ChatPromptTemplate.from_template(noRAG_template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "rL5AwkVM9pfU",
        "outputId": "06b321a1-4e7d-412d-d788-82c5c95c6f70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Respuesta del modelo sin RAG:\n"
          ]
        },
        {
          "data": {
            "text/markdown": "**Ventajas de los kilogramos (kg):**\n\n* **Estándar internacional:** El kilogramo es la unidad base de masa en el Sistema Internacional de Unidades (SI), lo que lo convierte en un estándar reconocido internacionalmente.\n* **Precisión:** El kilogramo se define con una precisión extremadamente alta, lo que lo hace adecuado para mediciones científicas y técnicas.\n* **Fácil de usar:** Los kilogramos son una unidad familiar y fácil de entender, lo que los hace convenientes para el uso diario.\n* **Ampliamente utilizado:** Los kilogramos se utilizan en una amplia gama de aplicaciones, desde el comercio hasta la medicina y la ingeniería.\n\n**Desventajas de los kilogramos:**\n\n* **Dependencia de un artefacto físico:** El kilogramo se define en relación con un artefacto físico específico, el Prototipo Internacional del Kilogramo (IPK), que se almacena en Francia. Esto introduce una posible fuente de error si el IPK se daña o se pierde.\n* **Posible deriva:** Se ha observado que el IPK ha perdido masa con el tiempo, lo que plantea preocupaciones sobre la estabilidad de la definición del kilogramo.\n* **Limitaciones en la medición:** Los kilogramos son difíciles de medir con precisión en escalas pequeñas, lo que puede ser un inconveniente para ciertas aplicaciones.\n* **Confusión con otras unidades:** Existen otras unidades de masa, como libras y onzas, que pueden causar confusión cuando se convierten a kilogramos.",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "noRAG_chain = (\n",
        "    {\"question\": RunnablePassthrough()}\n",
        "    | noRAG_prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "response = noRAG_chain.invoke(pregunta)\n",
        "\n",
        "print(\"Respuesta del modelo sin RAG:\")\n",
        "Markdown(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bk8bR1czmJ07"
      },
      "source": [
        "Como quizás hayas podido ver, el modelo devuelve una respuesta presumiblemente es errónea, incompleta o fuera de contexto.\n",
        "\n",
        "Considerar que *Gemini* fue entrenado con datos de Google e incluso está conectado al motor de búsqueda en tiempo real, por lo que es probable que tienda a generar respuestas basadas en información disponible en la Web."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_6-EFZgnSOg"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "# **Conclusión:**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adN6R4lMmmek"
      },
      "source": [
        "En esta notebook desarrollamos un sistema sencillo de Question-Answering para analizar documentos en formato PDF, utilizando LLMs e implementando la técnica RAG, con el objetivo de mitigar el fenómeno de las alucinaciones. Esto implicó la aplicación y comprensión de diversos conceptos, tales como recuperación de información, bases de datos vectoriales, embeddings, búsqueda semántica, métricas de similitud y prompting. Aplicando el modelo Gemini, observamos que es posible obtener respuestas justificadas considerando el contexto del documento en estudio. Además, mediante un ejemplo sencillo, confirmamos que sin RAG el modelo puede generar información coherente pero incorrecta, lo que podría confundir a la audiencia. Esto resalta la importancia de la técnica RAG, posicionándola como un enfoque interesante para abordar los diversos desafíos que implica el uso de los LLMs.\n",
        "\n",
        "<br>\n",
        "\n",
        "**Esperamos que te haya gustado y hayas aprendido algo nuevo! 🙂**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4zKfmbhkmuD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYvmZuxZ1MRd"
      },
      "source": [
        "# Bonus: obtener requirements.txt\n",
        "\n",
        "### Ejecuta la siguiente celda y guarda todas las dependencias.\n",
        "#### Esto es útil porque las librerías de conda y pip están en constante actualización, pudiendo quedar tu implementación obsoleta.\n",
        "\n",
        "*    Debes indicar en la lista (LIBRARIES_TO_SAVE) todas las librerias que utilices en esta notebook (\"import ...\", \"from ...\", etc.)\n",
        "*   Se creará un archivo *requirements.txt* que luego podrás utilizar para ejecutar esta notebook en cualquier momento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "6u4ZXJ9j1UG1",
        "outputId": "81d9189a-b123-46cc-c4f2-ffe72e3b54f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Check versions of libraries:\n",
            "chromadb==0.5.11\n",
            "google-generativeai==0.7.2\n",
            "langchain==0.3.1\n",
            "langchain-community==0.3.1\n",
            "langchain-google-genai==2.0.0\n",
            "langchainhub==0.1.21\n",
            "pypdf==5.0.1\n",
            "PyPDF2==3.0.1\n",
            "python-dotenv==1.0.1\n",
            "sentence-transformers==3.1.1\n",
            "Download file? (y/n): y\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "download(\"download_b6819498-ab05-47f8-8f00-d4de2061b2d2\", \"requirements.txt\", 215)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import os\n",
        "import subprocess\n",
        "from google.colab import files\n",
        "\"\"\"\n",
        "!pip install langchain\n",
        "!pip install google-generativeai langchain-google-genai\n",
        "!pip install chromadb pypdf2 python-dotenv\n",
        "!pip install PyPDF\n",
        "!pip install -U langchain-community\n",
        "!pip install sentence-transformers\n",
        "!pip install langchainhub\n",
        "\"\"\"\n",
        "\n",
        "# Agregar cada librería (ya deben estar importadas/instaladas)\n",
        "LIBRARIES_TO_SAVE = [\"langchain\", \"google-generativeai\", \"langchain-google-genai\",\n",
        "                     \"chromadb\", \"pypdf2\", \"python-dotenv\", \"PyPDF\", \"langchain-community\", \"sentence-transformers\", \"langchainhub\"]\n",
        "\n",
        "# Obtener versiones de las librerías\n",
        "out = subprocess.run([\"pip\", \"freeze\"], capture_output=True, text=True).stdout\n",
        "filter = [ line\n",
        "          for line in out.split('\\n')\n",
        "              if any(library.lower() == line.split(\"==\")[0].lower()\n",
        "                          for library in LIBRARIES_TO_SAVE  )\n",
        "        ]\n",
        "# Guardar requirements.txt (en entorno actual)\n",
        "with open(\"requirements.txt\", 'w') as f:\n",
        "  f.write(\"\\n\".join(filter))\n",
        "\n",
        "# Chequear requirements.txt\n",
        "print(\"Check versions of libraries:\")\n",
        "with open(\"requirements.txt\", 'r') as f:\n",
        "  print(f\"{f.read()}\")\n",
        "\n",
        "# req_file_path = \"/content\"\n",
        "# # Puedes guardar requirements.txt en tu directorio raíz\n",
        "# if input(f\"Save in: {req_file_path}? (y/n)\").lower() == 'y':\n",
        "#   output_path = os.path.join(req_file_path, \"requirements.txt\")\n",
        "#   with open(output_path, 'w') as f:\n",
        "#     f.write(\"\\n\".join(filter))\n",
        "#   print(\"> Saved!\", output_path)\n",
        "\n",
        "\n",
        "# Puedes descargar requirements.txt\n",
        "if input(\"Download file? (y/n): \").lower() == 'y':\n",
        "  files.download('requirements.txt')\n",
        "\n",
        "# Para instalar las librerías en tu nueva notebook puedes usar:\n",
        "# print(\"Instalando librerias...\")\n",
        "# !pip install -r requirements.txt\n",
        "# print(\"Reiniciar automáticamente la notebook para luego cargar las librerías instaladas\")\n",
        "# os._exit(00)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zn804zhSWJN9"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "SKoD_8jgnyE6",
        "ln8N7fidawat",
        "wTQRhS33gR4Z",
        "9bIxb-O1eg7D",
        "vcakUtOigg6A",
        "eVUGiN3ngj_T",
        "GsbFtWiuhpK5",
        "tEugq7VRkq_K",
        "9yHE1y5mkN_j",
        "nUwIroK9lj8t",
        "jJlpN5rCmBho",
        "L_6-EFZgnSOg",
        "adN6R4lMmmek",
        "CYvmZuxZ1MRd"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0631426403144f98bd75fcfef573e217": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ea48145d9a2f49c0ad56f99411d9c043",
            "max": 190,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_599144a6612642fb98dfe66d210c571b",
            "value": 190
          }
        },
        "070c9ddacac8462c85d2c9af6ae07b0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dd8ae2f7387d499cb22d7999e58e773f",
            "max": 466081,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3480bd6c704440feaf23aea2974ce263",
            "value": 466081
          }
        },
        "0909bd3a02334ebb9c937582ab58dbb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "094ee7646d86499983f4eef515f6c8ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0965899c059e4c119ed765609df5392b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a67cb9a26544b64a71ae32d07c13b7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0b4f48185dea46f5beb00314869eb15c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c8ccbb4cda746b29dfcfedc37a85399": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0f00ac0bf22b47f293e2c640c2a63364": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1cc487bffce94a148a223248509dcde9",
            "max": 314,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3125fdb19f04498fb8e8e63dd1d624b9",
            "value": 314
          }
        },
        "127076aed02d45e9a831961f4b865c44": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dd4d7eb9cb7247dfb3b3b50680690a4e",
              "IPY_MODEL_070c9ddacac8462c85d2c9af6ae07b0b",
              "IPY_MODEL_92406a75e32f4bb180076e706787f3a6"
            ],
            "layout": "IPY_MODEL_6070d82d367f436bbc4a50d750168f21"
          }
        },
        "18a3056a6ea64286826a3afc56add186": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1adf1c2e50c84847953aee88d5d7566e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1ba8972eb72c48b5bb88d57c7f17c53c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1cc487bffce94a148a223248509dcde9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20b54694d82a4f2e8b8344d6b4dece42": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f839cabfd8714719896ec63f5ec2c1a1",
            "placeholder": "​",
            "style": "IPY_MODEL_5deea9a58d9244a2ad5256df79aad61a",
            "value": "modules.json: 100%"
          }
        },
        "284e9ae9d8e74ae48f28bb20772e0c6a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "28ae155a4a7e4d63a3717d301543ce5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "299d0bd58ec74160859c7f653c8b60f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2b7df0309786450aa7026bc6748c6710": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_20b54694d82a4f2e8b8344d6b4dece42",
              "IPY_MODEL_afa886057f904497a8545425ccc9554b",
              "IPY_MODEL_528976674269476c9f785abc1d905deb"
            ],
            "layout": "IPY_MODEL_4723fc12c187413893cba9a36030b1b1"
          }
        },
        "2e48bed3ffa34fbcacc7dec59323b812": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_896e4cf7c31847d0a2d30da8177f1ec6",
            "placeholder": "​",
            "style": "IPY_MODEL_0c8ccbb4cda746b29dfcfedc37a85399",
            "value": " 53.0/53.0 [00:00&lt;00:00, 2.25kB/s]"
          }
        },
        "2e8d61bc2efc4dfe845141b1fbb22d50": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3125fdb19f04498fb8e8e63dd1d624b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3480bd6c704440feaf23aea2974ce263": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3851b7a2ceaa4c3c8609b6744187025a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39ae3c88984545e58886c565d20b1df6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b0808a452904663a73ada839caf0984": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3dbb2b6833a14fb684a9d679a46edd83": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4118a73b2ec640ce84d6ddd753b15618": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c34c4a3d6a5a4efc9b4a100ca7130c0c",
            "placeholder": "​",
            "style": "IPY_MODEL_61efc3e2bf0142eaa07adb0fd94e6d24",
            "value": " 314/314 [00:00&lt;00:00, 17.0kB/s]"
          }
        },
        "43a31d5fe545438b97dc1c1300d19d9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c81bf00fcbfa42eeb5a52f9520bf0532",
              "IPY_MODEL_aa10ca246aeb481ea16c996ca75c5745",
              "IPY_MODEL_962eccad21a44bc6b2d6c0f9418a1ec0"
            ],
            "layout": "IPY_MODEL_60bd72fecf034ae5a347fc59043962ad"
          }
        },
        "4513a31307844856a29c9b128586c73a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45f46296b31d4e4899703b766ae79c1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "463c3f9642fb42b28e12469a64233605": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7fdf6f6ad15445658163698fd32e292c",
              "IPY_MODEL_b1d29366256949b2932638955c135311",
              "IPY_MODEL_fdcd675e07bb400e91aa021dd5be9a98"
            ],
            "layout": "IPY_MODEL_715290fe97494a14b1fd57af36d1de95"
          }
        },
        "46e36b3cae2343d5993716151d3990ff": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4723fc12c187413893cba9a36030b1b1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48287d97529b4cf09d62261b91ee94a4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49766cd63f1b4fa9b495b1f1d0144bf9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49e3fa3e17d54a31adf2d47741b5c3ab": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c84fa8c0bc349cebdea3e9401898ad2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0965899c059e4c119ed765609df5392b",
            "max": 3729,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_45f46296b31d4e4899703b766ae79c1b",
            "value": 3729
          }
        },
        "528976674269476c9f785abc1d905deb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_96ab33fef38140a4b591d6a3e9000984",
            "placeholder": "​",
            "style": "IPY_MODEL_5b676b69524d4612b3e99e0769b4805f",
            "value": " 229/229 [00:00&lt;00:00, 13.8kB/s]"
          }
        },
        "599144a6612642fb98dfe66d210c571b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5b676b69524d4612b3e99e0769b4805f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5deea9a58d9244a2ad5256df79aad61a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5ecac8bb08d647369b934cedeb7165b6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6070d82d367f436bbc4a50d750168f21": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60bd72fecf034ae5a347fc59043962ad": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61efc3e2bf0142eaa07adb0fd94e6d24": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "632d369a72c94f3ba73080f3851ae559": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f9984cf059943b3a5e75eae1858d0ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d6508d745c3e4bc2a244f67a6027a631",
            "placeholder": "​",
            "style": "IPY_MODEL_f2917330828b49dd8a5197515969c663",
            "value": "config.json: 100%"
          }
        },
        "715290fe97494a14b1fd57af36d1de95": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73dcddfce36246d888f05cd3e76cddd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d4a0d6c3fad64f6c913c2c7e7060ca4d",
              "IPY_MODEL_a94f5a15af6b41da836f4a1483070a9c",
              "IPY_MODEL_917ad605a4a74b998868722a043f8ef0"
            ],
            "layout": "IPY_MODEL_955e69c14d754139a6041b7b0fdaa935"
          }
        },
        "781e273cd4be47fbb9b47ee2de35427b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d639022bcc384ad88a9c2e0d2f2bf11c",
            "max": 53,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fa26418b38124340b93936c17cd2be5d",
            "value": 53
          }
        },
        "7ac12e8d5c254dfd9979ca6c4bdebdd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6f9984cf059943b3a5e75eae1858d0ce",
              "IPY_MODEL_f20d9cf4ccf54243a29f87402f7a166d",
              "IPY_MODEL_e9faebfd14e3495fbfeebdeaa356f047"
            ],
            "layout": "IPY_MODEL_9d37f3e7718b4c0d84cd7d15d547e459"
          }
        },
        "7fdf6f6ad15445658163698fd32e292c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fe76081e53fb40dabf2198815fd14e61",
            "placeholder": "​",
            "style": "IPY_MODEL_80a8ec9b6b4d4686bf9f2a7874ff49d7",
            "value": "config_sentence_transformers.json: 100%"
          }
        },
        "80a8ec9b6b4d4686bf9f2a7874ff49d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "825509b73b8f47a994d69e083f8322ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "827d64cdb37d4cbeb62f0795a8cdc978": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "86e71df5d86448e68cced6df2d17c395": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8832d707b4e04bb9ad36ae24f9c7e7e4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "896e4cf7c31847d0a2d30da8177f1ec6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d16a6f846a5411fbd9e9576b145496a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8d610311f26b4c6ea90c0063047b4e22": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8de8e2973a9c469bbc193fee8ad2cede": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8ec5c4019969406dba9ddeba56ebb33f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "917ad605a4a74b998868722a043f8ef0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39ae3c88984545e58886c565d20b1df6",
            "placeholder": "​",
            "style": "IPY_MODEL_af6dad1da929414581a8bcc32ae11968",
            "value": " 232k/232k [00:00&lt;00:00, 8.89MB/s]"
          }
        },
        "92406a75e32f4bb180076e706787f3a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aaca50a213074637af60d052b9581fa2",
            "placeholder": "​",
            "style": "IPY_MODEL_a8925c0da5a540589a2229b991df63c5",
            "value": " 466k/466k [00:00&lt;00:00, 3.59MB/s]"
          }
        },
        "955e69c14d754139a6041b7b0fdaa935": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "962eccad21a44bc6b2d6c0f9418a1ec0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_284e9ae9d8e74ae48f28bb20772e0c6a",
            "placeholder": "​",
            "style": "IPY_MODEL_cb7b9dc04e8b4cd1ae8776e5e907725b",
            "value": " 90.9M/90.9M [00:02&lt;00:00, 64.1MB/s]"
          }
        },
        "96ab33fef38140a4b591d6a3e9000984": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9775c53802844b03ae72e453b49a9a38": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97cba74d8f294ba5abd5470038ddb6a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dcbbc4c11b2041bda4a07bfcc5e8289c",
            "placeholder": "​",
            "style": "IPY_MODEL_28ae155a4a7e4d63a3717d301543ce5e",
            "value": "sentence_bert_config.json: 100%"
          }
        },
        "9ad9cbbd434249ea8b405de17cc380ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_18a3056a6ea64286826a3afc56add186",
            "placeholder": "​",
            "style": "IPY_MODEL_2e8d61bc2efc4dfe845141b1fbb22d50",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "9d37f3e7718b4c0d84cd7d15d547e459": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4ae9bdde9664a6abda8da1a81007018": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c2690ecb2344402493df7eeb8c7ab73f",
              "IPY_MODEL_0631426403144f98bd75fcfef573e217",
              "IPY_MODEL_f71b45218df34ca48607e5737a7648f5"
            ],
            "layout": "IPY_MODEL_3dbb2b6833a14fb684a9d679a46edd83"
          }
        },
        "a6256613007a4e2e8e7542e1fbc8bd53": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f9ca5420060d4dcf9bf0dfb61cb73d45",
            "placeholder": "​",
            "style": "IPY_MODEL_8de8e2973a9c469bbc193fee8ad2cede",
            "value": " 112/112 [00:00&lt;00:00, 1.47kB/s]"
          }
        },
        "a65eca6af86d4706bd6cbc90858b80f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8832d707b4e04bb9ad36ae24f9c7e7e4",
            "placeholder": "​",
            "style": "IPY_MODEL_8d16a6f846a5411fbd9e9576b145496a",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "a8925c0da5a540589a2229b991df63c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a90f3ade3a414b04b9e0cd0cb5900e3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_97cba74d8f294ba5abd5470038ddb6a7",
              "IPY_MODEL_781e273cd4be47fbb9b47ee2de35427b",
              "IPY_MODEL_2e48bed3ffa34fbcacc7dec59323b812"
            ],
            "layout": "IPY_MODEL_dec19e176391414ea4ab2ccc5f2d463b"
          }
        },
        "a94f5a15af6b41da836f4a1483070a9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4513a31307844856a29c9b128586c73a",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1ba8972eb72c48b5bb88d57c7f17c53c",
            "value": 231508
          }
        },
        "aa10ca246aeb481ea16c996ca75c5745": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d610311f26b4c6ea90c0063047b4e22",
            "max": 90868373,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ef45cb0544f149ec8e3c50bfc49bf08d",
            "value": 90868373
          }
        },
        "aaca50a213074637af60d052b9581fa2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aafd9e107ea74978bcbb0ea45d79b25c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9775c53802844b03ae72e453b49a9a38",
            "placeholder": "​",
            "style": "IPY_MODEL_f3490c19f41445b48240d0f709844fc1",
            "value": " 3.73k/3.73k [00:00&lt;00:00, 146kB/s]"
          }
        },
        "af6dad1da929414581a8bcc32ae11968": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "afa886057f904497a8545425ccc9554b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eabe480281ba400999dd0e423a1c3a8f",
            "max": 229,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e997ef95cbb04145b2436236582a396c",
            "value": 229
          }
        },
        "b0964db1123547baaf30fd6da3066261": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c046d8b46f324f29921da945d6de6cda",
              "IPY_MODEL_4c84fa8c0bc349cebdea3e9401898ad2",
              "IPY_MODEL_aafd9e107ea74978bcbb0ea45d79b25c"
            ],
            "layout": "IPY_MODEL_49e3fa3e17d54a31adf2d47741b5c3ab"
          }
        },
        "b1d29366256949b2932638955c135311": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5ecac8bb08d647369b934cedeb7165b6",
            "max": 122,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0909bd3a02334ebb9c937582ab58dbb8",
            "value": 122
          }
        },
        "b204d3524e194dcf9fae16aa670d2791": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b7da521dc25f4b599467ed55511fd1af": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd133b9d08ba49bd9f5cc1a5479b9fde": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a65eca6af86d4706bd6cbc90858b80f5",
              "IPY_MODEL_ea4952cb69ff4b5f86a84360cfe38529",
              "IPY_MODEL_a6256613007a4e2e8e7542e1fbc8bd53"
            ],
            "layout": "IPY_MODEL_b7da521dc25f4b599467ed55511fd1af"
          }
        },
        "c046d8b46f324f29921da945d6de6cda": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_49766cd63f1b4fa9b495b1f1d0144bf9",
            "placeholder": "​",
            "style": "IPY_MODEL_827d64cdb37d4cbeb62f0795a8cdc978",
            "value": "README.md: 100%"
          }
        },
        "c2690ecb2344402493df7eeb8c7ab73f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c96b87e011c3458397001776a43a3649",
            "placeholder": "​",
            "style": "IPY_MODEL_e77cbe7640844155b1600143347d8c18",
            "value": "1_Pooling/config.json: 100%"
          }
        },
        "c34c4a3d6a5a4efc9b4a100ca7130c0c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c81bf00fcbfa42eeb5a52f9520bf0532": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b4f48185dea46f5beb00314869eb15c",
            "placeholder": "​",
            "style": "IPY_MODEL_f7a52ad6c7334d628506bba2e2eb84cf",
            "value": "model.safetensors: 100%"
          }
        },
        "c96b87e011c3458397001776a43a3649": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb7b9dc04e8b4cd1ae8776e5e907725b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d4a0d6c3fad64f6c913c2c7e7060ca4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_46e36b3cae2343d5993716151d3990ff",
            "placeholder": "​",
            "style": "IPY_MODEL_0a67cb9a26544b64a71ae32d07c13b7b",
            "value": "vocab.txt: 100%"
          }
        },
        "d639022bcc384ad88a9c2e0d2f2bf11c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d6508d745c3e4bc2a244f67a6027a631": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dcbbc4c11b2041bda4a07bfcc5e8289c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd4d7eb9cb7247dfb3b3b50680690a4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b0808a452904663a73ada839caf0984",
            "placeholder": "​",
            "style": "IPY_MODEL_094ee7646d86499983f4eef515f6c8ea",
            "value": "tokenizer.json: 100%"
          }
        },
        "dd8ae2f7387d499cb22d7999e58e773f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dec19e176391414ea4ab2ccc5f2d463b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e77cbe7640844155b1600143347d8c18": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e997ef95cbb04145b2436236582a396c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e9faebfd14e3495fbfeebdeaa356f047": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ec5c4019969406dba9ddeba56ebb33f",
            "placeholder": "​",
            "style": "IPY_MODEL_ea9409d2e3c8494fa6035b861f388a77",
            "value": " 629/629 [00:00&lt;00:00, 29.5kB/s]"
          }
        },
        "ea48145d9a2f49c0ad56f99411d9c043": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea4952cb69ff4b5f86a84360cfe38529": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f77e5910f7694623ba23ea499edd1f61",
            "max": 112,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1adf1c2e50c84847953aee88d5d7566e",
            "value": 112
          }
        },
        "ea9409d2e3c8494fa6035b861f388a77": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eabe480281ba400999dd0e423a1c3a8f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef45cb0544f149ec8e3c50bfc49bf08d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f20d9cf4ccf54243a29f87402f7a166d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_48287d97529b4cf09d62261b91ee94a4",
            "max": 629,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_299d0bd58ec74160859c7f653c8b60f4",
            "value": 629
          }
        },
        "f2917330828b49dd8a5197515969c663": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f3490c19f41445b48240d0f709844fc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f71b45218df34ca48607e5737a7648f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3851b7a2ceaa4c3c8609b6744187025a",
            "placeholder": "​",
            "style": "IPY_MODEL_825509b73b8f47a994d69e083f8322ae",
            "value": " 190/190 [00:00&lt;00:00, 1.13kB/s]"
          }
        },
        "f77e5910f7694623ba23ea499edd1f61": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7a52ad6c7334d628506bba2e2eb84cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f839cabfd8714719896ec63f5ec2c1a1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9ca5420060d4dcf9bf0dfb61cb73d45": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa26418b38124340b93936c17cd2be5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fb70204ec341483bb836299ac7f1ed47": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9ad9cbbd434249ea8b405de17cc380ff",
              "IPY_MODEL_0f00ac0bf22b47f293e2c640c2a63364",
              "IPY_MODEL_4118a73b2ec640ce84d6ddd753b15618"
            ],
            "layout": "IPY_MODEL_632d369a72c94f3ba73080f3851ae559"
          }
        },
        "fdcd675e07bb400e91aa021dd5be9a98": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_86e71df5d86448e68cced6df2d17c395",
            "placeholder": "​",
            "style": "IPY_MODEL_b204d3524e194dcf9fae16aa670d2791",
            "value": " 122/122 [00:00&lt;00:00, 4.53kB/s]"
          }
        },
        "fe76081e53fb40dabf2198815fd14e61": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}